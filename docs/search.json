[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methods for Structural Microeconometrics",
    "section": "",
    "text": "Welcome to this course! Find details on the syllabus here, and check out the menu to find other materials.\nHere is a link to the git repo for this course, which by cloning is probably the simplest way for you to get access to the data, code, and assignments. I will continue to add materials throughout the course. You may prefer to selectively download these materials."
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "Methods for Structural Microeconometrics",
    "section": "Topics",
    "text": "Topics\n\nIdentification, Credible Inference, and Marschak’s Maxim\n2024 is the last year I will make this topic a part of the course.\nWe formally define identification and discuss (via examples) what people really mean when they talk about identification and credible inference. We use the Generalized Roy Model to compare identification via functional form to nonparametric identification.\nWe introduce Marschak’s Maxim as a guide for doing empirical model-based research.\n\nReading\nThe two survey articles by Keane (2010) (link) and Angrist and Pischke (2010) (link) - although aging - provide two important perspectives on the issues of credible inference in economics. Low and Meghir (2017) provide a nice review of the advantages of the structural approach.\nThe original paper by Marschak (1953) may be of interest. Heckman and Vytlacil (2007) provide a nice discussion of Marschak’s Maxim in the context of policy evaluation. They introduce (Heckman and Vytlacil 2005; Carneiro, Heckman, and Vytlacil 2011) the Marginal Treatment Effect as a tool for thinking about quasi-experimental estimators and policy evaluation.\n\n\n\nDynamic Discrete Choice\nWe introduce the dynamic discrete choice model and briefly discuss identification when all persistent state variables are observed. We review some of the basics of discrete choice such as the generalized extreme value distribution, which produces tractable choice probabilities with relatively flexible cross-price elasticities.\n\nReading\nThe main example that we work with throughout the course can be found in Mullins (2022) (pdf).\nRust (1987) is the canonical example demonstrating estimation of dynamic discrete choice models with maximum likelihood and a nested solution method.\nWe show that if one can directly estimate choice probabilities, several tractable approaches produce estimates of structural parameters without repeatedly solving the model. These include Hotz and Miller (1993), Aguirregabiria and Mira (2002), Aguirregabiria and Mira (2007), Pesendorfer and Schmidt-Dengler (2008), and Arcidiacono and Miller (2011). We will review these methods and why they are not appropriate to use in Mullins (2022).\n\n\n\nEstimation of Dynamic Models with Unobserved Heterogeneity\nUsing Mullins (2022) as an example, we talk about the inferential pitfalls that can occur when models fail to account for unobserved heterogeneity. We briefly discuss how this can depend on estimation approaches and sources of identification.\nWe review methods for estimation of dynamic models, including the Expectation-Maximization algorithm (EM) (see, e.g. Arcidiacono and Miller (2011)) and the clustering approach of Bonhomme and Manresa (2015). We review practical considerations for these approaches and introduce the Forward-Back algorithm for implementing EM in hidden Markov Models with time-varying unobserved state variables. We introduce a sparse matrix implementation of this algorithm.\n\n\nIdentification of Dynamic Models with Unobserved Heterogeneity\nWe discuss how either panel data or instrumental variables can facilitate identification of models with unobserved heterogeneity, and briefly review identification results for finite mixtures in panel data settings due to Kasahara and Shimotsu (2009) and Bonhomme, Jochmans, and Robin (2016). Berry and Compiani (2023) analyze identification and estimation of dynamic models with persistent heterogeneity using instrumental variables."
  },
  {
    "objectID": "index.html#assessment",
    "href": "index.html#assessment",
    "title": "Methods for Structural Microeconometrics",
    "section": "Assessment",
    "text": "Assessment\nThere will be 7 problem sets. Your best 5 of these 7 problem sets will be worth 20%. Hence, you can skip two if you want.\nHere is the proposed timeline of due dates. Submissions must be made through Canvas as a notebook (e.g. jupyter or quarto) formatted to html with printed output.\n\n\n\nAssignment\nDue Date\n\n\n\n\nAssignment 1\nMarch 22\n\n\nAssignment 2\nMarch 29\n\n\nAssignment 3\nApril 5\n\n\nAssignment 4\nApril 12\n\n\nAssignment 5\nApril 19 April 26\n\n\nAssignment 6\nApril 26 May 3\n\n\nAssignment 7\nMay 3 May 10"
  },
  {
    "objectID": "index.html#office-hours",
    "href": "index.html#office-hours",
    "title": "Methods for Structural Microeconometrics",
    "section": "Office Hours",
    "text": "Office Hours\nI will provide a link on Canvas to sign up for my weekly office hours."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "assignments/Assignment-1.html",
    "href": "assignments/Assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Here is code to load the dataset and do a little cleaning / filtering.\nThe sample is all mothers in the PSID who are unmarried at the time of their first childbirth.\n\nusing CSV, DataFrames, DataFramesMeta\n\ndata = @chain begin\n    CSV.read(\"../children-cash-transfers/data/MainPanelFile.csv\",DataFrame,missingstring = \"NA\")\n    @select :MID :year :wage :hrs :earn :SOI :CPIU :WelfH :FSInd\n    @subset :year.&gt;=1985 :year.&lt;=2010\n    @transform :AFDC = :WelfH.&gt;0\n    @rename :FS = :FSInd\n    end\n\n89747×10 DataFrame89722 rows omitted\n\n\n\nRow\nMID\nyear\nwage\nhrs\nearn\nSOI\nCPIU\nWelfH\nFS\nAFDC\n\n\n\nInt64\nInt64\nFloat64?\nInt64?\nFloat64?\nInt64\nFloat64\nFloat64?\nInt64?\nBool?\n\n\n\n\n1\n4031\n1990\nmissing\nmissing\nmissing\n43\n0.758793\n0.0\n0\nfalse\n\n\n2\n4031\n1991\nmissing\nmissing\nmissing\n43\n0.790786\n0.0\n0\nfalse\n\n\n3\n4031\n1992\nmissing\nmissing\nmissing\n43\n0.814835\n0.0\n1\nfalse\n\n\n4\n4031\n1993\nmissing\nmissing\nmissing\n43\n0.839034\n0.0\n0\nfalse\n\n\n5\n4031\n1994\nmissing\n0\n0.0\n43\n0.860812\n1704.0\n1\ntrue\n\n\n6\n4031\n1995\nmissing\n0\n0.0\n43\n0.88496\n1704.0\n1\ntrue\n\n\n7\n4031\n1996\nmissing\n0\n0.0\n43\n0.910948\n1704.0\n1\ntrue\n\n\n8\n4031\n1997\nmissing\nmissing\nmissing\n43\n0.932244\nmissing\nmissing\nmissing\n\n\n9\n4031\n1998\nmissing\n0\n0.0\n43\n0.946664\n0.0\n1\nfalse\n\n\n10\n4031\n1999\nmissing\nmissing\nmissing\n43\n0.967426\nmissing\nmissing\nmissing\n\n\n11\n4031\n2000\n3.33333\n120\n400.0\n43\n1.0\n0.0\n0\nfalse\n\n\n12\n4031\n2001\nmissing\nmissing\nmissing\n43\n1.02817\nmissing\nmissing\nmissing\n\n\n13\n4031\n2002\nmissing\n0\n0.0\n43\n1.04457\n0.0\n0\nfalse\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n89736\n9308002\n1999\nmissing\nmissing\nmissing\n39\n0.967426\nmissing\nmissing\nmissing\n\n\n89737\n9308002\n2000\nmissing\nmissing\nmissing\n39\n1.0\nmissing\nmissing\nmissing\n\n\n89738\n9308002\n2001\nmissing\nmissing\nmissing\n39\n1.02817\nmissing\nmissing\nmissing\n\n\n89739\n9308002\n2002\nmissing\nmissing\nmissing\n39\n1.04457\nmissing\nmissing\nmissing\n\n\n89740\n9308002\n2003\nmissing\nmissing\nmissing\n39\n1.06857\nmissing\nmissing\nmissing\n\n\n89741\n9308002\n2004\nmissing\nmissing\nmissing\n39\n1.09708\nmissing\nmissing\nmissing\n\n\n89742\n9308002\n2005\nmissing\nmissing\nmissing\n39\n1.13401\nmissing\nmissing\nmissing\n\n\n89743\n9308002\n2006\nmissing\nmissing\nmissing\n39\n1.17054\nmissing\nmissing\nmissing\n\n\n89744\n9308002\n2007\nmissing\nmissing\nmissing\n39\n1.20414\nmissing\nmissing\nmissing\n\n\n89745\n9308002\n2008\nmissing\nmissing\nmissing\n39\n1.25008\nmissing\nmissing\nmissing\n\n\n89746\n9308002\n2009\nmissing\nmissing\nmissing\n39\n1.24608\nmissing\nmissing\nmissing\n\n\n89747\n9308002\n2010\nmissing\nmissing\nmissing\n39\n1.26647\nmissing\nmissing\nmissing\n\n\n\n\n\n\nYou may be unfamiliar with some of these commands, which make use of DataFrames and DataFramesMeta. In particular, think of the @chain macro as a way to compose functions. So for example:\n\nd1 = @chain d2 begin\n    func1(x)\n    func2(y)\n    func3(z)\nend\n\nis equivalent to calling:\n\nd1 = func3(func2(func1(d2,x),y),z)\n\nIf you want to understand better, google is your friend!"
  },
  {
    "objectID": "assignments/Assignment-1.html#setup-loading-the-data",
    "href": "assignments/Assignment-1.html#setup-loading-the-data",
    "title": "Assignment 1",
    "section": "",
    "text": "Here is code to load the dataset and do a little cleaning / filtering.\nThe sample is all mothers in the PSID who are unmarried at the time of their first childbirth.\n\nusing CSV, DataFrames, DataFramesMeta\n\ndata = @chain begin\n    CSV.read(\"../children-cash-transfers/data/MainPanelFile.csv\",DataFrame,missingstring = \"NA\")\n    @select :MID :year :wage :hrs :earn :SOI :CPIU :WelfH :FSInd\n    @subset :year.&gt;=1985 :year.&lt;=2010\n    @transform :AFDC = :WelfH.&gt;0\n    @rename :FS = :FSInd\n    end\n\n89747×10 DataFrame89722 rows omitted\n\n\n\nRow\nMID\nyear\nwage\nhrs\nearn\nSOI\nCPIU\nWelfH\nFS\nAFDC\n\n\n\nInt64\nInt64\nFloat64?\nInt64?\nFloat64?\nInt64\nFloat64\nFloat64?\nInt64?\nBool?\n\n\n\n\n1\n4031\n1990\nmissing\nmissing\nmissing\n43\n0.758793\n0.0\n0\nfalse\n\n\n2\n4031\n1991\nmissing\nmissing\nmissing\n43\n0.790786\n0.0\n0\nfalse\n\n\n3\n4031\n1992\nmissing\nmissing\nmissing\n43\n0.814835\n0.0\n1\nfalse\n\n\n4\n4031\n1993\nmissing\nmissing\nmissing\n43\n0.839034\n0.0\n0\nfalse\n\n\n5\n4031\n1994\nmissing\n0\n0.0\n43\n0.860812\n1704.0\n1\ntrue\n\n\n6\n4031\n1995\nmissing\n0\n0.0\n43\n0.88496\n1704.0\n1\ntrue\n\n\n7\n4031\n1996\nmissing\n0\n0.0\n43\n0.910948\n1704.0\n1\ntrue\n\n\n8\n4031\n1997\nmissing\nmissing\nmissing\n43\n0.932244\nmissing\nmissing\nmissing\n\n\n9\n4031\n1998\nmissing\n0\n0.0\n43\n0.946664\n0.0\n1\nfalse\n\n\n10\n4031\n1999\nmissing\nmissing\nmissing\n43\n0.967426\nmissing\nmissing\nmissing\n\n\n11\n4031\n2000\n3.33333\n120\n400.0\n43\n1.0\n0.0\n0\nfalse\n\n\n12\n4031\n2001\nmissing\nmissing\nmissing\n43\n1.02817\nmissing\nmissing\nmissing\n\n\n13\n4031\n2002\nmissing\n0\n0.0\n43\n1.04457\n0.0\n0\nfalse\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n89736\n9308002\n1999\nmissing\nmissing\nmissing\n39\n0.967426\nmissing\nmissing\nmissing\n\n\n89737\n9308002\n2000\nmissing\nmissing\nmissing\n39\n1.0\nmissing\nmissing\nmissing\n\n\n89738\n9308002\n2001\nmissing\nmissing\nmissing\n39\n1.02817\nmissing\nmissing\nmissing\n\n\n89739\n9308002\n2002\nmissing\nmissing\nmissing\n39\n1.04457\nmissing\nmissing\nmissing\n\n\n89740\n9308002\n2003\nmissing\nmissing\nmissing\n39\n1.06857\nmissing\nmissing\nmissing\n\n\n89741\n9308002\n2004\nmissing\nmissing\nmissing\n39\n1.09708\nmissing\nmissing\nmissing\n\n\n89742\n9308002\n2005\nmissing\nmissing\nmissing\n39\n1.13401\nmissing\nmissing\nmissing\n\n\n89743\n9308002\n2006\nmissing\nmissing\nmissing\n39\n1.17054\nmissing\nmissing\nmissing\n\n\n89744\n9308002\n2007\nmissing\nmissing\nmissing\n39\n1.20414\nmissing\nmissing\nmissing\n\n\n89745\n9308002\n2008\nmissing\nmissing\nmissing\n39\n1.25008\nmissing\nmissing\nmissing\n\n\n89746\n9308002\n2009\nmissing\nmissing\nmissing\n39\n1.24608\nmissing\nmissing\nmissing\n\n\n89747\n9308002\n2010\nmissing\nmissing\nmissing\n39\n1.26647\nmissing\nmissing\nmissing\n\n\n\n\n\n\nYou may be unfamiliar with some of these commands, which make use of DataFrames and DataFramesMeta. In particular, think of the @chain macro as a way to compose functions. So for example:\n\nd1 = @chain d2 begin\n    func1(x)\n    func2(y)\n    func3(z)\nend\n\nis equivalent to calling:\n\nd1 = func3(func2(func1(d2,x),y),z)\n\nIf you want to understand better, google is your friend!"
  },
  {
    "objectID": "assignments/Assignment-1.html#question-1",
    "href": "assignments/Assignment-1.html#question-1",
    "title": "Assignment 1",
    "section": "Question 1",
    "text": "Question 1\nCalculate average welfare participation (AFDC) by year and plot it. What do you think happened with welfare participation in 1996 and after? If you don’t know the historical context, a quick search online or a read of this paper should help you out.\nIf you are new to julia, here is average hours calculated and plotted to get you started.\n\nusing StatsPlots, Statistics\n\nd = @chain data begin\n    groupby(:year)\n    @combine :Hours = mean(skipmissing(:hrs))\n    @subset .!isnan.(:Hours)\nend\n\n@df d plot(:year,:Hours, legend = :none, linewidth = 2)\nxlabel!(\"Year\")\nylabel!(\"Average Welfare Participation\")"
  },
  {
    "objectID": "assignments/Assignment-1.html#question-2",
    "href": "assignments/Assignment-1.html#question-2",
    "title": "Assignment 1",
    "section": "Question 2",
    "text": "Question 2\nNow write code to\n\nDeflate earnings by CPI (CPIU).\nCalculate annual average earnings for each individual (identified by MID).\nDrop individuals with fewer than 10 years of data.\nCategorize individuals by whether their average earnings is below or above the median across individuals.\nPlot average participation in each year for individuals in each of these two categories.\n\nDo you think this pattern is likely to be generated by a model without persistent unobserved heterogeneity? No strictly correct answer here, just curious to read what you think.\nIn case it helps, here is code for the first three steps. You could edit this to add additional operations to the chain or work with d directly.\n\nd = @chain data begin\n    @transform :earn = :earn ./ :CPIU\n    groupby(:MID)\n    @combine :T = sum(.!ismissing.(:earn)) :earn = mean(skipmissing(:earn)) \n    @subset :T .&gt;= 10\nend\n\n1089×3 DataFrame1064 rows omitted\n\n\n\nRow\nMID\nT\nearn\n\n\n\nInt64\nInt64\nFloat64\n\n\n\n\n1\n4031\n10\n40.0\n\n\n2\n4179\n16\n6089.12\n\n\n3\n7030\n11\n4500.53\n\n\n4\n41007\n12\n16147.7\n\n\n5\n41008\n11\n0.0\n\n\n6\n45030\n11\n14374.3\n\n\n7\n45031\n11\n17693.5\n\n\n8\n47031\n11\n15946.7\n\n\n9\n84005\n18\n30769.6\n\n\n10\n105030\n14\n4633.61\n\n\n11\n106173\n13\n17714.4\n\n\n12\n122173\n13\n56275.2\n\n\n13\n126003\n19\n6705.08\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n1078\n6843006\n19\n1805.93\n\n\n1079\n6843173\n19\n5005.44\n\n\n1080\n6845005\n19\n19795.4\n\n\n1081\n6849005\n19\n2450.55\n\n\n1082\n6849188\n15\n23259.0\n\n\n1083\n6853003\n19\n4856.42\n\n\n1084\n6862005\n11\n18899.4\n\n\n1085\n6862008\n19\n26634.8\n\n\n1086\n6864002\n19\n8695.92\n\n\n1087\n6864003\n18\n13627.5\n\n\n1088\n6867013\n13\n389.097\n\n\n1089\n6872171\n17\n3294.07"
  },
  {
    "objectID": "assignments/Assignment-1.html#question-3",
    "href": "assignments/Assignment-1.html#question-3",
    "title": "Assignment 1",
    "section": "Question 3",
    "text": "Question 3\nThis question is to familiarize you with the module Tranfers.jl which will enable you to calculate post-tax and transfer income for individuals given their earnings, non-labor income, state, year, and family size. The function budget in this module takes the arguments:\n\nE: monthly earnings (either real or nominal)\nN: monthly non-labor income (real or nominal)\nSOI: the SOI code for state of residence\nyear: calendar year\nnum_kids: the number of children\ncpi: set to 1. if E and N are nominal\np: equal to 0 if no programs, 1 if food stamps, 2 if food stamps + welfare.\n\nFor example the function call:\n\nTransfers.budget(500.,0.,23,2000,2,1.,2)\n\ncalculates net income for a mother in Michigan (SOI code 23) with 2 kids, nominal labor income of $500 a month in the year 2000, and no non-labor income.\n\ninclude(\"../children-cash-transfers/src/Transfers.jl\")\n\nTransfers.budget(500.,0.,23,2000,2,1.,2)\n\n978.5408333333334\n\n\nCreate a graph that represents total net transfers for a single mother with two kids in the years 1990 and 2000 and in the states of Mississippi and New York. Depict these transfers as a function of earnings between the values of 0 and $1,000 a month (nominal). You can assume that all households are receiving both food stamps and welfare.\nWhat do you make of the differences in these transfers across states and over time?"
  },
  {
    "objectID": "assignments/Assignment-2.html",
    "href": "assignments/Assignment-2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "We are building toward estimating a dynamic discrete choice model with persistent unobserved heterogeneity. To make a start, in this problem set you will estimate a static discrete choice problem on the data."
  },
  {
    "objectID": "assignments/Assignment-2.html#setup-code-for-mle",
    "href": "assignments/Assignment-2.html#setup-code-for-mle",
    "title": "Assignment 2",
    "section": "Setup: Code for MLE",
    "text": "Setup: Code for MLE\n\nReading in Data\nYou are going to build on the code below that estimates a very simple model of labor supply and program participation.\nFirst, here is a chunk of code to read in the data and prepare it. The variables in julia DataFrames are typically not type stable which can slow down your code considerably. So you will note that in the final step here I bring the variables I want to use out of a DataFrame and into a named tuple with vectors containing a fixed type.\n\nusing CSV, DataFrames, DataFramesMeta, Optim\ninclude(\"../children-cash-transfers/src/Transfers.jl\")\n\n# for this simple model we can just drop missing data. When we estimate the model with persistent latent heterogeneity, we will need complete panels (including years where choices are missing).\n\ndata = @chain begin\n    CSV.read(\"../children-cash-transfers/data/MainPanelFile.csv\",DataFrame,missingstring = \"NA\")\n    #@select :MID :year :wage :hrs :earn :SOI :CPIU :WelfH :FSInd :num_child :age\n    @subset :year.&gt;=1985 :year.&lt;=2010\n    @transform :AFDC = :WelfH.&gt;0 \n    @rename :FS = :FSInd\n    @transform :P  = :FS + :AFDC :H = min.(2,round.(Union{Int64, Missing},:hrs / (52*20)))\n    @subset .!ismissing.(:P) .&& .!ismissing.(:H)\n    @transform @byrow :wage = begin\n        if :hrs&gt;0 && :earn&gt;0\n            return :earn / :hrs / :CPIU\n        else\n            return missing\n        end\n    end\nend\n\ndata_mle = (;P = Int64.(data.P), H = Int64.(data.H), year = data.year, age = data.age,\n            soi = data.SOI, num_kids = data.num_child, cpi = data.CPIU,\n            logwage = log.(coalesce.(data.wage,1.)),wage_missing = ismissing.(data.wage))\n\n(P = [2, 2, 2, 1, 0, 0, 0, 2, 1, 0  …  2, 2, 1, 2, 0, 0, 1, 1, 1, 1], H = [0, 0, 0, 0, 0, 0, 0, 0, 0, 2  …  0, 0, 2, 2, 2, 2, 0, 0, 0, 0], year = [1994, 1995, 1996, 1998, 2000, 2002, 2004, 2006, 2008, 1990  …  1991, 1992, 1991, 1992, 1991, 1992, 1991, 1992, 1991, 1992], age = [21, 22, 23, 25, 27, 29, 31, 33, 35, 17  …  21, 22, 23, 24, 39, 40, 30, 31, 25, 26], soi = [43, 43, 43, 43, 43, 43, 43, 43, 43, 17  …  44, 44, 7, 7, 5, 5, 39, 39, 39, 39], num_kids = [2, 2, 3, 3, 3, 3, 3, 3, 3, 1  …  2, 2, 2, 2, 3, 3, 4, 4, 2, 2], cpi = [0.860812349005761, 0.884959812302546, 0.910948243820851, 0.946664188812488, 1.0, 1.04457233785542, 1.09707768072849, 1.17054218546739, 1.25008130459022, 0.758792510685746  …  0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336], logwage = [0.0, 0.0, 0.0, 0.0, 1.2039728043259361, 0.0, 0.0, 0.0, 0.0, 0.8230555833449215  …  0.0, 0.0, 2.6512574120409043, 2.3314412292534223, 1.265948758245773, 1.8744481199656744, 0.0, 0.0, 0.0, 0.0], wage_missing = Bool[1, 1, 1, 1, 0, 1, 1, 1, 1, 0  …  1, 1, 0, 0, 0, 0, 1, 1, 1, 1])\n\n\n\n\nThe Model\nConsider a static model with 9 discrete choices. Hours choices are given by \\(H_{j}\\in\\{0,20,40\\}\\) and participation choices indicated by \\(P_{j}\\in\\{0,1,2\\}\\). Individuals may choose any combination of these, with a Type I extreme value distributed shock \\(\\epsilon_{j}\\) for choice \\(j\\) that is iid over time and across individuals. The scale of these shocks is given by \\(\\sigma\\).\nUtility for individual \\(i\\) at time \\(t\\) is given by:\n\\[ U_{itj} = \\log(\\max\\{50,Y_{it}(W_{it}H_{j})\\}) + \\alpha_{l}\\log(112 - H_{j}) - \\alpha_{P,1}\\mathbf{1}\\{P_{j}&gt;0\\} - \\alpha_{P,2}\\mathbf{1}\\{P_{j}&gt;1\\} \\]\nwhere \\(Y_{it}\\) is a net income function for individual \\(i\\) at time \\(t\\) and depends on their earnings (\\(W_{it}H_{j}\\)), the number of children in the household, and the policy environment in the individual’s state of residence.\nWages are a deterministic function of age only:\n\\[ \\log(W_{it}) = \\gamma_{0} + \\gamma_{1}\\text{Age}_{it} + \\gamma_{2}\\text{Age}_{it}^2 \\]\nand are measured with normally distributed iid measurement error:\n\\[ \\log(W^{o}_{it}) = \\log(W_{it}) + \\zeta_{it},\\qquad\\zeta_{it}\\sim\\mathcal{N}(0,\\sigma^2_{W}) \\]\nThe code below calculates utility and indexes choices.\n\nj_idx(p,h) = p*3 + h + 1\nfunction utility(p,h,soi,cpi,year,num_kids,wage,pars)\n    (;αl,Hgrid,σ,αP) = pars\n    hrs = Hgrid[1+h]\n    earn = wage * hrs\n    net_income = max(50.,Transfers.budget(earn,0.,soi,year,num_kids,cpi,p))\n    return log(net_income) + αl*log(112-hrs) - αP[1]*(p&gt;0) - αP[2]*(p&gt;1)\nend\n\npars = (;αl = 1., σ = 1., σW = 1., γ = zeros(3),\n    Hgrid = [0,20.,40.],αP = zeros(2))\n\n(αl = 1.0, σ = 1.0, σW = 1.0, γ = [0.0, 0.0, 0.0], Hgrid = [0.0, 20.0, 40.0], αP = [0.0, 0.0])\n\n\nChoice probabilities are given by the logit formula.\n\nfunction choice_prob!(logP,it,data,pars)\n    (;σ,γ) = pars\n    wage = exp(γ[1] + γ[2] * data.age[it] + γ[3] * data.age[it]^2)\n    denom = 0.\n    umax = -Inf\n    for p in 0:2\n        for h in 0:2\n            j = j_idx(p,h)\n            u = utility(p,h,data.soi[it],data.cpi[it],data.year[it],data.num_kids[it],wage,pars)\n            logP[j] = u / σ\n            umax = u&gt;umax ? u : umax\n        end\n    end\n    logP[:] .-= umax / σ\n    denom = log(sum(exp.(logP)))\n    logP[:] .-= denom #&lt;- nornalize choice probabilities\nend\n\nchoice_prob! (generic function with 1 method)\n\n\nAnd so the log-likelihood takes a simple form.\n\nfunction log_likelihood(logP,it,data,pars)\n    (;γ,σW) = pars\n    ll = 0.\n    if !data.wage_missing[it]\n        ll += -0.5((data.logwage[it] - γ[1] - γ[2]*data.age[it] - γ[3]*data.age[it]^2) / σW)^2 - log(σW)\n    end\n    choice_prob!(logP,it,data,pars)\n    j = j_idx(data.P[it],data.H[it])\n    ll += logP[j]\n    return ll\nend\n\nfunction update(x,p)\n    αl = exp(x[1])\n    σ = exp(x[2])\n    γ = x[3:5]\n    σW = exp(x[6])\n    αP = x[7:8]\n    return (;p...,αl,σ,σW,αP,γ)\nend\n\nfunction log_likelihood(x,data,pars)\n    pars = update(x,pars)\n    logP = zeros(eltype(x),9)\n    ll = 0.\n    for it in eachindex(data.P)\n        ll += log_likelihood(logP,it,data,pars)\n    end\n    return ll\nend\n\nlog_likelihood (generic function with 2 methods)\n\n\nWe can use Optim to maximize the log-likelihood:\n\nx0 = zeros(8)\nx0 = [0.,0.,log(10.),0.,0.,0.,0.,0.]\nres = optimize(x-&gt;-log_likelihood(x,data_mle,pars),x0,BFGS(),autodiff=:forward,Optim.Options(show_trace = true))\npars = update(res.minimizer,pars)\n\nIter     Function value   Gradient norm \n     0     7.234596e+04     3.085114e+06\n * time: 0.00633692741394043\n     1     7.219007e+04     7.930873e+05\n * time: 0.4547231197357178\n     2     7.189756e+04     8.240034e+05\n * time: 0.5587921142578125\n     3     6.311750e+04     1.040038e+06\n * time: 0.803109884262085\n     4     5.773072e+04     3.637938e+06\n * time: 1.2063300609588623\n     5     5.738226e+04     6.079719e+06\n * time: 1.3685970306396484\n     6     5.692685e+04     7.218571e+06\n * time: 1.5767319202423096\n     7     5.644027e+04     8.094668e+06\n * time: 1.9543049335479736\n     8     5.629959e+04     8.406176e+06\n * time: 2.011056900024414\n     9     5.408423e+04     4.483122e+06\n * time: 2.0493218898773193\n    10     5.315575e+04     3.603074e+06\n * time: 2.0878500938415527\n    11     5.209566e+04     2.625888e+05\n * time: 2.145103931427002\n    12     5.195541e+04     2.083292e+05\n * time: 2.1833839416503906\n    13     5.188423e+04     6.807290e+05\n * time: 2.221687078475952\n    14     5.187199e+04     2.396172e+05\n * time: 2.279866933822632\n    15     5.186933e+04     5.094151e+03\n * time: 2.3182590007781982\n    16     5.186887e+04     7.774766e+03\n * time: 2.3758020401000977\n    17     5.186878e+04     2.823744e+04\n * time: 2.433206081390381\n    18     5.186871e+04     1.085063e+04\n * time: 2.4902100563049316\n    19     5.186852e+04     2.080594e+04\n * time: 2.56597900390625\n    20     5.186850e+04     5.468883e+02\n * time: 2.6230380535125732\n    21     5.186850e+04     4.523469e+00\n * time: 2.679996967315674\n    22     5.186850e+04     8.466642e-01\n * time: 2.736649990081787\n    23     5.186850e+04     3.014776e-02\n * time: 2.7939069271087646\n    24     5.186850e+04     1.164486e-03\n * time: 2.8322980403900146\n    25     5.186850e+04     1.957495e-05\n * time: 2.888066053390503\n    26     5.186850e+04     1.250626e-07\n * time: 2.9259419441223145\n    27     5.186850e+04     1.206905e-08\n * time: 2.9638988971710205\n    28     5.186850e+04     3.893547e-09\n * time: 3.0017800331115723\n\n\n(αl = 2.8546233783929473, σ = 0.9509286759263859, σW = 0.7224860062936598, γ = [-0.023620750582789916, 0.10877398375254817, -0.0012722347394312002], Hgrid = [0.0, 20.0, 40.0], αP = [2.035301636135555, 1.0137619542631466])"
  },
  {
    "objectID": "assignments/Assignment-2.html#question-1",
    "href": "assignments/Assignment-2.html#question-1",
    "title": "Assignment 2",
    "section": "Question 1",
    "text": "Question 1\nWrite code to calculate the standard errors for these parameters and present the estimates with standard errors in a table."
  },
  {
    "objectID": "assignments/Assignment-2.html#question-2",
    "href": "assignments/Assignment-2.html#question-2",
    "title": "Assignment 2",
    "section": "Question 2",
    "text": "Question 2\nWhat sources of variation are identifying \\(\\sigma\\), the responsiveness of choices to changes in the payoffs? How is the responsivess of program participation and labor supply to changes in incentives limited in this framework?"
  },
  {
    "objectID": "assignments/Assignment-2.html#question-3",
    "href": "assignments/Assignment-2.html#question-3",
    "title": "Assignment 2",
    "section": "Question 3",
    "text": "Question 3\nRe-estimate the model with a one layer nested logit structure, where the individual first makes one of the three participation choices before making one of the three hours choices. That is, if the choices can be ordered as: \\[ \\{P_{j}\\}_{j=1}^9 = \\{0,0,0,1,1,1,2,2,2\\}, \\{H_{j}\\}_{j=1}^9 = \\{0,20,40,0,20,40,0,20,40\\} \\]\nthen the partition is:\n\\[ \\mathcal{B} = \\{\\{1,2,3\\},\\{4,5,6\\},\\{7,8,9\\}\\} \\]\nLet \\(\\sigma_{H}\\) be the dispersion of shocks within each partition and \\(\\sigma_{P}\\) be the dispersion of shocks across partitions. Intuitively, how are these dispersion parameters identified? (Hint: think about what is changing payoffs across observations and think about the two dimensions of the discrete choice)."
  },
  {
    "objectID": "assignments/Assignment-2.html#question-4",
    "href": "assignments/Assignment-2.html#question-4",
    "title": "Assignment 2",
    "section": "Question 4",
    "text": "Question 4\nWhat sort of variation do you think is in the data that we are failing to put in our model? What do you think are the implications for key parameter estimates?"
  },
  {
    "objectID": "assignments/Assignment-3.html",
    "href": "assignments/Assignment-3.html",
    "title": "Assignment 3 - Solving the Dynamic Program",
    "section": "",
    "text": "Since you only have a week, I’m not going to make you code every piece of the model solution. The folder /children-cash-transfers/src contains:\n\nfunctions to calculate utility and net income from each choice\nan indexing rule that maps choices to hours and participation\nthe structure for the nested logit and code to calculate nested logit probabilities and inclusive values\nfunctions to calculate transition probabilities\ncode to define a default set of parameters and hyperparameters (the number of wage shocks, number of types, etc)\n\nIf you have cloned the course git repo, you can load all of this source code as follows:\n\ninclude(\"../children-cash-transfers/src/model.jl\")\n\nplain_logit (generic function with 1 method)\n\n\nIt will help to quickly explain some of this, but I recommend you read the code carefully since I won’t explain the functions in depth.\n\n\nRecall that in the model there are only three state variables to track: (1) the individual’s type (\\(k\\)); (2) the wage shock (\\(\\varepsilon\\)); and (3) cumulative welfare use (\\(\\omega\\)). Below we define a struct called model_data that contains all of the exogenous state variables that are taken as given for each individual in the data. The struct is defined in /children-cash-transfers/src/model.jl, but we repeat the definition here:\n\nstruct model_data\n    T::Int64 #&lt;- length of problem\n\n    y0::Int64 #&lt;- year to begin problem\n    age0::Int64 # &lt;- mother's age at start of problem\n    SOI::Vector{Int64} #&lt;- state SOI in each year\n    num_kids::Vector{Int64} #&lt;- number of kids in household that are between age 0 and 17\n    TotKids::Int64 #&lt;- indicares the total number of children that the mother will have over the available panel\n    age_kid::Matrix{Int64} #&lt; age_kid[f,t] is the age of child f at time t. Will be negative if child not born yet.\n    cpi::Vector{Float64} #&lt;- cpi\n\n    R::Vector{Int64} #&lt;- indicates if work requirement in time t\n    Kω::Int64 #&lt;- indicates length of time limit once introduced\n    TL::Vector{Bool} #&lt;- indicating that time limit is in place\nend\n\nEventually we will have one of these objects for every mother we observe in the data, and we’ll solve the resulting dynamic program for each of them. To test our functions below we can create a test version of this struct as well as some default parameters:\n\nmd = test_model()\np = pars(2,5) #&lt;- set Kτ = 2 and Kε = 5\n\n\n\n\nThe function nested_logit takes the value of each choice vj and fills log-choice probabilities into a pre-allocated vector logP. It also returns the inclusive value (the “emax” or continuation value). The input \\(B=(B_1,B_2,...,B_L)\\) specifies the partitions in each layer of the tree, while the input \\(C\\) reports the final choices that are ultimately contained in each node in each layer. By definition, at the highest layer, the partition takes the trivial form \\(B_L=\\{\\{1,2,...K_L\\}\\}\\) where \\(K_L\\) is the number of partitions in layer \\(L-1\\). Similarly at the lowest layer, \\(C_{1}\\) must take the form \\(C_{1} = \\{\\{1\\},\\{2\\},...,\\{J\\}\\}\\). For this model with three participation choices that lead into an extensive marginal labor supply choice, the structure is:\n\nB₁ = [[1,2],[3,4],[5,6]]\nC₁ = [[1,],[2,],[3,],[4,],[5,],[6,]]\n\nB₂ = [[1,2,3]]\nC₂ = [[1,2],[3,4],[5,6]]\n\nB = (B₁,B₂)\nC = (C₁,C₂)\n\nThis is defined in src/model/choices.jl. This is for your understanding, but these inputs are all given to you so you can use the function naively if that is all your time allows for.\n\n\n\nYou may find the following objects useful for iterating over the state variables. Let \\(k_{\\tau}\\in\\{1,...,K_{\\tau}\\}\\) index latent types, let \\(k_{\\varepsilon}\\in\\{1,...,K_{\\varepsilon}\\}\\) index wage shocks, and let \\(k_{\\omega} = \\omega+1\\) index cumulative time use. The total size of the state space is \\(K=K_\\tau\\times K_{\\varepsilon} \\times K_{\\omega}\\).\nOne way to do simple indexing is to just work with multi-dimensional arrays and build this into every function. However if you want to add state variables later on it will make it cumbersome to change. Another option is to use LinearIndices objects and their converse, CartesianIndices.\nHere’s a demonstration:\n\n# Hypothetical state space dimensions:\nKε = 5\nKτ = 5\nKω = 6\n\nk_idx = LinearIndices((Kτ,Kε,Kω))\nk_inv = CartesianIndices(k_idx)\n\n# To get the aggregate index k, call:\nk = k_idx[2,3,2]\n@show k\n# Then if we have k we can work back with:\nk_tuple = Tuple(k_inv[k])\n@show k_tuple\n\nk = 37\nk_tuple = (2, 3, 2)\n\n\n(2, 3, 2)\n\n\nSo when iterating, you could think about passing around state indices along with instances of these linear and cartesian indices that allow you to convert back and forth.\n\n\n\nIn the paper, wage shocks are parameterized with a single parameter \\(\\pi_{W}\\) that dictates the probability that an individual remains in the same place on the grid space, with symmetric probabilities of moving up or down. The function fε in states_transitions.jl takes the current wage shock kε and the total number of shocks Kε, along with πW and returns two tuples. The first tuple is the set of grid points that are possible and the second is the probability of being in each of those points.\nFor example:\n\nfε(3,5,0.9)\n\n((2, 3, 4), (0.04999999999999999, 0.9, 0.04999999999999999))\n\n\nSo the function is telling me that when I’m in state 3 I can move to states 3, 4, or 5 next period with probabilities (0.05,0.9,0.05). When we are at the bottom or the top of the grid space, the probabilities are slightly different:\n\nfε(1,5,0.9)\n\n((1, 2), (0.04999999999999999, 0.95))\n\n\nWe could have alternatively just written the transition probabilities into a matrix, but this approach essentially limits us to points with positive probabilities and will simplify iteration.\n\n\n\n\nWrite a function calc_vj that calculates the choice-specific value (i.e. the deterministic value of the choice) of a particular choice \\(j\\) in a particular time period \\(t\\) given the state and other exogenous variables. If you are confident you can code this however you like, but given the existing setup you might like to write the function in a way that it takes the following inputs:\n\nj: the discrete choice\nt: the time period in the model\nstate: a tuple that contains the state \\((k_\\tau,k_\\varepsilon,k_\\omega)\\) as well as a linear indexing rule\nV: a vector that contains the continuation value for each state at time \\(t+1\\)\npars: the parameters of the model\nmd: an instance of the model_data object that holds all relevant state variables\n\nVerify that your function works by testing it on the model_data instance created by test_model. Use the @time macro to look at evaluation time and memory allocations.\n\n\n\nWrite a function called iterate! that iterates over all states at time period \\(t\\) and fills in choice probabilities and continuation values for period \\(t\\) in pre-allocated arrays. Again, you can do this however you like but here is a suggested set of inputs:\n\nt: the time period\nlogP: a \\(J \\times K \\times T\\) array of choice probabilities where the function will fill in logP[:,:,t]\nV: a \\(K \\times T\\) array of continuation values\nstate_idx: a named tuple that contains the size of the overall state space, a linear indexing rule that maps \\((k_\\tau,k_{\\varepsilon},k_{\\omega}\\)) to an overall state \\(k\\), and a Cartesian Indexing rule that inverts this mapping\nvj: a \\(J\\)-dimensional vector that, for each state, can be used as a container for the choice-specific values\npars: model parameters\nmd: model_data for the problem\n\nVerify that your function works by testing it on the model_data instance created by test_model. Use the @time macro to look at evaluation time and memory allocations.\n\n\n\nWrite a function called solve! that performs backward induction to calculate continuation values and choice probabilities (storing them in pre-allocated arrays) in every period of the data across the whole state space. As before, some suggested inputs:\n\nlogP: a \\(J\\times K\\times T\\) array for choice probabilities\nV: a \\(K \\times T\\) array for continuation values\nvj: a container or buffer for choice-specific values in each iteration\npars: model parameters\nmd: an instance of model_data\n\nVerify that your function works by testing it on the model_data instance created by test_model. Use the @time macro to look at evaluation time and memory allocations."
  },
  {
    "objectID": "assignments/Assignment-3.html#source-code-for-the-model",
    "href": "assignments/Assignment-3.html#source-code-for-the-model",
    "title": "Assignment 3 - Solving the Dynamic Program",
    "section": "",
    "text": "Since you only have a week, I’m not going to make you code every piece of the model solution. The folder /children-cash-transfers/src contains:\n\nfunctions to calculate utility and net income from each choice\nan indexing rule that maps choices to hours and participation\nthe structure for the nested logit and code to calculate nested logit probabilities and inclusive values\nfunctions to calculate transition probabilities\ncode to define a default set of parameters and hyperparameters (the number of wage shocks, number of types, etc)\n\nIf you have cloned the course git repo, you can load all of this source code as follows:\n\ninclude(\"../children-cash-transfers/src/model.jl\")\n\nplain_logit (generic function with 1 method)\n\n\nIt will help to quickly explain some of this, but I recommend you read the code carefully since I won’t explain the functions in depth.\n\n\nRecall that in the model there are only three state variables to track: (1) the individual’s type (\\(k\\)); (2) the wage shock (\\(\\varepsilon\\)); and (3) cumulative welfare use (\\(\\omega\\)). Below we define a struct called model_data that contains all of the exogenous state variables that are taken as given for each individual in the data. The struct is defined in /children-cash-transfers/src/model.jl, but we repeat the definition here:\n\nstruct model_data\n    T::Int64 #&lt;- length of problem\n\n    y0::Int64 #&lt;- year to begin problem\n    age0::Int64 # &lt;- mother's age at start of problem\n    SOI::Vector{Int64} #&lt;- state SOI in each year\n    num_kids::Vector{Int64} #&lt;- number of kids in household that are between age 0 and 17\n    TotKids::Int64 #&lt;- indicares the total number of children that the mother will have over the available panel\n    age_kid::Matrix{Int64} #&lt; age_kid[f,t] is the age of child f at time t. Will be negative if child not born yet.\n    cpi::Vector{Float64} #&lt;- cpi\n\n    R::Vector{Int64} #&lt;- indicates if work requirement in time t\n    Kω::Int64 #&lt;- indicates length of time limit once introduced\n    TL::Vector{Bool} #&lt;- indicating that time limit is in place\nend\n\nEventually we will have one of these objects for every mother we observe in the data, and we’ll solve the resulting dynamic program for each of them. To test our functions below we can create a test version of this struct as well as some default parameters:\n\nmd = test_model()\np = pars(2,5) #&lt;- set Kτ = 2 and Kε = 5\n\n\n\n\nThe function nested_logit takes the value of each choice vj and fills log-choice probabilities into a pre-allocated vector logP. It also returns the inclusive value (the “emax” or continuation value). The input \\(B=(B_1,B_2,...,B_L)\\) specifies the partitions in each layer of the tree, while the input \\(C\\) reports the final choices that are ultimately contained in each node in each layer. By definition, at the highest layer, the partition takes the trivial form \\(B_L=\\{\\{1,2,...K_L\\}\\}\\) where \\(K_L\\) is the number of partitions in layer \\(L-1\\). Similarly at the lowest layer, \\(C_{1}\\) must take the form \\(C_{1} = \\{\\{1\\},\\{2\\},...,\\{J\\}\\}\\). For this model with three participation choices that lead into an extensive marginal labor supply choice, the structure is:\n\nB₁ = [[1,2],[3,4],[5,6]]\nC₁ = [[1,],[2,],[3,],[4,],[5,],[6,]]\n\nB₂ = [[1,2,3]]\nC₂ = [[1,2],[3,4],[5,6]]\n\nB = (B₁,B₂)\nC = (C₁,C₂)\n\nThis is defined in src/model/choices.jl. This is for your understanding, but these inputs are all given to you so you can use the function naively if that is all your time allows for.\n\n\n\nYou may find the following objects useful for iterating over the state variables. Let \\(k_{\\tau}\\in\\{1,...,K_{\\tau}\\}\\) index latent types, let \\(k_{\\varepsilon}\\in\\{1,...,K_{\\varepsilon}\\}\\) index wage shocks, and let \\(k_{\\omega} = \\omega+1\\) index cumulative time use. The total size of the state space is \\(K=K_\\tau\\times K_{\\varepsilon} \\times K_{\\omega}\\).\nOne way to do simple indexing is to just work with multi-dimensional arrays and build this into every function. However if you want to add state variables later on it will make it cumbersome to change. Another option is to use LinearIndices objects and their converse, CartesianIndices.\nHere’s a demonstration:\n\n# Hypothetical state space dimensions:\nKε = 5\nKτ = 5\nKω = 6\n\nk_idx = LinearIndices((Kτ,Kε,Kω))\nk_inv = CartesianIndices(k_idx)\n\n# To get the aggregate index k, call:\nk = k_idx[2,3,2]\n@show k\n# Then if we have k we can work back with:\nk_tuple = Tuple(k_inv[k])\n@show k_tuple\n\nk = 37\nk_tuple = (2, 3, 2)\n\n\n(2, 3, 2)\n\n\nSo when iterating, you could think about passing around state indices along with instances of these linear and cartesian indices that allow you to convert back and forth.\n\n\n\nIn the paper, wage shocks are parameterized with a single parameter \\(\\pi_{W}\\) that dictates the probability that an individual remains in the same place on the grid space, with symmetric probabilities of moving up or down. The function fε in states_transitions.jl takes the current wage shock kε and the total number of shocks Kε, along with πW and returns two tuples. The first tuple is the set of grid points that are possible and the second is the probability of being in each of those points.\nFor example:\n\nfε(3,5,0.9)\n\n((2, 3, 4), (0.04999999999999999, 0.9, 0.04999999999999999))\n\n\nSo the function is telling me that when I’m in state 3 I can move to states 3, 4, or 5 next period with probabilities (0.05,0.9,0.05). When we are at the bottom or the top of the grid space, the probabilities are slightly different:\n\nfε(1,5,0.9)\n\n((1, 2), (0.04999999999999999, 0.95))\n\n\nWe could have alternatively just written the transition probabilities into a matrix, but this approach essentially limits us to points with positive probabilities and will simplify iteration."
  },
  {
    "objectID": "assignments/Assignment-3.html#part-1",
    "href": "assignments/Assignment-3.html#part-1",
    "title": "Assignment 3 - Solving the Dynamic Program",
    "section": "",
    "text": "Write a function calc_vj that calculates the choice-specific value (i.e. the deterministic value of the choice) of a particular choice \\(j\\) in a particular time period \\(t\\) given the state and other exogenous variables. If you are confident you can code this however you like, but given the existing setup you might like to write the function in a way that it takes the following inputs:\n\nj: the discrete choice\nt: the time period in the model\nstate: a tuple that contains the state \\((k_\\tau,k_\\varepsilon,k_\\omega)\\) as well as a linear indexing rule\nV: a vector that contains the continuation value for each state at time \\(t+1\\)\npars: the parameters of the model\nmd: an instance of the model_data object that holds all relevant state variables\n\nVerify that your function works by testing it on the model_data instance created by test_model. Use the @time macro to look at evaluation time and memory allocations."
  },
  {
    "objectID": "assignments/Assignment-3.html#part-2",
    "href": "assignments/Assignment-3.html#part-2",
    "title": "Assignment 3 - Solving the Dynamic Program",
    "section": "",
    "text": "Write a function called iterate! that iterates over all states at time period \\(t\\) and fills in choice probabilities and continuation values for period \\(t\\) in pre-allocated arrays. Again, you can do this however you like but here is a suggested set of inputs:\n\nt: the time period\nlogP: a \\(J \\times K \\times T\\) array of choice probabilities where the function will fill in logP[:,:,t]\nV: a \\(K \\times T\\) array of continuation values\nstate_idx: a named tuple that contains the size of the overall state space, a linear indexing rule that maps \\((k_\\tau,k_{\\varepsilon},k_{\\omega}\\)) to an overall state \\(k\\), and a Cartesian Indexing rule that inverts this mapping\nvj: a \\(J\\)-dimensional vector that, for each state, can be used as a container for the choice-specific values\npars: model parameters\nmd: model_data for the problem\n\nVerify that your function works by testing it on the model_data instance created by test_model. Use the @time macro to look at evaluation time and memory allocations."
  },
  {
    "objectID": "assignments/Assignment-3.html#part-3",
    "href": "assignments/Assignment-3.html#part-3",
    "title": "Assignment 3 - Solving the Dynamic Program",
    "section": "",
    "text": "Write a function called solve! that performs backward induction to calculate continuation values and choice probabilities (storing them in pre-allocated arrays) in every period of the data across the whole state space. As before, some suggested inputs:\n\nlogP: a \\(J\\times K\\times T\\) array for choice probabilities\nV: a \\(K \\times T\\) array for continuation values\nvj: a container or buffer for choice-specific values in each iteration\npars: model parameters\nmd: an instance of model_data\n\nVerify that your function works by testing it on the model_data instance created by test_model. Use the @time macro to look at evaluation time and memory allocations."
  },
  {
    "objectID": "assignments/Assignment-2-solutions.html",
    "href": "assignments/Assignment-2-solutions.html",
    "title": "Assignment 2 - Solutions",
    "section": "",
    "text": "Read in the data as before.\nusing CSV, DataFrames, DataFramesMeta, Optim\ninclude(\"../children-cash-transfers/src/Transfers.jl\")\n\n# for this simple model we can just drop missing data. When we estimate the model with persistent latent heterogeneity, we will need complete panels (including years where choices are missing).\n\ndata = @chain begin\n    CSV.read(\"../children-cash-transfers/data/MainPanelFile.csv\",DataFrame,missingstring = \"NA\")\n    #@select :MID :year :wage :hrs :earn :SOI :CPIU :WelfH :FSInd :num_child :age\n    @subset :year.&gt;=1985 :year.&lt;=2010\n    @transform :AFDC = :WelfH.&gt;0 \n    @rename :FS = :FSInd\n    @transform :P  = :FS + :AFDC :H = min.(2,round.(Union{Int64, Missing},:hrs / (52*20)))\n    @subset .!ismissing.(:P) .&& .!ismissing.(:H)\n    @transform @byrow :wage = begin\n        if :hrs&gt;0 && :earn&gt;0\n            return :earn / :hrs / :CPIU\n        else\n            return missing\n        end\n    end\nend\n\ndata_mle = (;P = Int64.(data.P), H = Int64.(data.H), year = data.year, age = data.age,\n            soi = data.SOI, num_kids = data.num_child, cpi = data.CPIU,\n            logwage = log.(coalesce.(data.wage,1.)),wage_missing = ismissing.(data.wage))\n\n(P = [2, 2, 2, 1, 0, 0, 0, 2, 1, 0  …  2, 2, 1, 2, 0, 0, 1, 1, 1, 1], H = [0, 0, 0, 0, 0, 0, 0, 0, 0, 2  …  0, 0, 2, 2, 2, 2, 0, 0, 0, 0], year = [1994, 1995, 1996, 1998, 2000, 2002, 2004, 2006, 2008, 1990  …  1991, 1992, 1991, 1992, 1991, 1992, 1991, 1992, 1991, 1992], age = [21, 22, 23, 25, 27, 29, 31, 33, 35, 17  …  21, 22, 23, 24, 39, 40, 30, 31, 25, 26], soi = [43, 43, 43, 43, 43, 43, 43, 43, 43, 17  …  44, 44, 7, 7, 5, 5, 39, 39, 39, 39], num_kids = [2, 2, 3, 3, 3, 3, 3, 3, 3, 1  …  2, 2, 2, 2, 3, 3, 4, 4, 2, 2], cpi = [0.860812349005761, 0.884959812302546, 0.910948243820851, 0.946664188812488, 1.0, 1.04457233785542, 1.09707768072849, 1.17054218546739, 1.25008130459022, 0.758792510685746  …  0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336], logwage = [0.0, 0.0, 0.0, 0.0, 1.2039728043259361, 0.0, 0.0, 0.0, 0.0, 0.8230555833449215  …  0.0, 0.0, 2.6512574120409043, 2.3314412292534223, 1.265948758245773, 1.8744481199656744, 0.0, 0.0, 0.0, 0.0], wage_missing = Bool[1, 1, 1, 1, 0, 1, 1, 1, 1, 0  …  1, 1, 0, 0, 0, 0, 1, 1, 1, 1])\nUtility and indexing:\nj_idx(p,h) = p*3 + h + 1\nfunction utility(p,h,soi,cpi,year,num_kids,wage,pars)\n    (;αl,Hgrid,σ,αP) = pars\n    hrs = Hgrid[1+h]\n    earn = wage * hrs\n    net_income = max(50.,Transfers.budget(earn,0.,soi,year,num_kids,cpi,p))\n    return log(net_income) + αl*log(112-hrs) - αP[1]*(p&gt;0) - αP[2]*(p&gt;1)\nend\n\npars = (;αl = 1., σ = 1., σW = 1., γ = zeros(3),\n    Hgrid = [0,20.,40.],αP = zeros(2))\n\n(αl = 1.0, σ = 1.0, σW = 1.0, γ = [0.0, 0.0, 0.0], Hgrid = [0.0, 20.0, 40.0], αP = [0.0, 0.0])\nChoice probabilities are given by the logit formula.\nfunction choice_prob!(logP,it,data,pars)\n    (;σ,γ) = pars\n    wage = exp(γ[1] + γ[2] * data.age[it] + γ[3] * data.age[it]^2)\n    denom = 0.\n    umax = -Inf\n    for p in 0:2\n        for h in 0:2\n            j = j_idx(p,h)\n            u = utility(p,h,data.soi[it],data.cpi[it],data.year[it],data.num_kids[it],wage,pars)\n            logP[j] = u / σ\n            umax = u&gt;umax ? u : umax\n        end\n    end\n    logP[:] .-= umax / σ\n    denom = log(sum(exp.(logP)))\n    logP[:] .-= denom #&lt;- nornalize choice probabilities\nend\n\nchoice_prob! (generic function with 1 method)\nAnd so the log-likelihood:\nfunction log_likelihood(logP,it,data,pars)\n    (;γ,σW) = pars\n    ll = 0.\n    if !data.wage_missing[it]\n        ll += -0.5 * ((data.logwage[it] - γ[1] - γ[2]*data.age[it] - γ[3]*data.age[it]^2) / σW)^2 - log(σW)\n    end\n    choice_prob!(logP,it,data,pars)\n    j = j_idx(data.P[it],data.H[it])\n    ll += logP[j]\n    return ll\nend\n\nfunction update(x,p)\n    αl = exp(x[1])\n    σ = exp(x[2])\n    γ = x[3:5]\n    σW = exp(x[6])\n    αP = x[7:8]\n    return (;p...,αl,σ,σW,αP,γ)\nend\n\nfunction log_likelihood(x,data,pars)\n    pars = update(x,pars)\n    logP = zeros(eltype(x),9)\n    ll = 0.\n    for it in eachindex(data.P)\n        ll += log_likelihood(logP,it,data,pars)\n    end\n    return ll\nend\n\nlog_likelihood (generic function with 2 methods)\nWe can use Optim to maximize the log-likelihood:\nx0 = zeros(8)\nx0 = [0.,0.,log(10.),0.,0.,0.,0.,0.]\nres = optimize(x-&gt;-log_likelihood(x,data_mle,pars),x0,BFGS(),autodiff=:forward,Optim.Options(show_trace = true))\npars = update(res.minimizer,pars)\n\nIter     Function value   Gradient norm \n     0     7.234596e+04     3.085114e+06\n * time: 0.0063669681549072266\n     1     7.219007e+04     7.930873e+05\n * time: 0.4502248764038086\n     2     7.189756e+04     8.240034e+05\n * time: 0.5542259216308594\n     3     6.311750e+04     1.040038e+06\n * time: 0.789099931716919\n     4     5.773072e+04     3.637938e+06\n * time: 1.1824579238891602\n     5     5.738226e+04     6.079719e+06\n * time: 1.339035987854004\n     6     5.692685e+04     7.218571e+06\n * time: 1.5397050380706787\n     7     5.644027e+04     8.094668e+06\n * time: 1.91392183303833\n     8     5.629959e+04     8.406176e+06\n * time: 1.9684689044952393\n     9     5.408423e+04     4.483122e+06\n * time: 2.005316972732544\n    10     5.315575e+04     3.603074e+06\n * time: 2.042008876800537\n    11     5.209566e+04     2.625888e+05\n * time: 2.097062826156616\n    12     5.195541e+04     2.083292e+05\n * time: 2.1336588859558105\n    13     5.188423e+04     6.807290e+05\n * time: 2.170222043991089\n    14     5.187199e+04     2.396172e+05\n * time: 2.2251899242401123\n    15     5.186933e+04     5.094151e+03\n * time: 2.2616169452667236\n    16     5.186887e+04     7.774766e+03\n * time: 2.3174760341644287\n    17     5.186878e+04     2.823744e+04\n * time: 2.3776559829711914\n    18     5.186871e+04     1.085063e+04\n * time: 2.4334499835968018\n    19     5.186852e+04     2.080594e+04\n * time: 2.5089468955993652\n    20     5.186850e+04     5.468883e+02\n * time: 2.5645508766174316\n    21     5.186850e+04     4.523469e+00\n * time: 2.619621992111206\n    22     5.186850e+04     8.466642e-01\n * time: 2.675424814224243\n    23     5.186850e+04     3.014776e-02\n * time: 2.7305779457092285\n    24     5.186850e+04     1.164486e-03\n * time: 2.7672388553619385\n    25     5.186850e+04     1.957495e-05\n * time: 2.8266618251800537\n    26     5.186850e+04     1.250626e-07\n * time: 2.865818977355957\n    27     5.186850e+04     1.206905e-08\n * time: 2.9028689861297607\n    28     5.186850e+04     3.893547e-09\n * time: 2.9430859088897705\n\n\n(αl = 2.8546233783929473, σ = 0.9509286759263859, σW = 0.7224860062936598, γ = [-0.023620750582789916, 0.10877398375254817, -0.0012722347394312002], Hgrid = [0.0, 20.0, 40.0], αP = [2.035301636135555, 1.0137619542631466])"
  },
  {
    "objectID": "assignments/Assignment-2-solutions.html#question-1",
    "href": "assignments/Assignment-2-solutions.html#question-1",
    "title": "Assignment 2 - Solutions",
    "section": "Question 1",
    "text": "Question 1\nLet’s use the Hessian of the average log-likelihood to estimate the standard errors.\n\nusing ForwardDiff\nN = length(data_mle.age)\nH = ForwardDiff.hessian(x-&gt;log_likelihood(x,data_mle,pars),res.minimizer) / N\nV = - H / N\n\n8×8 Matrix{Float64}:\n  1.04029e-5    4.11235e-6   -6.42072e-6  …  -2.46919e-6  -1.15412e-6\n  4.11235e-6    1.95866e-5   -1.25177e-5     -1.15811e-5  -5.76085e-6\n -6.42072e-6   -1.25177e-5    6.01038e-5      5.73597e-6   2.50154e-6\n -0.000199471  -0.000389124   0.00193279      0.00017803   7.68593e-5\n -0.00665377   -0.0130835     0.0662433       0.00587496   0.00250189\n -0.0          -0.0           5.82263e-6  …  -0.0         -0.0\n -2.46919e-6   -1.15811e-5    5.73597e-6      9.58444e-6   3.41442e-6\n -1.15412e-6   -5.76085e-6    2.50154e-6      3.41442e-6   4.67709e-6\n\n\nNow we can make the table, but we have to be careful because some of our parameters are transformations of the vector res.minimizer (so we use the delta method to get the standard error of the transformed value)\n\nusing LinearAlgebra\np_str = [\"αl\",\"σ\",\"γ₁\",\"γ₂\",\"γ₃\",\"σW\",\"αP₁\",\"αP₂\"]\np_est = [pars.αl ; pars.σ ; pars.γ ; pars.σW ; pars.αP]\nse = sqrt.(diag(V))\np_se = [se[1:2].*p_est[1:2] ; se[3:5] ; se[6].*p_est[6] ; se[7:8]]\nd = DataFrame(par = p_str, est = p_est, se = p_se)\n\n8×3 DataFrame\n\n\n\nRow\npar\nest\nse\n\n\n\nString\nFloat64\nFloat64\n\n\n\n\n1\nαl\n2.85462\n0.00920715\n\n\n2\nσ\n0.950929\n0.00420851\n\n\n3\nγ₁\n-0.0236208\n0.00775266\n\n\n4\nγ₂\n0.108774\n0.257378\n\n\n5\nγ₃\n-0.00127223\n9.6305\n\n\n6\nσW\n0.722486\n0.00529469\n\n\n7\nαP₁\n2.0353\n0.00309587\n\n\n8\nαP₂\n1.01376\n0.00216266"
  },
  {
    "objectID": "assignments/Assignment-2-solutions.html#question-2",
    "href": "assignments/Assignment-2-solutions.html#question-2",
    "title": "Assignment 2 - Solutions",
    "section": "Question 2",
    "text": "Question 2\nWhat sources of variation are identifying \\(\\sigma\\), the responsiveness of choices to changes in the payoffs? How is the responsivess of program participation and labor supply to changes in incentives limited in this framework?\nSome comments we went over in class:\n\n\\(\\sigma\\) is identified to by covariation in the probability of making particular decisions with changes in the financial returns to those decisions, these in turn are driven by\nThe responsiveness of labor supply and program participation are therefore both dictated by \\(\\sigma\\)"
  },
  {
    "objectID": "assignments/Assignment-2-solutions.html#question-3",
    "href": "assignments/Assignment-2-solutions.html#question-3",
    "title": "Assignment 2 - Solutions",
    "section": "Question 3",
    "text": "Question 3\nFirst we’ll write the function to calculate nested logit probabilities given a vector of utilities u and a partition B.\nIn the next problem set I’ll introduce a function for evaluating the nested logit for a general number of layers. The function below will work for any nested logit with only two layers (i.e. one partition).\n\nfunction nested_logit(logP,u,σ,B)\n    # calculate nest probabilities and inclusive values\n    for k in eachindex(B)\n        Bₖ = B[k]\n        # find the max\n        vmax = -Inf\n        for j in Bₖ\n            vmax = u[j] &gt; vmax ? u[j] : vmax\n        end\n        # calculate choice probs\n        iv = 0.\n        for j in Bₖ\n            logP[j] += (u[j] - vmax) / σ[2]\n            iv += exp((u[j] - vmax) / σ[2])\n        end\n        logP[Bₖ] .-= log(iv)\n        u[k] = vmax + σ[2] * log(iv) #&lt;- fill in inclusive value\n    end\n    # now calculate the partition probabilities\n    vmax = -Inf\n    for k in eachindex(B)\n        vmax = u[k] &gt; vmax ? u[k] : vmax\n    end\n    iv = 0.\n    for k in eachindex(B)\n        iv += exp((u[k]-vmax) / σ[1])\n        for j in B[k]\n            logP[j] += (u[k]-vmax) / σ[1]\n        end\n    end\n    logP[:] .-= log(iv)\n    V = vmax + σ[1] * log(iv)\nend\n\nnested_logit (generic function with 1 method)\n\n\nNow let’s write a new set of log-likelihood routines.\n\nfunction fill_utilities!(u,it,data,pars)\n    (;γ) = pars\n    wage = exp(γ[1] + γ[2]*data.age[it] + γ[3]*data.age[it]^2)\n    for p in 0:2\n        for h in 0:2\n            j = j_idx(p,h)\n            u[j] = utility(p,h,data.soi[1],data.cpi[1],data.year[1],data.num_kids[1],wage,pars)\n        end\n    end\nend\nfunction log_likelihood(logP,u,it,data,pars)\n    (;γ,σW,σ,B) = pars\n    ll = 0.\n    if !data.wage_missing[it]\n        ll += -0.5 * ((data.logwage[it] - γ[1] - γ[2]*data.age[it] - γ[3]*data.age[it]^2) / σW)^2 - log(σW)\n    end\n    fill_utilities!(u,it,data,pars)\n    V = nested_logit(logP,u,σ,B)\n    j = j_idx(data.P[it],data.H[it])\n    ll += logP[j]\n    return ll\nend\n\nfunction update(x,p)\n    αl = exp(x[1])\n    σ = exp.(x[2:3])\n    γ = x[4:6]\n    σW = exp(x[7])\n    αP = x[8:9]\n    return (;p...,αl,σ,σW,αP,γ)\nend\n\nfunction log_likelihood(x,data,pars)\n    pars = update(x,pars)\n    logP = zeros(eltype(x),9)\n    u = zeros(eltype(x),9)\n    ll = 0.\n    for it in eachindex(data.P)\n        fill!(logP,0.)\n        ll += log_likelihood(logP,u,it,data,pars)\n    end\n    return ll\nend\n\nlog_likelihood (generic function with 3 methods)\n\n\nAnd here is code to maximize the log-likelihood. Notice that we get very different participation elasticities vs labor force elasticities.\n\nB = [[1,2,3],[4,5,6],[7,8,9]]\npars = (;αl = 1., σ = [1.,1.], σW = 1., γ = zeros(3),\n    Hgrid = [0,20.,40.],αP = zeros(2), B)\nx0 = zeros(9)\nx0 = [0.,0.,log(10.),0.,0.,0.,0.,0.,0.]\nres = optimize(x-&gt;-log_likelihood(x,data_mle,pars),x0,BFGS(),autodiff=:forward,Optim.Options(show_trace = true))\npars = update(res.minimizer,pars)\n\nIter     Function value   Gradient norm \n     0     1.298462e+05     4.851678e+07\n * time: 8.678436279296875e-5\n     1     8.428536e+04     6.296835e+05\n * time: 0.23514485359191895\n     2     7.391455e+04     5.520307e+05\n * time: 0.38832688331604004\n     3     6.370511e+04     4.396082e+06\n * time: 0.6106898784637451\n     4     6.345510e+04     1.117865e+06\n * time: 0.7055327892303467\n     5     5.652808e+04     3.601002e+06\n * time: 1.1712467670440674\n     6     5.644734e+04     3.879931e+06\n * time: 1.3372769355773926\n     7     5.637478e+04     4.047389e+06\n * time: 1.728111982345581\n     8     5.631564e+04     4.429878e+06\n * time: 2.0891449451446533\n     9     5.625670e+04     4.426924e+06\n * time: 2.5212409496307373\n    10     5.621568e+04     4.487090e+06\n * time: 2.5944719314575195\n    11     5.445576e+04     5.568793e+06\n * time: 2.6421899795532227\n    12     5.321236e+04     3.804673e+06\n * time: 2.690370798110962\n    13     5.309794e+04     2.771147e+06\n * time: 2.7412807941436768\n    14     5.263932e+04     1.965296e+06\n * time: 2.790564775466919\n    15     5.251223e+04     1.026082e+06\n * time: 2.841972827911377\n    16     5.249235e+04     4.504616e+05\n * time: 2.8927547931671143\n    17     5.248819e+04     5.498308e+04\n * time: 2.944981813430786\n    18     5.248768e+04     6.342023e+04\n * time: 2.994086980819702\n    19     5.248734e+04     6.166998e+02\n * time: 3.0707948207855225\n    20     5.248377e+04     2.627173e+05\n * time: 3.1757378578186035\n    21     5.247479e+04     3.265779e+05\n * time: 3.2517528533935547\n    22     5.247121e+04     3.335803e+05\n * time: 3.3013668060302734\n    23     5.246340e+04     2.918189e+05\n * time: 3.3502278327941895\n    24     5.245355e+04     2.052197e+05\n * time: 3.4001967906951904\n    25     5.244366e+04     1.847566e+05\n * time: 3.4750797748565674\n    26     5.243858e+04     4.394642e+04\n * time: 3.5480167865753174\n    27     5.243507e+04     6.597019e+04\n * time: 3.5967459678649902\n    28     5.243380e+04     1.882748e+04\n * time: 3.6472280025482178\n    29     5.243337e+04     4.479266e+04\n * time: 3.6955578327178955\n    30     5.243331e+04     1.261248e+03\n * time: 3.7441039085388184\n    31     5.243331e+04     4.613115e+02\n * time: 3.7943758964538574\n    32     5.243331e+04     2.374307e+02\n * time: 3.8450160026550293\n    33     5.243331e+04     7.004367e+00\n * time: 3.896716833114624\n    34     5.243331e+04     6.215960e-01\n * time: 3.9466938972473145\n    35     5.243331e+04     9.749320e-02\n * time: 3.9989638328552246\n    36     5.243331e+04     4.051009e-03\n * time: 4.047600984573364\n    37     5.243331e+04     6.876542e-06\n * time: 4.096721887588501\n    38     5.243331e+04     2.178613e-07\n * time: 4.168418884277344\n    39     5.243331e+04     1.537637e-08\n * time: 4.217536926269531\n    40     5.243331e+04     7.319159e-09\n * time: 4.26809287071228\n\n\n(αl = 2.8294318298128394, σ = [0.37004828541787427, 1.3098778384091117], σW = 0.7186963786509343, γ = [0.672930718912166, 0.06812663769542579, -0.0006981805671947965], Hgrid = [0.0, 20.0, 40.0], αP = [1.5170804560061646, 0.5324967821971897], B = [[1, 2, 3], [4, 5, 6], [7, 8, 9]])"
  },
  {
    "objectID": "assignments/Assignment-2-solutions.html#question-4",
    "href": "assignments/Assignment-2-solutions.html#question-4",
    "title": "Assignment 2 - Solutions",
    "section": "Question 4",
    "text": "Question 4\nWe discussed this in class but let me know if you would like this to be discussed further."
  },
  {
    "objectID": "assignments/Assignment-4.html",
    "href": "assignments/Assignment-4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "This week you will estimate a simple hidden markov model using Expectation-Maximization.\nHere is the model. There is a discrete state variable \\(k\\in\\{1,2,3,..,K\\}\\) and a binary outcome \\(j\\in\\{0,1\\}\\) that:\nLet \\(p\\) be a \\(K\\)-dimensional vector where \\(p_{k}\\) holds the probability that \\(j=1\\) given that the model is in state \\(k\\).\nThe state \\(k\\) is never observed and the outcome \\(j\\) is only observed half the time (i.e. it is missing with probability 0.5). Thus, define \\(j^*\\) to be:\n\\[ j^* = \\left\\{ \\begin{array}{cl} j & \\text{with probability}\\ 0.5 \\\\ -1 & \\text{with probability}\\ 0.5 \\end{array} \\right. \\]\nYour task is to estimate the vector of outcome probabilities \\(p\\) non-parametrically using the EM algorithm with the Forward-Back routine to calculate the E-step.\nThe chunk of code below parameterizes the true vector \\(p\\) for this model and writes code to simulate panel data:\n\\[ (j^*_{nt})_{t=1,n=1}^{T,N} \\]\nTo start with, we’ll assume \\(k_{n,1} = 1\\) for all \\(n\\).\nusing Random, Distributions\nK = 5\nPj = 1 ./ (1 .+ exp.(LinRange(-1,1,K))) #&lt;- choice probability as a function of k\nknext(k,j,K) = min(K,k+j) \n\nfunction simulate(N,T,Pj)\n    J = zeros(T,N)\n    K = length(Pj)\n    for n in axes(J,2)\n        k = 1\n        for t in axes(J,1)\n            j = rand()&lt;Pj[k]\n            # record j probabilistically\n            if rand()&lt;0.5\n                J[t,n] = -1\n            else\n                J[t,n] = j\n            end\n            # update state:\n            k = knext(k,j,K)\n        end\n    end\n    return J\nend\n\nJ_data = simulate(1000,10,Pj)\n\n10×1000 Matrix{Float64}:\n  1.0   1.0  -1.0  -1.0  -1.0  -1.0  …  -1.0   1.0   0.0   1.0   1.0  -1.0\n -1.0   1.0  -1.0   0.0  -1.0  -1.0      1.0   1.0  -1.0   0.0  -1.0  -1.0\n -1.0  -1.0   1.0   0.0  -1.0  -1.0      1.0  -1.0   0.0   0.0  -1.0  -1.0\n  1.0  -1.0  -1.0   1.0   1.0  -1.0     -1.0  -1.0  -1.0  -1.0  -1.0  -1.0\n  0.0  -1.0   1.0  -1.0  -1.0   1.0     -1.0   1.0  -1.0   1.0  -1.0   1.0\n  0.0   1.0  -1.0   1.0  -1.0  -1.0  …   1.0   0.0   0.0  -1.0   0.0   1.0\n  0.0  -1.0   1.0   0.0  -1.0  -1.0     -1.0  -1.0  -1.0   1.0   0.0  -1.0\n -1.0  -1.0  -1.0   0.0   0.0  -1.0      0.0  -1.0  -1.0  -1.0   1.0  -1.0\n -1.0  -1.0   0.0   0.0  -1.0  -1.0     -1.0  -1.0   0.0   1.0  -1.0  -1.0\n -1.0  -1.0   1.0   0.0   0.0  -1.0      0.0  -1.0   0.0   0.0  -1.0   1.0\nSince the outcomes \\(j\\) are periodically unobserved, there are two ways to set up the EM problem:\nWhen the number of discrete outcomes is larger, it becomes more difficult to integrate them out when missing. Since \\(j\\) is only binary here, this homework will step you through using the second approach. You can use the first approach if you prefer."
  },
  {
    "objectID": "assignments/Assignment-4.html#part-1",
    "href": "assignments/Assignment-4.html#part-1",
    "title": "Assignment 4",
    "section": "Part 1",
    "text": "Part 1\nWrite a function that (given a guess of the parameters \\(p\\)) takes a sequence of \\((j^*)^{T}_{t=1}\\) and runs the forward-back algorithm. In doing so your function should fill in three objects: (1) A \\(K\\times T\\) array of backward looking probabilities (\\(\\alpha\\)); (2) a \\(K\\times T\\) array of forward probabilities (\\(\\beta\\)); and (3) a \\(K\\times T\\) array of posterior probabilities over each state \\(k\\) (\\(Q\\)).\nRecall from class that (1) \\(\\alpha\\) is the joint probability of the state today with the sequence of outcomes up until today; and (2) \\(\\beta\\) is the conditional probability of future outcomes given the state today.\n\\[ \\alpha[k,s] = \\mathbb{P}[k_{s}=k,(j^*)^{s}_{t=1}] \\]\n\\[ \\beta[k,s] = \\mathbb{P}[(j^*)_{t=s+1}^{T} | k_{s}=k] \\]\nand\n\\[ Q[k,s] = \\mathbb{P}[k_{s}=k | (j^*)_{t=1}^{T} ] \\]"
  },
  {
    "objectID": "assignments/Assignment-4.html#part-2",
    "href": "assignments/Assignment-4.html#part-2",
    "title": "Assignment 4",
    "section": "Part 2",
    "text": "Part 2\nWrite a function that iterates over all observations and calculates posterior state probabilities for every observation. i.e. fill in a \\(K\\times T \\times N\\) array of posterior weights."
  },
  {
    "objectID": "assignments/Assignment-4.html#part-3",
    "href": "assignments/Assignment-4.html#part-3",
    "title": "Assignment 4",
    "section": "Part 3",
    "text": "Part 3\nTake as given a set of posterior weights \\(q_{ntk} = Q[n,t,k]\\). The expected log-likelihood for the M-step is:\n\\[ \\mathcal{L}(p) = \\sum_{n}\\sum_{t}\\sum_{k}q_{ntk}\\left(\\mathbf{1}\\{j^*_{nt}=1\\}\\log(p_{k}) + \\mathbf{1}\\{j^*_{nt}=0\\}\\log(1-p_{k})\\right) \\]\nShow that the non-parametric maximum likelihood estimate of \\(p\\) given the posterior weights is a frequency estimator:\n\\[ \\hat{p}_{k} = \\frac{\\sum_{n}\\sum_{t}\\mathbf{1}\\{j^*_{nt}=1\\} q_{ntk}}{\\sum_{n}\\sum_{t}\\mathbf{1}\\{j^*_{nt}\\neq -1\\} q_{ntk}} \\]\nWrite a function to calculate this frequency estimator given posterior weights."
  },
  {
    "objectID": "assignments/Assignment-4.html#part-4",
    "href": "assignments/Assignment-4.html#part-4",
    "title": "Assignment 4",
    "section": "Part 4",
    "text": "Part 4\nRun the E-M routine by iterating on this E-step and M-step until you get convergence in \\(\\hat{p}\\). Does it look like you can recover the true parameters?"
  },
  {
    "objectID": "assignments/Assignment-4.html#extra-credit",
    "href": "assignments/Assignment-4.html#extra-credit",
    "title": "Assignment 4",
    "section": "Extra credit",
    "text": "Extra credit\nIf you found this exercise too straightforward, try adjusting the model so that transitions depend probabilistically on the state \\(k\\) and the choice \\(j\\). This requires you to update also posterior transition probabilities and estimate those transition probabilities as part of the maximization step."
  },
  {
    "objectID": "assignments/Assignment-4-solutions.html",
    "href": "assignments/Assignment-4-solutions.html",
    "title": "Assignment 4",
    "section": "",
    "text": "This week you will estimate a simple hidden markov model using Expectation-Maximization.\nHere is the model. There is a discrete state variable \\(k\\in\\{1,2,3,..,K\\}\\) and a binary outcome \\(j\\in\\{0,1\\}\\) that:\nLet \\(p\\) be a \\(K\\)-dimensional vector where \\(p_{k}\\) holds the probability that \\(j=1\\) given that the model is in state \\(k\\).\nThe state \\(k\\) is never observed and the outcome \\(j\\) is only observed half the time (i.e. it is missing with probability 0.5). Thus, define \\(j^*\\) to be:\n\\[ j^* = \\left\\{ \\begin{array}{cl} j & \\text{with probability}\\ 0.5 \\\\ -1 & \\text{with probability}\\ 0.5 \\end{array} \\right. \\]\nYour task is to estimate the vector of outcome probabilities \\(p\\) non-parametrically using the EM algorithm with the Forward-Back routine to calculate the E-step.\nThe chunk of code below parameterizes the true vector \\(p\\) for this model and writes code to simulate panel data:\n\\[ (j^*_{nt})_{t=1,n=1}^{T,N} \\]\nTo start with, we’ll assume \\(k_{n,1} = 1\\) for all \\(n\\).\nusing Random, Distributions\nK = 5\nT = 10\nPj = 1 ./ (1 .+ exp.(LinRange(-1,1,K))) #&lt;- choice probability as a function of k\nknext(k,j,K) = min(K,k+j) \n\nfunction simulate(N,T,Pj)\n    J = zeros(Int64,T,N)\n    K = length(Pj)\n    for n in axes(J,2)\n        k = 1\n        for t in axes(J,1)\n            j = rand()&lt;Pj[k]\n            # record j probabilistically\n            if rand()&lt;0.5\n                J[t,n] = -1\n            else\n                J[t,n] = j\n            end\n            # update state:\n            k = knext(k,j,K)\n        end\n    end\n    return J\nend\n\nN = 1000\nJ_data = simulate(N,T,Pj)\n\n10×1000 Matrix{Int64}:\n -1  -1   1  -1  -1  -1   0   1   1  …  -1  -1   1   1   0  -1  -1   1   0\n -1   1  -1  -1  -1   0  -1  -1  -1     -1   0   1   1   1   1   1   1   1\n -1  -1  -1   0  -1   0  -1  -1   0      1  -1  -1  -1   1  -1   0   1  -1\n  1  -1   1   1   1   1   1   0  -1      1   1   0   0   1  -1  -1   0   0\n  1   1   1   0   1   1  -1  -1  -1      1   1  -1  -1   0  -1  -1  -1   0\n -1  -1  -1  -1  -1   1  -1  -1   1  …   0   0  -1  -1   1   1  -1   0   0\n  0   1  -1   0   0  -1  -1   0  -1     -1  -1   0  -1  -1  -1   1  -1  -1\n  0   1  -1   0   1   0  -1   1  -1      0  -1   0   0   0  -1  -1   0  -1\n  0   0   0   0  -1   0  -1  -1   0     -1   1  -1  -1   0   0  -1   0   0\n -1  -1   0  -1   0   0  -1   1  -1     -1   0  -1  -1   0  -1   0   0   0\nSince the outcomes \\(j\\) are periodically unobserved, there are two ways to set up the EM problem:\nWhen the number of discrete outcomes is larger, it becomes more difficult to integrate them out when missing. Since \\(j\\) is only binary here, this homework will step you through using the second approach. You can use the first approach if you prefer."
  },
  {
    "objectID": "assignments/Assignment-4-solutions.html#part-1",
    "href": "assignments/Assignment-4-solutions.html#part-1",
    "title": "Assignment 4",
    "section": "Part 1",
    "text": "Part 1\nWrite a function that (given a guess of the parameters \\(p\\)) takes a sequence of \\((j^*)^{T}_{t=1}\\) and runs the forward-back algorithm. In doing so your function should fill in three objects: (1) A \\(K\\times T\\) array of backward looking probabilities (\\(\\alpha\\)); (2) a \\(K\\times T\\) array of forward probabilities (\\(\\beta\\)); and (3) a \\(K\\times T\\) array of posterior probabilities over each state \\(k\\) (\\(Q\\)).\nRecall from class that (1) \\(\\alpha\\) is the joint probability of the state today with the sequence of outcomes up until today; and (2) \\(\\beta\\) is the conditional probability of future outcomes given the state today.\n\\[ \\alpha[k,s] = \\mathbb{P}[k_{s}=k,(j^*)^{s}_{t=1}] \\]\n\\[ \\beta[k,s] = \\mathbb{P}[(j^*)_{t=s+1}^{T} | k_{s}=k] \\]\nand\n\\[ Q[k,s] = \\mathbb{P}[k_{s}=k | (j^*)_{t=1}^{T} ] \\]\nThis is a problem where there is a lot of sparseness. For the size of this problem we won’t have to worry about exploiting that, but it’s something you want to think about for larger problems. I’ll introduce you to a sparse implementation in class and in code.\nSo, ignoring sparsenss, here is a function that gives transition probabilities given the three potential outcomes for \\(j^*\\).\n\nusing LinearAlgebra\n# this function assumes Π is a K x K x 3 array of transition probabilities where\nfunction get_transitions!(Π,p)\n    fill!(Π,0.)\n    # for j^*=-1 (i.e. missing)\n    for k in axes(Π,2)\n        Π[k,k,1] += p[k]\n        k_up = min(K,k+1)\n        Π[k_up,k,1] += 1-p[k]\n    end\n    # for j^* = 0\n    Π[:,:,2] = I(K) #&lt;- transitions are the identity matrix\n    # for j^*=1\n    for k in axes(Π,2)\n        k_up = min(K,k+1)\n        Π[k_up,k,3] = 1.\n    end\nend\n\nΠ = zeros(K,K,3)\nget_transitions!(Π,Pj)\n\nNow a function for the forward-back routine:\n\nfunction forward_back!(Q,α,β,j_obs,Π,p)\n    K,T = size(α)\n    fill!(β,0.)\n    @views fill!(β[:,T],1.)\n    fill!(α,0.)\n    α[1,1] = 1. #&lt;- since we know that all units begin in state 1\n    # run it forward\n    for t in 2:T\n        jlag_idx = 2 + j_obs[t-1]\n        for k in axes(α,1)\n            for klag in axes(α,1)\n                α[k,t] += Π[k,klag,jlag_idx] *  α[klag,t-1]\n            end\n            if j_obs[t]&gt;-1\n                α[k,t] *= j_obs[t]*p[k] + (1-j_obs[t])*(1-p[k])\n            end\n        end\n    end\n    # run it back\n    for t in reverse(1:T-1)\n        j_idx = 2 + j_obs[t]\n        for k in axes(β,1)\n            for knext in axes(β,1)\n                if j_obs[t+1]!=-1\n                    pj = j_obs[t+1]*p[knext] + (1-j_obs[t+1])*(1-p[knext])\n                else\n                    pj = 1.\n                end\n                β[k,t] += pj * Π[knext,k,j_idx]\n            end\n        end\n    end\n    # calculate the posteriors\n    for t in axes(Q,2)\n        @views Q[:,t] .= α[:,t] .* β[:,t]\n        @views Q[:,t] ./= sum(Q[:,t])\n    end\nend\n\nα = zeros(K,T)\nβ = zeros(K,T)\nQ = zeros(K,T)\nforward_back!(Q,α,β,J_data[:,1],Π,Pj)"
  },
  {
    "objectID": "assignments/Assignment-4-solutions.html#part-2",
    "href": "assignments/Assignment-4-solutions.html#part-2",
    "title": "Assignment 4",
    "section": "Part 2",
    "text": "Part 2\nWrite a function that iterates over all observations and calculates posterior state probabilities for every observation. i.e. fill in a \\(K\\times T \\times N\\) array of posterior weights.\nI’ll also add a line of code to update the transition matrices \\(\\Pi\\).\n\nfunction forward_back!(Q,α,β,J_obs::Array{Int64},Π,p)\n    get_transitions!(Π,p)\n    for n in axes(J_obs,2)\n        @views forward_back!(Q[:,:,n],α,β,J_obs[:,n],Π,p)\n    end\nend\n\nQ = zeros(K,T,N)\nforward_back!(Q,α,β,J_data,Π,Pj)"
  },
  {
    "objectID": "assignments/Assignment-4-solutions.html#part-3",
    "href": "assignments/Assignment-4-solutions.html#part-3",
    "title": "Assignment 4",
    "section": "Part 3",
    "text": "Part 3\nTake as given a set of posterior weights \\(q_{ntk} = Q[n,t,k]\\). The expected log-likelihood for the M-step is:\n\\[ \\mathcal{L}(p) = \\sum_{n}\\sum_{t}\\sum_{k}q_{ntk}\\left(\\mathbf{1}\\{j^*_{nt}=1\\}\\log(p_{k}) + \\mathbf{1}\\{j^*_{nt}=0\\}\\log(1-p_{k})\\right) \\]\nShow that the non-parametric maximum likelihood estimate of \\(p\\) given the posterior weights is a frequency estimator:\n\\[ \\hat{p}_{k} = \\frac{\\sum_{n}\\sum_{t}\\mathbf{1}\\{j^*_{nt}=1\\} q_{ntk}}{\\sum_{n}\\sum_{t}\\mathbf{1}\\{j^*_{nt}\\neq -1\\} q_{ntk}} \\]\nWrite a function to calculate this frequency estimator given posterior weights.\n\nfunction m_step(Q,J_data)\n    K = size(Q,1)\n    p = zeros(K)\n    denom = zeros(K)\n    for n in axes(J_data,2)\n        for t in axes(J_data,1)\n            if J_data[t,n]!=-1\n                @views denom .+= Q[:,t,n]\n                if J_data[t,n]==1\n                    @views p .+= Q[:,t,n]\n                end\n            end\n        end\n    end\n    return p ./ denom\nend\n\np_est = m_step(Q,J_data)\n\n5-element Vector{Float64}:\n 0.6893915560871474\n 0.548451505221972\n 0.4505939493128934\n 0.32605335098046356\n 0.24167980459619326"
  },
  {
    "objectID": "assignments/Assignment-4-solutions.html#part-4",
    "href": "assignments/Assignment-4-solutions.html#part-4",
    "title": "Assignment 4",
    "section": "Part 4",
    "text": "Part 4\nRun the E-M routine by iterating on this E-step and M-step until you get convergence in \\(\\hat{p}\\). Does it look like you can recover the true parameters?\n\nfunction EM_routine(p0,Π,data ; max_iter = 1000, tol = 1e-7, verbose = true)\n    (;α,β,Q,J_data) = data\n    err = Inf\n    iter = 0\n    while err&gt;tol && iter&lt;max_iter\n        forward_back!(Q,α,β,J_data,Π,p0)\n        p1 = m_step(Q,J_data)\n        err = maximum(abs.(p1 .- p0))\n        iter += 1\n        if mod(iter,10)==0 && verbose\n            println(\"Current error is $err\")\n        end\n        p0 = p1\n    end\n    return p0\nend\n\ndata = (;α,β,Q,J_data)\n\n# let's try using a \n\np_guess = fill(1 / K, K)\np_est = EM_routine(p_guess,Π,data)\n\n[p_est Pj]\n\nCurrent error is 0.002843866497475034\nCurrent error is 0.0005218130317447001\nCurrent error is 8.811623202636953e-5\nCurrent error is 1.4687127339818584e-5\nCurrent error is 2.4432631483550793e-6\nCurrent error is 4.0632648923288883e-7\n\n\n5×2 Matrix{Float64}:\n 0.701427  0.731059\n 0.524607  0.622459\n 0.496549  0.5\n 0.29034   0.377541\n 0.264751  0.268941\n\n\nLooks good! You could test the estimator a bit more by running it over a number of samples to see how you do.\n\nB = 100\nP_montecarlo = zeros(K,B)\nfor b in axes(P_montecarlo,2)\n    J_data = simulate(N,T,Pj)\n    data = (;data...,J_data)\n    P_montecarlo[:,b] = EM_routine(p_guess,Π,data ; verbose = false)\nend\n\n\nusing Plots\nhistogram(P_montecarlo[1,:],legend = false)\nplot!([Pj[1],Pj[1]],[0,30],linewidth=3,legend = false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see a little bit of bias, but then MLE is not guaranteed to be unbiased. You could re-run this with a larger sample size if you wanted to convince yourself of consistency."
  },
  {
    "objectID": "assignments/Assignment-4-solutions.html#extra-credit",
    "href": "assignments/Assignment-4-solutions.html#extra-credit",
    "title": "Assignment 4",
    "section": "Extra credit",
    "text": "Extra credit\nIf you found this exercise too straightforward, try adjusting the model so that transitions depend probabilistically on the state \\(k\\) and the choice \\(j\\). This requires you to update also posterior transition probabilities and estimate those transition probabilities as part of the maximization step."
  },
  {
    "objectID": "assignments/Assignment-5.html",
    "href": "assignments/Assignment-5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "In this assignment you will implement a choice probability estimator for a toy model of dynamic labor supply.\n\n\nAgents in the model make one of four choices (\\(j \\in \\{0,1,2,3\\}\\)) that correspond to a binary work decision (\\(H_{j}\\)) and a binary program participation decision (\\(P_{j}\\)):\n\\[ \\{H_0,\\ H_1,\\ H_2,\\ H_3\\} = \\{0,1,0,1\\},\\qquad \\{P_0,\\ P_1,\\ P_2,\\ P_3\\} = \\{0,0,1,1\\} \\]\n\n\n\nPotential earnings are given by:\n\\[ \\log(W) = \\phi_0 + \\phi_1 \\kappa \\]\nwhere \\(\\kappa\\) is accumulated labor market experience that evolves according to:\n\\[ \\kappa_{t+1} = \\kappa_{t} + H_{jt} \\]\nWeekly net income is a function of this work decision and the participation decision:\n\\[ Y_{j}(W,\\omega) = H_{j}W + P_{j}\\mathbf{1}\\{\\omega&lt;\\Omega\\}(200 - 0.5\\times H_{j}W) \\]\nwhere:\n\n\\(W\\) is the individual’s potential wage\n\\(\\Omega\\) is the lifetime limit on program participation\n\\(\\omega\\) is the individual’s cumulative prior program use.\n\n\n\n\nIndividuals discount the future at an exponential rate of \\(1-\\beta\\). Per-period payoffs are given by:\n\\[ u_{j}(W) = \\log(\\gamma + Y_{j}(W,\\omega)) - \\alpha P_{j} - \\varphi H_{j} \\]\nThe per-period payoff for any individual \\(n\\) at time \\(t\\) is given by\n\\[ U_{ntj} = u_{j}(W_{nt}) + \\epsilon_{ntj} \\]\nwhere \\(\\epsilon_{nt}\\) is a preference shock that has a type 1 extreme value distribution with scale parameter \\(\\sigma\\). It is iid across individuals and time periods and us only observed by individuals when period \\(t\\) arrives.\n\n\n\nAll individuals begin in \\(t=1\\) with \\(\\omega_{1}=0\\) and \\(\\kappa_{1}=0\\) and live for \\(T\\) periods.\n\n\n\nAssume that log-wages are observed with measurement error:\n\\[ \\log(W^o) = \\log(W) + \\xi,\\qquad \\xi\\sim\\mathcal{N}(0,\\sigma^2_\\xi) \\]\n\n\n\nHere is code to solve the model for a default set of parameters.\n\nT = 10 #&lt;- length of life-cycle\nϕ = [2., 0.05]\nΩ = 4 #&lt;- time limit\nwagefunc(κ,ϕ) = exp(ϕ[1] + ϕ[2]*κ)\nnet_income(W,H,P,eligible) = W*H + P*eligible*(200 - 0.5*W*H) \nj_idx(H,P) = P*2 + H + 1\nutility(W,H,P,eligible,pars) = log(pars.γ + net_income(W,H,P,eligible)) - pars.α*P - pars.φ*H   \npars = (;α = 1., φ = 1., σ = 1., γ = 1., Ω, T,ϕ,β = 0.9)\n\nfunction iterate!(P,v,V,pars,ω_idx,κ_idx,t)\n    (;β,ϕ,Ω,σ) = pars\n    ω = ω_idx - 1\n    κ = κ_idx - 1\n    W = wagefunc(κ,ϕ)\n    eligible = ω&lt;Ω\n    for P in 0:1, H in 0:1\n        j = j_idx(H,P)\n        κ_next = min(κ_idx + H,T)\n        ω_next = min(ω_idx + P,Ω+1)\n        v[j]  = utility(W,H,P,eligible,pars) + β*V[ω_next,κ_next,t+1]\n    end\n    norm = sum(exp.(v ./ σ))\n    P[:,ω_idx,κ_idx,t] = exp.(v ./ σ) ./ norm\n    V[ω_idx,κ_idx,t] = σ * log(norm)\nend\n\nfunction solve!(P,v,V,pars)\n    for t in reverse(axes(P,4))\n        for κ_idx in axes(P,3), ω_idx in axes(P,2)\n            iterate!(P,v,V,pars,ω_idx,κ_idx,t)\n        end\n    end\nend\n\nV = zeros(Ω+1,T,T+1)\nP = zeros(4,Ω+1,T,T)\nv = zeros(4)\n\nsolve!(P,v,V,pars)"
  },
  {
    "objectID": "assignments/Assignment-5.html#introduction-and-setup",
    "href": "assignments/Assignment-5.html#introduction-and-setup",
    "title": "Assignment 5",
    "section": "",
    "text": "In this assignment you will implement a choice probability estimator for a toy model of dynamic labor supply.\n\n\nAgents in the model make one of four choices (\\(j \\in \\{0,1,2,3\\}\\)) that correspond to a binary work decision (\\(H_{j}\\)) and a binary program participation decision (\\(P_{j}\\)):\n\\[ \\{H_0,\\ H_1,\\ H_2,\\ H_3\\} = \\{0,1,0,1\\},\\qquad \\{P_0,\\ P_1,\\ P_2,\\ P_3\\} = \\{0,0,1,1\\} \\]\n\n\n\nPotential earnings are given by:\n\\[ \\log(W) = \\phi_0 + \\phi_1 \\kappa \\]\nwhere \\(\\kappa\\) is accumulated labor market experience that evolves according to:\n\\[ \\kappa_{t+1} = \\kappa_{t} + H_{jt} \\]\nWeekly net income is a function of this work decision and the participation decision:\n\\[ Y_{j}(W,\\omega) = H_{j}W + P_{j}\\mathbf{1}\\{\\omega&lt;\\Omega\\}(200 - 0.5\\times H_{j}W) \\]\nwhere:\n\n\\(W\\) is the individual’s potential wage\n\\(\\Omega\\) is the lifetime limit on program participation\n\\(\\omega\\) is the individual’s cumulative prior program use.\n\n\n\n\nIndividuals discount the future at an exponential rate of \\(1-\\beta\\). Per-period payoffs are given by:\n\\[ u_{j}(W) = \\log(\\gamma + Y_{j}(W,\\omega)) - \\alpha P_{j} - \\varphi H_{j} \\]\nThe per-period payoff for any individual \\(n\\) at time \\(t\\) is given by\n\\[ U_{ntj} = u_{j}(W_{nt}) + \\epsilon_{ntj} \\]\nwhere \\(\\epsilon_{nt}\\) is a preference shock that has a type 1 extreme value distribution with scale parameter \\(\\sigma\\). It is iid across individuals and time periods and us only observed by individuals when period \\(t\\) arrives.\n\n\n\nAll individuals begin in \\(t=1\\) with \\(\\omega_{1}=0\\) and \\(\\kappa_{1}=0\\) and live for \\(T\\) periods.\n\n\n\nAssume that log-wages are observed with measurement error:\n\\[ \\log(W^o) = \\log(W) + \\xi,\\qquad \\xi\\sim\\mathcal{N}(0,\\sigma^2_\\xi) \\]\n\n\n\nHere is code to solve the model for a default set of parameters.\n\nT = 10 #&lt;- length of life-cycle\nϕ = [2., 0.05]\nΩ = 4 #&lt;- time limit\nwagefunc(κ,ϕ) = exp(ϕ[1] + ϕ[2]*κ)\nnet_income(W,H,P,eligible) = W*H + P*eligible*(200 - 0.5*W*H) \nj_idx(H,P) = P*2 + H + 1\nutility(W,H,P,eligible,pars) = log(pars.γ + net_income(W,H,P,eligible)) - pars.α*P - pars.φ*H   \npars = (;α = 1., φ = 1., σ = 1., γ = 1., Ω, T,ϕ,β = 0.9)\n\nfunction iterate!(P,v,V,pars,ω_idx,κ_idx,t)\n    (;β,ϕ,Ω,σ) = pars\n    ω = ω_idx - 1\n    κ = κ_idx - 1\n    W = wagefunc(κ,ϕ)\n    eligible = ω&lt;Ω\n    for P in 0:1, H in 0:1\n        j = j_idx(H,P)\n        κ_next = min(κ_idx + H,T)\n        ω_next = min(ω_idx + P,Ω+1)\n        v[j]  = utility(W,H,P,eligible,pars) + β*V[ω_next,κ_next,t+1]\n    end\n    norm = sum(exp.(v ./ σ))\n    P[:,ω_idx,κ_idx,t] = exp.(v ./ σ) ./ norm\n    V[ω_idx,κ_idx,t] = σ * log(norm)\nend\n\nfunction solve!(P,v,V,pars)\n    for t in reverse(axes(P,4))\n        for κ_idx in axes(P,3), ω_idx in axes(P,2)\n            iterate!(P,v,V,pars,ω_idx,κ_idx,t)\n        end\n    end\nend\n\nV = zeros(Ω+1,T,T+1)\nP = zeros(4,Ω+1,T,T)\nv = zeros(4)\n\nsolve!(P,v,V,pars)"
  },
  {
    "objectID": "assignments/Assignment-5.html#part-1",
    "href": "assignments/Assignment-5.html#part-1",
    "title": "Assignment 5",
    "section": "Part 1",
    "text": "Part 1\nWrite a function that simulates a panel dataset consisting of choices and wages for \\(N\\) individuals in every period \\(t\\) of their life-cycle."
  },
  {
    "objectID": "assignments/Assignment-5.html#part-2",
    "href": "assignments/Assignment-5.html#part-2",
    "title": "Assignment 5",
    "section": "Part 2",
    "text": "Part 2\nFor a simulated sample of size \\(N=1000\\), write a function to estimate the array \\(P\\) of choice probabilities using a frequency estimator. Note that in period \\(t\\), only experience levels up to \\(t-1\\) can be observed so you do not need to estimate this array in its entirety."
  },
  {
    "objectID": "assignments/Assignment-5.html#part-3",
    "href": "assignments/Assignment-5.html#part-3",
    "title": "Assignment 5",
    "section": "Part 3",
    "text": "Part 3\nWrite an estimation routine for the structural parameters \\((\\phi,\\alpha,\\varphi,\\sigma,\\gamma,\\beta)\\) that utilizes the first stage estimates \\(\\hat{P}\\) from part 2. You can either work backwards from \\(T\\) or exploit shorter finite dependence horizons.\nOne small hint: you can estimate \\(\\phi\\) directly using OLS."
  },
  {
    "objectID": "assignments/Assignment-5.html#part-4",
    "href": "assignments/Assignment-5.html#part-4",
    "title": "Assignment 5",
    "section": "Part 4",
    "text": "Part 4\nIf you can, write a monte-carlo simulation in which you repeatedly draw a sample of size \\(N=1000\\) and produce an estimate. Use this sample to look at how the estimates are distributed around the true parameters and therefore check that your estimator is working properly."
  },
  {
    "objectID": "assignments/Assignment-6.html",
    "href": "assignments/Assignment-6.html",
    "title": "Assignment 6",
    "section": "",
    "text": "Let’s go back to the simple model of labor supply and program participation from Assignment 2.\nConsider the following extension to the model. Suppose that each individual \\(n\\) belongs to one of \\(K\\) finite types, \\(k(n)\\in\\{1,2,...,K\\}\\). Types determine differences in the cost of work and program participation as well as differences in wages:\n\\[ U_{itj} = \\log(\\max\\{50,Y_{it}(W_{it}H_{j})\\}) + \\alpha_{l,k}\\log(112 - H_{j}) - \\alpha_{P,k,1}\\mathbf{1}\\{P_{j}&gt;0\\} - \\alpha_{P,k,2}\\mathbf{1}\\{P_{j}&gt;1\\} \\]\nand \\[ \\log(W_{it}) = \\gamma_{k,0} + \\gamma_{k,1}\\text{Age}_{it} + \\gamma_{k,2}\\text{Age}_{it}^2 \\]\nSo the parameters \\(\\alpha_{l},\\alpha_{P},\\gamma\\) are now heterogeneous by type."
  },
  {
    "objectID": "assignments/Assignment-6.html#introduction-and-setup",
    "href": "assignments/Assignment-6.html#introduction-and-setup",
    "title": "Assignment 6",
    "section": "",
    "text": "Let’s go back to the simple model of labor supply and program participation from Assignment 2.\nConsider the following extension to the model. Suppose that each individual \\(n\\) belongs to one of \\(K\\) finite types, \\(k(n)\\in\\{1,2,...,K\\}\\). Types determine differences in the cost of work and program participation as well as differences in wages:\n\\[ U_{itj} = \\log(\\max\\{50,Y_{it}(W_{it}H_{j})\\}) + \\alpha_{l,k}\\log(112 - H_{j}) - \\alpha_{P,k,1}\\mathbf{1}\\{P_{j}&gt;0\\} - \\alpha_{P,k,2}\\mathbf{1}\\{P_{j}&gt;1\\} \\]\nand \\[ \\log(W_{it}) = \\gamma_{k,0} + \\gamma_{k,1}\\text{Age}_{it} + \\gamma_{k,2}\\text{Age}_{it}^2 \\]\nSo the parameters \\(\\alpha_{l},\\alpha_{P},\\gamma\\) are now heterogeneous by type."
  },
  {
    "objectID": "assignments/Assignment-6.html#part-1",
    "href": "assignments/Assignment-6.html#part-1",
    "title": "Assignment 6",
    "section": "Part 1",
    "text": "Part 1\nWrite a routine to classify individuals in the data into one of \\(K=3\\) types using K-means clustering. You may find the package Clustering.jl useful."
  },
  {
    "objectID": "assignments/Assignment-6.html#part-2",
    "href": "assignments/Assignment-6.html#part-2",
    "title": "Assignment 6",
    "section": "Part 2",
    "text": "Part 2\nCalculate and plot average work and average program participation over time for each of these types. Comment on what you are seeing."
  },
  {
    "objectID": "assignments/Assignment-6.html#part-3",
    "href": "assignments/Assignment-6.html#part-3",
    "title": "Assignment 6",
    "section": "Part 3",
    "text": "Part 3\nWrite code to estimate this extended model. You could just make a small extension to the maximum likelihood estimator you used in Assignment 2, or you could try another approach if you prefer."
  }
]