[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methods for Structural Microeconometrics",
    "section": "",
    "text": "Welcome to this course! Find details on the syllabus here, and check out the menu to find other materials.\nHere is a link to the git repo for this course, which by cloning is probably the simplest way for you to get access to the data, code, and assignments. I will continue to add materials throughout the course. You may prefer to selectively download these materials."
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "Methods for Structural Microeconometrics",
    "section": "Topics",
    "text": "Topics\n\nIdentification, Credible Inference, and Marschak’s Maxim\nWe formally define identification and discuss (via examples) what people really mean when they talk about identification and credible inference. We use the Generalized Roy Model to compare identification via functional form to nonparametric identification.\nWe introduce Marschak’s Maxim as a guide for doing empirical model-based research.\n\nReading\nThe two survey articles by Keane (2010) (link) and Angrist and Pischke (2010) (link) - although aging - provide two important perspectives on the issues of credible inference in economics. Low and Meghir (2017) provide a nice review of the advantages of the structural approach.\nThe original paper by Marschak (1953) may be of interest. Heckman and Vytlacil (2007) provide a nice discussion of Marschak’s Maxim in the context of policy evaluation. They introduce (Heckman and Vytlacil 2005; Carneiro, Heckman, and Vytlacil 2011) the Marginal Treatment Effect as a tool for thinking about quasi-experimental estimators and policy evaluation.\n\n\n\nDynamic Discrete Choice\nWe introduce the dynamic discrete choice model and briefly discuss identification when all persistent state variables are observed. We review some of the basics of discrete choice such as the generalized extreme value distribution, which produces tractable choice probabilities with relatively flexible cross-price elasticities.\n\nReading\nRust (1987) is the canonical example demonstrating estimation of dynamic discrete choice models with maximum likelihood and a nested solution method.\nWe show that if one can directly estimate choice probabilities, several tractable approaches produce estimates of structural parameters without repeatedly solving the model. These include Hotz and Miller (1993), Aguirregabiria and Mira (2002), Aguirregabiria and Mira (2007), Pesendorfer and Schmidt-Dengler (2008), and Arcidiacono and Miller (2011). We will review these methods and why they are not appropriate to use in Mullins (2022).\n\n\n\nEstimation of Dynamic Models with Unobserved Heterogeneity\nUsing Mullins (2022) as an example, we talk about the inferential pitfalls that can occur when models fail to account for unobserved heterogeneity. We briefly discuss how this can depend on estimation approaches and sources of identification.\nWe review methods for estimation of dynamic models, including the Expectation-Maximization algorithm (EM) (see, e.g. Arcidiacono and Miller (2011)) and the clustering approach of Bonhomme and Manresa (2015). We review practical considerations for these approaches and introduce the Forward-Back algorithm for implementing EM in hidden Markov Models with time-varying unobserved state variables. We introduce a sparse matrix implementation of this algorithm.\n\n\nIdentification of Dynamic Models with Unobserved Heterogeneity\nWe discuss how either panel data or instrumental variables can facilitate identification of models with unobserved heterogeneity, and briefly review identification results for finite mixtures in panel data settings due to Kasahara and Shimotsu (2009) and Bonhomme, Jochmans, and Robin (2016). Berry and Compiani (2023) analyze identification and estimation of dynamic models with persistent heterogeneity using instrumental variables."
  },
  {
    "objectID": "index.html#assessment",
    "href": "index.html#assessment",
    "title": "Methods for Structural Microeconometrics",
    "section": "Assessment",
    "text": "Assessment\nThere will be 6 problem sets. Your best 4 of these 6 problem sets will be worth 25%. Hence, you can skip two if you want.\nHere is the proposed timeline of due dates. Submissions must be made through Canvas as a notebook (e.g. jupyter or quarto) formatted to html with printed output.\n\n\n\nAssignment\nDue Date\n\n\n\n\nAssignment 1\nNovember 7\n\n\nAssignment 2\nNovember 14\n\n\nAssignment 3\nNovember 21\n\n\nAssignment 4\nNovember 28\n\n\nAssignment 5\nDecember 5\n\n\nAssignment 6\nDecember 12"
  },
  {
    "objectID": "index.html#office-hours",
    "href": "index.html#office-hours",
    "title": "Methods for Structural Microeconometrics",
    "section": "Office Hours",
    "text": "Office Hours\nI will provide a link on Canvas to sign up for my weekly office hours."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "assignments/Assignment-1.html",
    "href": "assignments/Assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Consider a dynamic labor supply model (with no uncertainty) where each agent \\(n\\) chooses a sequence of consumption and hours, \\(\\{c_{t},h_{t}\\}_{t=1}^{\\infty}\\), to solve: \\[ \\max \\sum_{t=0}^\\infty \\beta^{t} \\left(\\frac{c_{t}^{1-\\sigma}}{1-\\sigma} - \\frac{\\alpha_{n}^{-1}}{1 + 1\\psi}h_{t}^{1+1/\\psi}\\right)\\] subject to the intertemporal budget constraint: \\[ \\sum_{t}q_{t}c_{t} \\leq A_{n,0} + \\sum_{t}q_{t}W_{n,t}h_{t},\\qquad q_{t} = (1+r)^{-t}.\\] Let \\(H_{n,t}\\) and \\(C_{n,t}\\) be the realizations of labor supply for agent \\(n\\) at time \\(t\\). Labor supply in this model obeys: \\[H_{n,t} = (\\beta(1+r))^{-t}(\\alpha_{n}W_{n,t})C^{-\\sigma}_{n,t}.\\] To simplify below, assume that \\(\\beta=(1+r)^{-1}\\), so that the optimal solution features perfectly smoothed consumption, \\(C^*_{n}\\). Making appropriate substitutions gives \\(C^*_{n}\\) as the solution to: \\[ \\left(\\sum_{t}q_{t}\\right)C^*_{n} = \\sum_{t}\\left(q_{t}W_{n,t}^{1+\\psi}\\right)\\alpha_{n}^{\\psi}(C^*)^{-\\sigma}_{n} + A_{n,0}.\\]"
  },
  {
    "objectID": "assignments/Assignment-1.html#setup-loading-the-data",
    "href": "assignments/Assignment-1.html#setup-loading-the-data",
    "title": "Assignment 1",
    "section": "",
    "text": "Here is code to load the dataset and do a little cleaning / filtering.\nThe sample is all mothers in the PSID who are unmarried at the time of their first childbirth.\n\nusing CSV, DataFrames, DataFramesMeta\n\ndata = @chain begin\n    CSV.read(\"../children-cash-transfers/data/MainPanelFile.csv\",DataFrame,missingstring = \"NA\")\n    @select :MID :year :wage :hrs :earn :SOI :CPIU :WelfH :FSInd\n    @subset :year.&gt;=1985 :year.&lt;=2010\n    @transform :AFDC = :WelfH.&gt;0\n    @rename :FS = :FSInd\n    end\n\n89747×10 DataFrame89722 rows omitted\n\n\n\nRow\nMID\nyear\nwage\nhrs\nearn\nSOI\nCPIU\nWelfH\nFS\nAFDC\n\n\n\nInt64\nInt64\nFloat64?\nInt64?\nFloat64?\nInt64\nFloat64\nFloat64?\nInt64?\nBool?\n\n\n\n\n1\n4031\n1990\nmissing\nmissing\nmissing\n43\n0.758793\n0.0\n0\nfalse\n\n\n2\n4031\n1991\nmissing\nmissing\nmissing\n43\n0.790786\n0.0\n0\nfalse\n\n\n3\n4031\n1992\nmissing\nmissing\nmissing\n43\n0.814835\n0.0\n1\nfalse\n\n\n4\n4031\n1993\nmissing\nmissing\nmissing\n43\n0.839034\n0.0\n0\nfalse\n\n\n5\n4031\n1994\nmissing\n0\n0.0\n43\n0.860812\n1704.0\n1\ntrue\n\n\n6\n4031\n1995\nmissing\n0\n0.0\n43\n0.88496\n1704.0\n1\ntrue\n\n\n7\n4031\n1996\nmissing\n0\n0.0\n43\n0.910948\n1704.0\n1\ntrue\n\n\n8\n4031\n1997\nmissing\nmissing\nmissing\n43\n0.932244\nmissing\nmissing\nmissing\n\n\n9\n4031\n1998\nmissing\n0\n0.0\n43\n0.946664\n0.0\n1\nfalse\n\n\n10\n4031\n1999\nmissing\nmissing\nmissing\n43\n0.967426\nmissing\nmissing\nmissing\n\n\n11\n4031\n2000\n3.33333\n120\n400.0\n43\n1.0\n0.0\n0\nfalse\n\n\n12\n4031\n2001\nmissing\nmissing\nmissing\n43\n1.02817\nmissing\nmissing\nmissing\n\n\n13\n4031\n2002\nmissing\n0\n0.0\n43\n1.04457\n0.0\n0\nfalse\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n89736\n9308002\n1999\nmissing\nmissing\nmissing\n39\n0.967426\nmissing\nmissing\nmissing\n\n\n89737\n9308002\n2000\nmissing\nmissing\nmissing\n39\n1.0\nmissing\nmissing\nmissing\n\n\n89738\n9308002\n2001\nmissing\nmissing\nmissing\n39\n1.02817\nmissing\nmissing\nmissing\n\n\n89739\n9308002\n2002\nmissing\nmissing\nmissing\n39\n1.04457\nmissing\nmissing\nmissing\n\n\n89740\n9308002\n2003\nmissing\nmissing\nmissing\n39\n1.06857\nmissing\nmissing\nmissing\n\n\n89741\n9308002\n2004\nmissing\nmissing\nmissing\n39\n1.09708\nmissing\nmissing\nmissing\n\n\n89742\n9308002\n2005\nmissing\nmissing\nmissing\n39\n1.13401\nmissing\nmissing\nmissing\n\n\n89743\n9308002\n2006\nmissing\nmissing\nmissing\n39\n1.17054\nmissing\nmissing\nmissing\n\n\n89744\n9308002\n2007\nmissing\nmissing\nmissing\n39\n1.20414\nmissing\nmissing\nmissing\n\n\n89745\n9308002\n2008\nmissing\nmissing\nmissing\n39\n1.25008\nmissing\nmissing\nmissing\n\n\n89746\n9308002\n2009\nmissing\nmissing\nmissing\n39\n1.24608\nmissing\nmissing\nmissing\n\n\n89747\n9308002\n2010\nmissing\nmissing\nmissing\n39\n1.26647\nmissing\nmissing\nmissing\n\n\n\n\n\n\nYou may be unfamiliar with some of these commands, which make use of DataFrames and DataFramesMeta. In particular, think of the @chain macro as a way to compose functions. So for example:\n\nd1 = @chain d2 begin\n    func1(x)\n    func2(y)\n    func3(z)\nend\n\nis equivalent to calling:\n\nd1 = func3(func2(func1(d2,x),y),z)\n\nIf you want to understand better, google is your friend!"
  },
  {
    "objectID": "assignments/Assignment-1.html#question-1",
    "href": "assignments/Assignment-1.html#question-1",
    "title": "Assignment 1",
    "section": "Question 1",
    "text": "Question 1\nCalculate average welfare participation (AFDC) by year and plot it. What do you think happened with welfare participation in 1996 and after? If you don’t know the historical context, a quick search online or a read of this paper should help you out.\nIf you are new to julia, here is average hours calculated and plotted to get you started.\n\nusing StatsPlots, Statistics\n\nd = @chain data begin\n    groupby(:year)\n    @combine :Hours = mean(skipmissing(:hrs))\n    @subset .!isnan.(:Hours)\nend\n\n@df d plot(:year,:Hours, legend = :none, linewidth = 2)\nxlabel!(\"Year\")\nylabel!(\"Average Welfare Participation\")"
  },
  {
    "objectID": "assignments/Assignment-1.html#question-2",
    "href": "assignments/Assignment-1.html#question-2",
    "title": "Assignment 1",
    "section": "Question 2",
    "text": "Question 2\nNow write code to\n\nDeflate earnings by CPI (CPIU).\nCalculate annual average earnings for each individual (identified by MID).\nDrop individuals with fewer than 10 years of data.\nCategorize individuals by whether their average earnings is below or above the median across individuals.\nPlot average participation in each year for individuals in each of these two categories.\n\nDo you think this pattern is likely to be generated by a model without persistent unobserved heterogeneity? No strictly correct answer here, just curious to read what you think.\nIn case it helps, here is code for the first three steps. You could edit this to add additional operations to the chain or work with d directly.\n\nd = @chain data begin\n    @transform :earn = :earn ./ :CPIU\n    groupby(:MID)\n    @combine :T = sum(.!ismissing.(:earn)) :earn = mean(skipmissing(:earn)) \n    @subset :T .&gt;= 10\nend\n\n1089×3 DataFrame1064 rows omitted\n\n\n\nRow\nMID\nT\nearn\n\n\n\nInt64\nInt64\nFloat64\n\n\n\n\n1\n4031\n10\n40.0\n\n\n2\n4179\n16\n6089.12\n\n\n3\n7030\n11\n4500.53\n\n\n4\n41007\n12\n16147.7\n\n\n5\n41008\n11\n0.0\n\n\n6\n45030\n11\n14374.3\n\n\n7\n45031\n11\n17693.5\n\n\n8\n47031\n11\n15946.7\n\n\n9\n84005\n18\n30769.6\n\n\n10\n105030\n14\n4633.61\n\n\n11\n106173\n13\n17714.4\n\n\n12\n122173\n13\n56275.2\n\n\n13\n126003\n19\n6705.08\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n1078\n6843006\n19\n1805.93\n\n\n1079\n6843173\n19\n5005.44\n\n\n1080\n6845005\n19\n19795.4\n\n\n1081\n6849005\n19\n2450.55\n\n\n1082\n6849188\n15\n23259.0\n\n\n1083\n6853003\n19\n4856.42\n\n\n1084\n6862005\n11\n18899.4\n\n\n1085\n6862008\n19\n26634.8\n\n\n1086\n6864002\n19\n8695.92\n\n\n1087\n6864003\n18\n13627.5\n\n\n1088\n6867013\n13\n389.097\n\n\n1089\n6872171\n17\n3294.07"
  },
  {
    "objectID": "assignments/Assignment-1.html#question-3",
    "href": "assignments/Assignment-1.html#question-3",
    "title": "Assignment 1",
    "section": "Question 3",
    "text": "Question 3\nThis question is to familiarize you with the module Tranfers.jl which will enable you to calculate post-tax and transfer income for individuals given their earnings, non-labor income, state, year, and family size. The function budget in this module takes the arguments:\n\nE: monthly earnings (either real or nominal)\nN: monthly non-labor income (real or nominal)\nSOI: the SOI code for state of residence\nyear: calendar year\nnum_kids: the number of children\ncpi: set to 1. if E and N are nominal\np: equal to 0 if no programs, 1 if food stamps, 2 if food stamps + welfare.\n\nFor example the function call:\n\nTransfers.budget(500.,0.,23,2000,2,1.,2)\n\ncalculates net income for a mother in Michigan (SOI code 23) with 2 kids, nominal labor income of $500 a month in the year 2000, and no non-labor income.\n\ninclude(\"../children-cash-transfers/src/Transfers.jl\")\n\nTransfers.budget(500.,0.,23,2000,2,1.,2)\n\n978.5408333333334\n\n\nCreate a graph that represents total net transfers for a single mother with two kids in the years 1990 and 2000 and in the states of Mississippi and New York. Depict these transfers as a function of earnings between the values of 0 and $1,000 a month (nominal). You can assume that all households are receiving both food stamps and welfare.\nWhat do you make of the differences in these transfers across states and over time?"
  },
  {
    "objectID": "assignments/Assignment-2.html",
    "href": "assignments/Assignment-2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Use the same labor supply model as Assignment 1. We now assume that net log wages follow a process: \\[ \\log(\\tilde{W}_{n,t}) = \\gamma_{0} + \\gamma_{1} Z_{n,t} + \\epsilon_{n,t} \\] where \\(Z_{n,t}\\) is independent of the pair (\\(\\alpha_{n},\\epsilon_{n,t}\\)). You can think of \\(Z_{n,t}\\) as an instrument for labor demand or an indicator for the presence of a wage subsidy. As before, we assume no uncertainty, so the path of \\(Z_{n,t}\\) is treated as known.\nTo begin, suppose that you have a single cross-section of data: \\[ (H_{n,0},\\tilde{W}_{n,0},Z_{n,0},C_{n,0},A_{n,0})_{n=1}^{N} \\] where \\(Z_{n,0}\\in\\{0,1\\}\\) is perceived to be a permanent wage subsidy (so \\(Z_{n,0}=Z_{n,t}\\) for all \\(t&gt;0\\))."
  },
  {
    "objectID": "assignments/Assignment-2.html#setup-code-for-mle",
    "href": "assignments/Assignment-2.html#setup-code-for-mle",
    "title": "Assignment 2",
    "section": "Setup: Code for MLE",
    "text": "Setup: Code for MLE\n\nReading in Data\nYou are going to build on the code below that estimates a very simple model of labor supply and program participation.\nFirst, here is a chunk of code to read in the data and prepare it. The variables in julia DataFrames are typically not type stable which can slow down your code considerably. So you will note that in the final step here I bring the variables I want to use out of a DataFrame and into a named tuple with vectors containing a fixed type.\n\nusing CSV, DataFrames, DataFramesMeta, Optim\ninclude(\"../children-cash-transfers/src/Transfers.jl\")\n\n# for this simple model we can just drop missing data. When we estimate the model with persistent latent heterogeneity, we will need complete panels (including years where choices are missing).\n\ndata = @chain begin\n    CSV.read(\"../children-cash-transfers/data/MainPanelFile.csv\",DataFrame,missingstring = \"NA\")\n    #@select :MID :year :wage :hrs :earn :SOI :CPIU :WelfH :FSInd :num_child :age\n    @subset :year.&gt;=1985 :year.&lt;=2010\n    @transform :AFDC = :WelfH.&gt;0 \n    @rename :FS = :FSInd\n    @transform :P  = :FS + :AFDC :H = min.(2,round.(Union{Int64, Missing},:hrs / (52*20)))\n    @subset .!ismissing.(:P) .&& .!ismissing.(:H)\n    @transform @byrow :wage = begin\n        if :hrs&gt;0 && :earn&gt;0\n            return :earn / :hrs / :CPIU\n        else\n            return missing\n        end\n    end\nend\n\ndata_mle = (;P = Int64.(data.P), H = Int64.(data.H), year = data.year, age = data.age,\n            soi = data.SOI, num_kids = data.num_child, cpi = data.CPIU,\n            logwage = log.(coalesce.(data.wage,1.)),wage_missing = ismissing.(data.wage))\n\n(P = [2, 2, 2, 1, 0, 0, 0, 2, 1, 0  …  2, 2, 1, 2, 0, 0, 1, 1, 1, 1], H = [0, 0, 0, 0, 0, 0, 0, 0, 0, 2  …  0, 0, 2, 2, 2, 2, 0, 0, 0, 0], year = [1994, 1995, 1996, 1998, 2000, 2002, 2004, 2006, 2008, 1990  …  1991, 1992, 1991, 1992, 1991, 1992, 1991, 1992, 1991, 1992], age = [21, 22, 23, 25, 27, 29, 31, 33, 35, 17  …  21, 22, 23, 24, 39, 40, 30, 31, 25, 26], soi = [43, 43, 43, 43, 43, 43, 43, 43, 43, 17  …  44, 44, 7, 7, 5, 5, 39, 39, 39, 39], num_kids = [2, 2, 3, 3, 3, 3, 3, 3, 3, 1  …  2, 2, 2, 2, 3, 3, 4, 4, 2, 2], cpi = [0.860812349005761, 0.884959812302546, 0.910948243820851, 0.946664188812488, 1.0, 1.04457233785542, 1.09707768072849, 1.17054218546739, 1.25008130459022, 0.758792510685746  …  0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336], logwage = [0.0, 0.0, 0.0, 0.0, 1.2039728043259361, 0.0, 0.0, 0.0, 0.0, 0.8230555833449215  …  0.0, 0.0, 2.6512574120409043, 2.3314412292534223, 1.265948758245773, 1.8744481199656744, 0.0, 0.0, 0.0, 0.0], wage_missing = Bool[1, 1, 1, 1, 0, 1, 1, 1, 1, 0  …  1, 1, 0, 0, 0, 0, 1, 1, 1, 1])\n\n\n\n\nThe Model\nConsider a static model with 9 discrete choices. Hours choices are given by \\(H_{j}\\in\\{0,20,40\\}\\) and participation choices indicated by \\(P_{j}\\in\\{0,1,2\\}\\). Individuals may choose any combination of these, with a Type I extreme value distributed shock \\(\\epsilon_{j}\\) for choice \\(j\\) that is iid over time and across individuals. The scale of these shocks is given by \\(\\sigma\\).\nUtility for individual \\(i\\) at time \\(t\\) is given by:\n\\[ U_{itj} = \\log(\\max\\{50,Y_{it}(W_{it}H_{j})\\}) + \\alpha_{l}\\log(112 - H_{j}) - \\alpha_{P,1}\\mathbf{1}\\{P_{j}&gt;0\\} - \\alpha_{P,2}\\mathbf{1}\\{P_{j}&gt;1\\} \\]\nwhere \\(Y_{it}\\) is a net income function for individual \\(i\\) at time \\(t\\) and depends on their earnings (\\(W_{it}H_{j}\\)), the number of children in the household, and the policy environment in the individual’s state of residence.\nWages are a deterministic function of age only:\n\\[ \\log(W_{it}) = \\gamma_{0} + \\gamma_{1}\\text{Age}_{it} + \\gamma_{2}\\text{Age}_{it}^2 \\]\nand are measured with normally distributed iid measurement error:\n\\[ \\log(W^{o}_{it}) = \\log(W_{it}) + \\zeta_{it},\\qquad\\zeta_{it}\\sim\\mathcal{N}(0,\\sigma^2_{W}) \\]\nThe code below calculates utility and indexes choices.\n\nj_idx(p,h) = p*3 + h + 1\nfunction utility(p,h,soi,cpi,year,num_kids,wage,pars)\n    (;αl,Hgrid,σ,αP) = pars\n    hrs = Hgrid[1+h]\n    earn = wage * hrs\n    net_income = max(50.,Transfers.budget(earn,0.,soi,year,num_kids,cpi,p))\n    return log(net_income) + αl*log(112-hrs) - αP[1]*(p&gt;0) - αP[2]*(p&gt;1)\nend\n\npars = (;αl = 1., σ = 1., σW = 1., γ = zeros(3),\n    Hgrid = [0,20.,40.],αP = zeros(2))\n\n(αl = 1.0, σ = 1.0, σW = 1.0, γ = [0.0, 0.0, 0.0], Hgrid = [0.0, 20.0, 40.0], αP = [0.0, 0.0])\n\n\nChoice probabilities are given by the logit formula.\n\nfunction choice_prob!(logP,it,data,pars)\n    (;σ,γ) = pars\n    wage = exp(γ[1] + γ[2] * data.age[it] + γ[3] * data.age[it]^2)\n    denom = 0.\n    umax = -Inf\n    for p in 0:2\n        for h in 0:2\n            j = j_idx(p,h)\n            u = utility(p,h,data.soi[it],data.cpi[it],data.year[it],data.num_kids[it],wage,pars)\n            logP[j] = u / σ\n            umax = u&gt;umax ? u : umax\n        end\n    end\n    logP[:] .-= umax / σ\n    denom = log(sum(exp.(logP)))\n    logP[:] .-= denom #&lt;- nornalize choice probabilities\nend\n\nchoice_prob! (generic function with 1 method)\n\n\nAnd so the log-likelihood takes a simple form.\n\nfunction log_likelihood(logP,it,data,pars)\n    (;γ,σW) = pars\n    ll = 0.\n    if !data.wage_missing[it]\n        ll += -0.5((data.logwage[it] - γ[1] - γ[2]*data.age[it] - γ[3]*data.age[it]^2) / σW)^2 - log(σW)\n    end\n    choice_prob!(logP,it,data,pars)\n    j = j_idx(data.P[it],data.H[it])\n    ll += logP[j]\n    return ll\nend\n\nfunction update(x,p)\n    αl = exp(x[1])\n    σ = exp(x[2])\n    γ = x[3:5]\n    σW = exp(x[6])\n    αP = x[7:8]\n    return (;p...,αl,σ,σW,αP,γ)\nend\n\nfunction log_likelihood(x,data,pars)\n    pars = update(x,pars)\n    logP = zeros(eltype(x),9)\n    ll = 0.\n    for it in eachindex(data.P)\n        ll += log_likelihood(logP,it,data,pars)\n    end\n    return ll\nend\n\nlog_likelihood (generic function with 2 methods)\n\n\nWe can use Optim to maximize the log-likelihood:\n\nx0 = zeros(8)\nx0 = [0.,0.,log(10.),0.,0.,0.,0.,0.]\nres = optimize(x-&gt;-log_likelihood(x,data_mle,pars),x0,BFGS(),autodiff=:forward,Optim.Options(show_trace = true))\npars = update(res.minimizer,pars)\n\nIter     Function value   Gradient norm \n     0     7.234596e+04     3.085114e+06\n * time: 0.00633692741394043\n     1     7.219007e+04     7.930873e+05\n * time: 0.4547231197357178\n     2     7.189756e+04     8.240034e+05\n * time: 0.5587921142578125\n     3     6.311750e+04     1.040038e+06\n * time: 0.803109884262085\n     4     5.773072e+04     3.637938e+06\n * time: 1.2063300609588623\n     5     5.738226e+04     6.079719e+06\n * time: 1.3685970306396484\n     6     5.692685e+04     7.218571e+06\n * time: 1.5767319202423096\n     7     5.644027e+04     8.094668e+06\n * time: 1.9543049335479736\n     8     5.629959e+04     8.406176e+06\n * time: 2.011056900024414\n     9     5.408423e+04     4.483122e+06\n * time: 2.0493218898773193\n    10     5.315575e+04     3.603074e+06\n * time: 2.0878500938415527\n    11     5.209566e+04     2.625888e+05\n * time: 2.145103931427002\n    12     5.195541e+04     2.083292e+05\n * time: 2.1833839416503906\n    13     5.188423e+04     6.807290e+05\n * time: 2.221687078475952\n    14     5.187199e+04     2.396172e+05\n * time: 2.279866933822632\n    15     5.186933e+04     5.094151e+03\n * time: 2.3182590007781982\n    16     5.186887e+04     7.774766e+03\n * time: 2.3758020401000977\n    17     5.186878e+04     2.823744e+04\n * time: 2.433206081390381\n    18     5.186871e+04     1.085063e+04\n * time: 2.4902100563049316\n    19     5.186852e+04     2.080594e+04\n * time: 2.56597900390625\n    20     5.186850e+04     5.468883e+02\n * time: 2.6230380535125732\n    21     5.186850e+04     4.523469e+00\n * time: 2.679996967315674\n    22     5.186850e+04     8.466642e-01\n * time: 2.736649990081787\n    23     5.186850e+04     3.014776e-02\n * time: 2.7939069271087646\n    24     5.186850e+04     1.164486e-03\n * time: 2.8322980403900146\n    25     5.186850e+04     1.957495e-05\n * time: 2.888066053390503\n    26     5.186850e+04     1.250626e-07\n * time: 2.9259419441223145\n    27     5.186850e+04     1.206905e-08\n * time: 2.9638988971710205\n    28     5.186850e+04     3.893547e-09\n * time: 3.0017800331115723\n\n\n(αl = 2.8546233783929473, σ = 0.9509286759263859, σW = 0.7224860062936598, γ = [-0.023620750582789916, 0.10877398375254817, -0.0012722347394312002], Hgrid = [0.0, 20.0, 40.0], αP = [2.035301636135555, 1.0137619542631466])"
  },
  {
    "objectID": "assignments/Assignment-2.html#question-1",
    "href": "assignments/Assignment-2.html#question-1",
    "title": "Assignment 2",
    "section": "Question 1",
    "text": "Question 1\nSuppose we estimate the following system via 2SLS: \\[ \\log(\\tilde{W}_{n,0}) = \\gamma_{0} + \\gamma_{1} Z_{n,t} + \\epsilon_{n,t} \\] \\[ \\log(H_{n,0}) = \\alpha_{0} + \\alpha_{1}\\log(W_{n,0}) + \\varepsilon_{n,0} \\] What is the population limit of the 2SLS estimator? Does it recover a structural or policy parameter of interest?"
  },
  {
    "objectID": "assignments/Assignment-2.html#question-2",
    "href": "assignments/Assignment-2.html#question-2",
    "title": "Assignment 2",
    "section": "Question 2",
    "text": "Question 2\nYou are talking to your friend from Assignment 1 (Mr Straw Man) and you point out to him that the correct specification is the structural one: \\[ \\log(H_{n,t}) = \\mu + \\psi\\log(\\tilde{W}_{n,0}) - \\sigma\\log(C_{n,0}) + \\eta_{n,0}.\\] Your friend tells you that you have no hope here because you have two endogenous variables and only one instrument. You are going to use the model to prove him wrong! Define \\(\\tilde{Z}_{n,0} = M_{n,0}\\times Z_{n,0}\\) where \\(M_{n,0}\\in\\{0,1\\}\\) is an indicator for whether \\(A_{n,0}\\) is above or below the median. Note that the conditional expectation of log consumption can be written wlog as: \\[ \\mathbb{E}[\\log(C)|M,Z] = \\delta_{0} + \\delta_{1}M + \\delta_{2}Z + \\delta_{3}\\underbrace{MZ}_{=\\tilde{Z}}\\] Use the model to argue that \\(\\delta_{3}\\neq0\\)."
  },
  {
    "objectID": "assignments/Assignment-2.html#question-3",
    "href": "assignments/Assignment-2.html#question-3",
    "title": "Assignment 2",
    "section": "Question 3",
    "text": "Question 3\nNow show that one can write: \\[ \\mathbb{E}[\\log(H)|M,Z] = \\kappa_{0} + \\kappa_{1}M + \\psi\\gamma Z - \\sigma \\delta_{2}Z - \\sigma \\tilde{Z}.\\] And combine these two expressions with the wage equation to argue that \\(psi\\) and \\(\\sigma\\) are identified. Why is important that \\(\\delta_{3}\\neq 0\\)?"
  },
  {
    "objectID": "assignments/Assignment-2.html#question-4",
    "href": "assignments/Assignment-2.html#question-4",
    "title": "Assignment 2",
    "section": "Question 4",
    "text": "Question 4\nWrite code to estimate the structural parameters (via GMM or 2SLS) and use a monte-carlo simulation to evaluate the performance of your estimator. You can adapt the code I used in the example here."
  },
  {
    "objectID": "assignments/Assignment-3.html",
    "href": "assignments/Assignment-3.html",
    "title": "Assignment 3 - Solving the Dynamic Program",
    "section": "",
    "text": "Since you only have a week, I’m not going to make you code every piece of the model solution. The folder /children-cash-transfers/src contains:\n\nfunctions to calculate utility and net income from each choice\nan indexing rule that maps choices to hours and participation\nthe structure for the nested logit and code to calculate nested logit probabilities and inclusive values\nfunctions to calculate transition probabilities\ncode to define a default set of parameters and hyperparameters (the number of wage shocks, number of types, etc)\n\nIf you have cloned the course git repo, you can load all of this source code as follows:\n\ninclude(\"../children-cash-transfers/src/model.jl\")\n\nplain_logit (generic function with 1 method)\n\n\nIt will help to quickly explain some of this, but I recommend you read the code carefully since I won’t explain the functions in depth.\n\n\nRecall that in the model there are only three state variables to track: (1) the individual’s type (\\(k\\)); (2) the wage shock (\\(\\varepsilon\\)); and (3) cumulative welfare use (\\(\\omega\\)). Below we define a struct called model_data that contains all of the exogenous state variables that are taken as given for each individual in the data. The struct is defined in /children-cash-transfers/src/model.jl, but we repeat the definition here:\n\nstruct model_data\n    T::Int64 #&lt;- length of problem\n\n    y0::Int64 #&lt;- year to begin problem\n    age0::Int64 # &lt;- mother's age at start of problem\n    SOI::Vector{Int64} #&lt;- state SOI in each year\n    num_kids::Vector{Int64} #&lt;- number of kids in household that are between age 0 and 17\n    TotKids::Int64 #&lt;- indicares the total number of children that the mother will have over the available panel\n    age_kid::Matrix{Int64} #&lt; age_kid[f,t] is the age of child f at time t. Will be negative if child not born yet.\n    cpi::Vector{Float64} #&lt;- cpi\n\n    R::Vector{Int64} #&lt;- indicates if work requirement in time t\n    Kω::Int64 #&lt;- indicates length of time limit once introduced\n    TL::Vector{Bool} #&lt;- indicating that time limit is in place\nend\n\nEventually we will have one of these objects for every mother we observe in the data, and we’ll solve the resulting dynamic program for each of them. To test our functions below we can create a test version of this struct as well as some default parameters:\n\nmd = test_model()\np = pars(2,5) #&lt;- set Kτ = 2 and Kε = 5\n\n\n\n\nThe function nested_logit takes the value of each choice vj and fills log-choice probabilities into a pre-allocated vector logP. It also returns the inclusive value (the “emax” or continuation value). The input \\(B=(B_1,B_2,...,B_L)\\) specifies the partitions in each layer of the tree, while the input \\(C\\) reports the final choices that are ultimately contained in each node in each layer. By definition, at the highest layer, the partition takes the trivial form \\(B_L=\\{\\{1,2,...K_L\\}\\}\\) where \\(K_L\\) is the number of partitions in layer \\(L-1\\). Similarly at the lowest layer, \\(C_{1}\\) must take the form \\(C_{1} = \\{\\{1\\},\\{2\\},...,\\{J\\}\\}\\). For this model with three participation choices that lead into an extensive marginal labor supply choice, the structure is:\n\nB₁ = [[1,2],[3,4],[5,6]]\nC₁ = [[1,],[2,],[3,],[4,],[5,],[6,]]\n\nB₂ = [[1,2,3]]\nC₂ = [[1,2],[3,4],[5,6]]\n\nB = (B₁,B₂)\nC = (C₁,C₂)\n\nThis is defined in src/model/choices.jl. This is for your understanding, but these inputs are all given to you so you can use the function naively if that is all your time allows for.\n\n\n\nYou may find the following objects useful for iterating over the state variables. Let \\(k_{\\tau}\\in\\{1,...,K_{\\tau}\\}\\) index latent types, let \\(k_{\\varepsilon}\\in\\{1,...,K_{\\varepsilon}\\}\\) index wage shocks, and let \\(k_{\\omega} = \\omega+1\\) index cumulative time use. The total size of the state space is \\(K=K_\\tau\\times K_{\\varepsilon} \\times K_{\\omega}\\).\nOne way to do simple indexing is to just work with multi-dimensional arrays and build this into every function. However if you want to add state variables later on it will make it cumbersome to change. Another option is to use LinearIndices objects and their converse, CartesianIndices.\nHere’s a demonstration:\n\n# Hypothetical state space dimensions:\nKε = 5\nKτ = 5\nKω = 6\n\nk_idx = LinearIndices((Kτ,Kε,Kω))\nk_inv = CartesianIndices(k_idx)\n\n# To get the aggregate index k, call:\nk = k_idx[2,3,2]\n@show k\n# Then if we have k we can work back with:\nk_tuple = Tuple(k_inv[k])\n@show k_tuple\n\nk = 37\nk_tuple = (2, 3, 2)\n\n\n(2, 3, 2)\n\n\nSo when iterating, you could think about passing around state indices along with instances of these linear and cartesian indices that allow you to convert back and forth.\n\n\n\nIn the paper, wage shocks are parameterized with a single parameter \\(\\pi_{W}\\) that dictates the probability that an individual remains in the same place on the grid space, with symmetric probabilities of moving up or down. The function fε in states_transitions.jl takes the current wage shock kε and the total number of shocks Kε, along with πW and returns two tuples. The first tuple is the set of grid points that are possible and the second is the probability of being in each of those points.\nFor example:\n\nfε(3,5,0.9)\n\n((2, 3, 4), (0.04999999999999999, 0.9, 0.04999999999999999))\n\n\nSo the function is telling me that when I’m in state 3 I can move to states 3, 4, or 5 next period with probabilities (0.05,0.9,0.05). When we are at the bottom or the top of the grid space, the probabilities are slightly different:\n\nfε(1,5,0.9)\n\n((1, 2), (0.04999999999999999, 0.95))\n\n\nWe could have alternatively just written the transition probabilities into a matrix, but this approach essentially limits us to points with positive probabilities and will simplify iteration.\n\n\n\n\nWrite a function calc_vj that calculates the choice-specific value (i.e. the deterministic value of the choice) of a particular choice \\(j\\) in a particular time period \\(t\\) given the state and other exogenous variables. If you are confident you can code this however you like, but given the existing setup you might like to write the function in a way that it takes the following inputs:\n\nj: the discrete choice\nt: the time period in the model\nstate: a tuple that contains the state \\((k_\\tau,k_\\varepsilon,k_\\omega)\\) as well as a linear indexing rule\nV: a vector that contains the continuation value for each state at time \\(t+1\\)\npars: the parameters of the model\nmd: an instance of the model_data object that holds all relevant state variables\n\nVerify that your function works by testing it on the model_data instance created by test_model. Use the @time macro to look at evaluation time and memory allocations.\n\n\n\nWrite a function called iterate! that iterates over all states at time period \\(t\\) and fills in choice probabilities and continuation values for period \\(t\\) in pre-allocated arrays. Again, you can do this however you like but here is a suggested set of inputs:\n\nt: the time period\nlogP: a \\(J \\times K \\times T\\) array of choice probabilities where the function will fill in logP[:,:,t]\nV: a \\(K \\times T\\) array of continuation values\nstate_idx: a named tuple that contains the size of the overall state space, a linear indexing rule that maps \\((k_\\tau,k_{\\varepsilon},k_{\\omega}\\)) to an overall state \\(k\\), and a Cartesian Indexing rule that inverts this mapping\nvj: a \\(J\\)-dimensional vector that, for each state, can be used as a container for the choice-specific values\npars: model parameters\nmd: model_data for the problem\n\nVerify that your function works by testing it on the model_data instance created by test_model. Use the @time macro to look at evaluation time and memory allocations.\n\n\n\nWrite a function called solve! that performs backward induction to calculate continuation values and choice probabilities (storing them in pre-allocated arrays) in every period of the data across the whole state space. As before, some suggested inputs:\n\nlogP: a \\(J\\times K\\times T\\) array for choice probabilities\nV: a \\(K \\times T\\) array for continuation values\nvj: a container or buffer for choice-specific values in each iteration\npars: model parameters\nmd: an instance of model_data\n\nVerify that your function works by testing it on the model_data instance created by test_model. Use the @time macro to look at evaluation time and memory allocations."
  },
  {
    "objectID": "assignments/Assignment-3.html#source-code-for-the-model",
    "href": "assignments/Assignment-3.html#source-code-for-the-model",
    "title": "Assignment 3 - Solving the Dynamic Program",
    "section": "",
    "text": "Since you only have a week, I’m not going to make you code every piece of the model solution. The folder /children-cash-transfers/src contains:\n\nfunctions to calculate utility and net income from each choice\nan indexing rule that maps choices to hours and participation\nthe structure for the nested logit and code to calculate nested logit probabilities and inclusive values\nfunctions to calculate transition probabilities\ncode to define a default set of parameters and hyperparameters (the number of wage shocks, number of types, etc)\n\nIf you have cloned the course git repo, you can load all of this source code as follows:\n\ninclude(\"../children-cash-transfers/src/model.jl\")\n\nplain_logit (generic function with 1 method)\n\n\nIt will help to quickly explain some of this, but I recommend you read the code carefully since I won’t explain the functions in depth.\n\n\nRecall that in the model there are only three state variables to track: (1) the individual’s type (\\(k\\)); (2) the wage shock (\\(\\varepsilon\\)); and (3) cumulative welfare use (\\(\\omega\\)). Below we define a struct called model_data that contains all of the exogenous state variables that are taken as given for each individual in the data. The struct is defined in /children-cash-transfers/src/model.jl, but we repeat the definition here:\n\nstruct model_data\n    T::Int64 #&lt;- length of problem\n\n    y0::Int64 #&lt;- year to begin problem\n    age0::Int64 # &lt;- mother's age at start of problem\n    SOI::Vector{Int64} #&lt;- state SOI in each year\n    num_kids::Vector{Int64} #&lt;- number of kids in household that are between age 0 and 17\n    TotKids::Int64 #&lt;- indicares the total number of children that the mother will have over the available panel\n    age_kid::Matrix{Int64} #&lt; age_kid[f,t] is the age of child f at time t. Will be negative if child not born yet.\n    cpi::Vector{Float64} #&lt;- cpi\n\n    R::Vector{Int64} #&lt;- indicates if work requirement in time t\n    Kω::Int64 #&lt;- indicates length of time limit once introduced\n    TL::Vector{Bool} #&lt;- indicating that time limit is in place\nend\n\nEventually we will have one of these objects for every mother we observe in the data, and we’ll solve the resulting dynamic program for each of them. To test our functions below we can create a test version of this struct as well as some default parameters:\n\nmd = test_model()\np = pars(2,5) #&lt;- set Kτ = 2 and Kε = 5\n\n\n\n\nThe function nested_logit takes the value of each choice vj and fills log-choice probabilities into a pre-allocated vector logP. It also returns the inclusive value (the “emax” or continuation value). The input \\(B=(B_1,B_2,...,B_L)\\) specifies the partitions in each layer of the tree, while the input \\(C\\) reports the final choices that are ultimately contained in each node in each layer. By definition, at the highest layer, the partition takes the trivial form \\(B_L=\\{\\{1,2,...K_L\\}\\}\\) where \\(K_L\\) is the number of partitions in layer \\(L-1\\). Similarly at the lowest layer, \\(C_{1}\\) must take the form \\(C_{1} = \\{\\{1\\},\\{2\\},...,\\{J\\}\\}\\). For this model with three participation choices that lead into an extensive marginal labor supply choice, the structure is:\n\nB₁ = [[1,2],[3,4],[5,6]]\nC₁ = [[1,],[2,],[3,],[4,],[5,],[6,]]\n\nB₂ = [[1,2,3]]\nC₂ = [[1,2],[3,4],[5,6]]\n\nB = (B₁,B₂)\nC = (C₁,C₂)\n\nThis is defined in src/model/choices.jl. This is for your understanding, but these inputs are all given to you so you can use the function naively if that is all your time allows for.\n\n\n\nYou may find the following objects useful for iterating over the state variables. Let \\(k_{\\tau}\\in\\{1,...,K_{\\tau}\\}\\) index latent types, let \\(k_{\\varepsilon}\\in\\{1,...,K_{\\varepsilon}\\}\\) index wage shocks, and let \\(k_{\\omega} = \\omega+1\\) index cumulative time use. The total size of the state space is \\(K=K_\\tau\\times K_{\\varepsilon} \\times K_{\\omega}\\).\nOne way to do simple indexing is to just work with multi-dimensional arrays and build this into every function. However if you want to add state variables later on it will make it cumbersome to change. Another option is to use LinearIndices objects and their converse, CartesianIndices.\nHere’s a demonstration:\n\n# Hypothetical state space dimensions:\nKε = 5\nKτ = 5\nKω = 6\n\nk_idx = LinearIndices((Kτ,Kε,Kω))\nk_inv = CartesianIndices(k_idx)\n\n# To get the aggregate index k, call:\nk = k_idx[2,3,2]\n@show k\n# Then if we have k we can work back with:\nk_tuple = Tuple(k_inv[k])\n@show k_tuple\n\nk = 37\nk_tuple = (2, 3, 2)\n\n\n(2, 3, 2)\n\n\nSo when iterating, you could think about passing around state indices along with instances of these linear and cartesian indices that allow you to convert back and forth.\n\n\n\nIn the paper, wage shocks are parameterized with a single parameter \\(\\pi_{W}\\) that dictates the probability that an individual remains in the same place on the grid space, with symmetric probabilities of moving up or down. The function fε in states_transitions.jl takes the current wage shock kε and the total number of shocks Kε, along with πW and returns two tuples. The first tuple is the set of grid points that are possible and the second is the probability of being in each of those points.\nFor example:\n\nfε(3,5,0.9)\n\n((2, 3, 4), (0.04999999999999999, 0.9, 0.04999999999999999))\n\n\nSo the function is telling me that when I’m in state 3 I can move to states 3, 4, or 5 next period with probabilities (0.05,0.9,0.05). When we are at the bottom or the top of the grid space, the probabilities are slightly different:\n\nfε(1,5,0.9)\n\n((1, 2), (0.04999999999999999, 0.95))\n\n\nWe could have alternatively just written the transition probabilities into a matrix, but this approach essentially limits us to points with positive probabilities and will simplify iteration."
  },
  {
    "objectID": "assignments/Assignment-3.html#part-1",
    "href": "assignments/Assignment-3.html#part-1",
    "title": "Assignment 3 - Solving the Dynamic Program",
    "section": "",
    "text": "Write a function calc_vj that calculates the choice-specific value (i.e. the deterministic value of the choice) of a particular choice \\(j\\) in a particular time period \\(t\\) given the state and other exogenous variables. If you are confident you can code this however you like, but given the existing setup you might like to write the function in a way that it takes the following inputs:\n\nj: the discrete choice\nt: the time period in the model\nstate: a tuple that contains the state \\((k_\\tau,k_\\varepsilon,k_\\omega)\\) as well as a linear indexing rule\nV: a vector that contains the continuation value for each state at time \\(t+1\\)\npars: the parameters of the model\nmd: an instance of the model_data object that holds all relevant state variables\n\nVerify that your function works by testing it on the model_data instance created by test_model. Use the @time macro to look at evaluation time and memory allocations."
  },
  {
    "objectID": "assignments/Assignment-3.html#part-2",
    "href": "assignments/Assignment-3.html#part-2",
    "title": "Assignment 3 - Solving the Dynamic Program",
    "section": "",
    "text": "Write a function called iterate! that iterates over all states at time period \\(t\\) and fills in choice probabilities and continuation values for period \\(t\\) in pre-allocated arrays. Again, you can do this however you like but here is a suggested set of inputs:\n\nt: the time period\nlogP: a \\(J \\times K \\times T\\) array of choice probabilities where the function will fill in logP[:,:,t]\nV: a \\(K \\times T\\) array of continuation values\nstate_idx: a named tuple that contains the size of the overall state space, a linear indexing rule that maps \\((k_\\tau,k_{\\varepsilon},k_{\\omega}\\)) to an overall state \\(k\\), and a Cartesian Indexing rule that inverts this mapping\nvj: a \\(J\\)-dimensional vector that, for each state, can be used as a container for the choice-specific values\npars: model parameters\nmd: model_data for the problem\n\nVerify that your function works by testing it on the model_data instance created by test_model. Use the @time macro to look at evaluation time and memory allocations."
  },
  {
    "objectID": "assignments/Assignment-3.html#part-3",
    "href": "assignments/Assignment-3.html#part-3",
    "title": "Assignment 3 - Solving the Dynamic Program",
    "section": "",
    "text": "Write a function called solve! that performs backward induction to calculate continuation values and choice probabilities (storing them in pre-allocated arrays) in every period of the data across the whole state space. As before, some suggested inputs:\n\nlogP: a \\(J\\times K\\times T\\) array for choice probabilities\nV: a \\(K \\times T\\) array for continuation values\nvj: a container or buffer for choice-specific values in each iteration\npars: model parameters\nmd: an instance of model_data\n\nVerify that your function works by testing it on the model_data instance created by test_model. Use the @time macro to look at evaluation time and memory allocations."
  },
  {
    "objectID": "assignments/Assignment-2-solutions.html",
    "href": "assignments/Assignment-2-solutions.html",
    "title": "Assignment 2 - Solutions",
    "section": "",
    "text": "Read in the data as before.\nusing CSV, DataFrames, DataFramesMeta, Optim\ninclude(\"../children-cash-transfers/src/Transfers.jl\")\n\n# for this simple model we can just drop missing data. When we estimate the model with persistent latent heterogeneity, we will need complete panels (including years where choices are missing).\n\ndata = @chain begin\n    CSV.read(\"../children-cash-transfers/data/MainPanelFile.csv\",DataFrame,missingstring = \"NA\")\n    #@select :MID :year :wage :hrs :earn :SOI :CPIU :WelfH :FSInd :num_child :age\n    @subset :year.&gt;=1985 :year.&lt;=2010\n    @transform :AFDC = :WelfH.&gt;0 \n    @rename :FS = :FSInd\n    @transform :P  = :FS + :AFDC :H = min.(2,round.(Union{Int64, Missing},:hrs / (52*20)))\n    @subset .!ismissing.(:P) .&& .!ismissing.(:H)\n    @transform @byrow :wage = begin\n        if :hrs&gt;0 && :earn&gt;0\n            return :earn / :hrs / :CPIU\n        else\n            return missing\n        end\n    end\nend\n\ndata_mle = (;P = Int64.(data.P), H = Int64.(data.H), year = data.year, age = data.age,\n            soi = data.SOI, num_kids = data.num_child, cpi = data.CPIU,\n            logwage = log.(coalesce.(data.wage,1.)),wage_missing = ismissing.(data.wage))\n\n(P = [2, 2, 2, 1, 0, 0, 0, 2, 1, 0  …  2, 2, 1, 2, 0, 0, 1, 1, 1, 1], H = [0, 0, 0, 0, 0, 0, 0, 0, 0, 2  …  0, 0, 2, 2, 2, 2, 0, 0, 0, 0], year = [1994, 1995, 1996, 1998, 2000, 2002, 2004, 2006, 2008, 1990  …  1991, 1992, 1991, 1992, 1991, 1992, 1991, 1992, 1991, 1992], age = [21, 22, 23, 25, 27, 29, 31, 33, 35, 17  …  21, 22, 23, 24, 39, 40, 30, 31, 25, 26], soi = [43, 43, 43, 43, 43, 43, 43, 43, 43, 17  …  44, 44, 7, 7, 5, 5, 39, 39, 39, 39], num_kids = [2, 2, 3, 3, 3, 3, 3, 3, 3, 1  …  2, 2, 2, 2, 3, 3, 4, 4, 2, 2], cpi = [0.860812349005761, 0.884959812302546, 0.910948243820851, 0.946664188812488, 1.0, 1.04457233785542, 1.09707768072849, 1.17054218546739, 1.25008130459022, 0.758792510685746  …  0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336], logwage = [0.0, 0.0, 0.0, 0.0, 1.2039728043259361, 0.0, 0.0, 0.0, 0.0, 0.8230555833449215  …  0.0, 0.0, 2.6512574120409043, 2.3314412292534223, 1.265948758245773, 1.8744481199656744, 0.0, 0.0, 0.0, 0.0], wage_missing = Bool[1, 1, 1, 1, 0, 1, 1, 1, 1, 0  …  1, 1, 0, 0, 0, 0, 1, 1, 1, 1])\nUtility and indexing:\nj_idx(p,h) = p*3 + h + 1\nfunction utility(p,h,soi,cpi,year,num_kids,wage,pars)\n    (;αl,Hgrid,σ,αP) = pars\n    hrs = Hgrid[1+h]\n    earn = wage * hrs\n    net_income = max(50.,Transfers.budget(earn,0.,soi,year,num_kids,cpi,p))\n    return log(net_income) + αl*log(112-hrs) - αP[1]*(p&gt;0) - αP[2]*(p&gt;1)\nend\n\npars = (;αl = 1., σ = 1., σW = 1., γ = zeros(3),\n    Hgrid = [0,20.,40.],αP = zeros(2))\n\n(αl = 1.0, σ = 1.0, σW = 1.0, γ = [0.0, 0.0, 0.0], Hgrid = [0.0, 20.0, 40.0], αP = [0.0, 0.0])\nChoice probabilities are given by the logit formula.\nfunction choice_prob!(logP,it,data,pars)\n    (;σ,γ) = pars\n    wage = exp(γ[1] + γ[2] * data.age[it] + γ[3] * data.age[it]^2)\n    denom = 0.\n    umax = -Inf\n    for p in 0:2\n        for h in 0:2\n            j = j_idx(p,h)\n            u = utility(p,h,data.soi[it],data.cpi[it],data.year[it],data.num_kids[it],wage,pars)\n            logP[j] = u / σ\n            umax = u&gt;umax ? u : umax\n        end\n    end\n    logP[:] .-= umax / σ\n    denom = log(sum(exp.(logP)))\n    logP[:] .-= denom #&lt;- nornalize choice probabilities\nend\n\nchoice_prob! (generic function with 1 method)\nAnd so the log-likelihood:\nfunction log_likelihood(logP,it,data,pars)\n    (;γ,σW) = pars\n    ll = 0.\n    if !data.wage_missing[it]\n        ll += -0.5 * ((data.logwage[it] - γ[1] - γ[2]*data.age[it] - γ[3]*data.age[it]^2) / σW)^2 - log(σW)\n    end\n    choice_prob!(logP,it,data,pars)\n    j = j_idx(data.P[it],data.H[it])\n    ll += logP[j]\n    return ll\nend\n\nfunction update(x,p)\n    αl = exp(x[1])\n    σ = exp(x[2])\n    γ = x[3:5]\n    σW = exp(x[6])\n    αP = x[7:8]\n    return (;p...,αl,σ,σW,αP,γ)\nend\n\nfunction log_likelihood(x,data,pars)\n    pars = update(x,pars)\n    logP = zeros(eltype(x),9)\n    ll = 0.\n    for it in eachindex(data.P)\n        ll += log_likelihood(logP,it,data,pars)\n    end\n    return ll\nend\n\nlog_likelihood (generic function with 2 methods)\nWe can use Optim to maximize the log-likelihood:\nx0 = zeros(8)\nx0 = [0.,0.,log(10.),0.,0.,0.,0.,0.]\nres = optimize(x-&gt;-log_likelihood(x,data_mle,pars),x0,BFGS(),autodiff=:forward,Optim.Options(show_trace = true))\npars = update(res.minimizer,pars)\n\nIter     Function value   Gradient norm \n     0     7.234596e+04     3.085114e+06\n * time: 0.0063669681549072266\n     1     7.219007e+04     7.930873e+05\n * time: 0.4502248764038086\n     2     7.189756e+04     8.240034e+05\n * time: 0.5542259216308594\n     3     6.311750e+04     1.040038e+06\n * time: 0.789099931716919\n     4     5.773072e+04     3.637938e+06\n * time: 1.1824579238891602\n     5     5.738226e+04     6.079719e+06\n * time: 1.339035987854004\n     6     5.692685e+04     7.218571e+06\n * time: 1.5397050380706787\n     7     5.644027e+04     8.094668e+06\n * time: 1.91392183303833\n     8     5.629959e+04     8.406176e+06\n * time: 1.9684689044952393\n     9     5.408423e+04     4.483122e+06\n * time: 2.005316972732544\n    10     5.315575e+04     3.603074e+06\n * time: 2.042008876800537\n    11     5.209566e+04     2.625888e+05\n * time: 2.097062826156616\n    12     5.195541e+04     2.083292e+05\n * time: 2.1336588859558105\n    13     5.188423e+04     6.807290e+05\n * time: 2.170222043991089\n    14     5.187199e+04     2.396172e+05\n * time: 2.2251899242401123\n    15     5.186933e+04     5.094151e+03\n * time: 2.2616169452667236\n    16     5.186887e+04     7.774766e+03\n * time: 2.3174760341644287\n    17     5.186878e+04     2.823744e+04\n * time: 2.3776559829711914\n    18     5.186871e+04     1.085063e+04\n * time: 2.4334499835968018\n    19     5.186852e+04     2.080594e+04\n * time: 2.5089468955993652\n    20     5.186850e+04     5.468883e+02\n * time: 2.5645508766174316\n    21     5.186850e+04     4.523469e+00\n * time: 2.619621992111206\n    22     5.186850e+04     8.466642e-01\n * time: 2.675424814224243\n    23     5.186850e+04     3.014776e-02\n * time: 2.7305779457092285\n    24     5.186850e+04     1.164486e-03\n * time: 2.7672388553619385\n    25     5.186850e+04     1.957495e-05\n * time: 2.8266618251800537\n    26     5.186850e+04     1.250626e-07\n * time: 2.865818977355957\n    27     5.186850e+04     1.206905e-08\n * time: 2.9028689861297607\n    28     5.186850e+04     3.893547e-09\n * time: 2.9430859088897705\n\n\n(αl = 2.8546233783929473, σ = 0.9509286759263859, σW = 0.7224860062936598, γ = [-0.023620750582789916, 0.10877398375254817, -0.0012722347394312002], Hgrid = [0.0, 20.0, 40.0], αP = [2.035301636135555, 1.0137619542631466])"
  },
  {
    "objectID": "assignments/Assignment-2-solutions.html#question-1",
    "href": "assignments/Assignment-2-solutions.html#question-1",
    "title": "Assignment 2 - Solutions",
    "section": "Question 1",
    "text": "Question 1\nLet’s use the Hessian of the average log-likelihood to estimate the standard errors.\n\nusing ForwardDiff\nN = length(data_mle.age)\nH = ForwardDiff.hessian(x-&gt;log_likelihood(x,data_mle,pars),res.minimizer) / N\nV = - H / N\n\n8×8 Matrix{Float64}:\n  1.04029e-5    4.11235e-6   -6.42072e-6  …  -2.46919e-6  -1.15412e-6\n  4.11235e-6    1.95866e-5   -1.25177e-5     -1.15811e-5  -5.76085e-6\n -6.42072e-6   -1.25177e-5    6.01038e-5      5.73597e-6   2.50154e-6\n -0.000199471  -0.000389124   0.00193279      0.00017803   7.68593e-5\n -0.00665377   -0.0130835     0.0662433       0.00587496   0.00250189\n -0.0          -0.0           5.82263e-6  …  -0.0         -0.0\n -2.46919e-6   -1.15811e-5    5.73597e-6      9.58444e-6   3.41442e-6\n -1.15412e-6   -5.76085e-6    2.50154e-6      3.41442e-6   4.67709e-6\n\n\nNow we can make the table, but we have to be careful because some of our parameters are transformations of the vector res.minimizer (so we use the delta method to get the standard error of the transformed value)\n\nusing LinearAlgebra\np_str = [\"αl\",\"σ\",\"γ₁\",\"γ₂\",\"γ₃\",\"σW\",\"αP₁\",\"αP₂\"]\np_est = [pars.αl ; pars.σ ; pars.γ ; pars.σW ; pars.αP]\nse = sqrt.(diag(V))\np_se = [se[1:2].*p_est[1:2] ; se[3:5] ; se[6].*p_est[6] ; se[7:8]]\nd = DataFrame(par = p_str, est = p_est, se = p_se)\n\n8×3 DataFrame\n\n\n\nRow\npar\nest\nse\n\n\n\nString\nFloat64\nFloat64\n\n\n\n\n1\nαl\n2.85462\n0.00920715\n\n\n2\nσ\n0.950929\n0.00420851\n\n\n3\nγ₁\n-0.0236208\n0.00775266\n\n\n4\nγ₂\n0.108774\n0.257378\n\n\n5\nγ₃\n-0.00127223\n9.6305\n\n\n6\nσW\n0.722486\n0.00529469\n\n\n7\nαP₁\n2.0353\n0.00309587\n\n\n8\nαP₂\n1.01376\n0.00216266"
  },
  {
    "objectID": "assignments/Assignment-2-solutions.html#question-2",
    "href": "assignments/Assignment-2-solutions.html#question-2",
    "title": "Assignment 2 - Solutions",
    "section": "Question 2",
    "text": "Question 2\nWhat sources of variation are identifying \\(\\sigma\\), the responsiveness of choices to changes in the payoffs? How is the responsivess of program participation and labor supply to changes in incentives limited in this framework?\nSome comments we went over in class:\n\n\\(\\sigma\\) is identified to by covariation in the probability of making particular decisions with changes in the financial returns to those decisions, these in turn are driven by\nThe responsiveness of labor supply and program participation are therefore both dictated by \\(\\sigma\\)"
  },
  {
    "objectID": "assignments/Assignment-2-solutions.html#question-3",
    "href": "assignments/Assignment-2-solutions.html#question-3",
    "title": "Assignment 2 - Solutions",
    "section": "Question 3",
    "text": "Question 3\nFirst we’ll write the function to calculate nested logit probabilities given a vector of utilities u and a partition B.\nIn the next problem set I’ll introduce a function for evaluating the nested logit for a general number of layers. The function below will work for any nested logit with only two layers (i.e. one partition).\n\nfunction nested_logit(logP,u,σ,B)\n    # calculate nest probabilities and inclusive values\n    for k in eachindex(B)\n        Bₖ = B[k]\n        # find the max\n        vmax = -Inf\n        for j in Bₖ\n            vmax = u[j] &gt; vmax ? u[j] : vmax\n        end\n        # calculate choice probs\n        iv = 0.\n        for j in Bₖ\n            logP[j] += (u[j] - vmax) / σ[2]\n            iv += exp((u[j] - vmax) / σ[2])\n        end\n        logP[Bₖ] .-= log(iv)\n        u[k] = vmax + σ[2] * log(iv) #&lt;- fill in inclusive value\n    end\n    # now calculate the partition probabilities\n    vmax = -Inf\n    for k in eachindex(B)\n        vmax = u[k] &gt; vmax ? u[k] : vmax\n    end\n    iv = 0.\n    for k in eachindex(B)\n        iv += exp((u[k]-vmax) / σ[1])\n        for j in B[k]\n            logP[j] += (u[k]-vmax) / σ[1]\n        end\n    end\n    logP[:] .-= log(iv)\n    V = vmax + σ[1] * log(iv)\nend\n\nnested_logit (generic function with 1 method)\n\n\nNow let’s write a new set of log-likelihood routines.\n\nfunction fill_utilities!(u,it,data,pars)\n    (;γ) = pars\n    wage = exp(γ[1] + γ[2]*data.age[it] + γ[3]*data.age[it]^2)\n    for p in 0:2\n        for h in 0:2\n            j = j_idx(p,h)\n            u[j] = utility(p,h,data.soi[1],data.cpi[1],data.year[1],data.num_kids[1],wage,pars)\n        end\n    end\nend\nfunction log_likelihood(logP,u,it,data,pars)\n    (;γ,σW,σ,B) = pars\n    ll = 0.\n    if !data.wage_missing[it]\n        ll += -0.5 * ((data.logwage[it] - γ[1] - γ[2]*data.age[it] - γ[3]*data.age[it]^2) / σW)^2 - log(σW)\n    end\n    fill_utilities!(u,it,data,pars)\n    V = nested_logit(logP,u,σ,B)\n    j = j_idx(data.P[it],data.H[it])\n    ll += logP[j]\n    return ll\nend\n\nfunction update(x,p)\n    αl = exp(x[1])\n    σ = exp.(x[2:3])\n    γ = x[4:6]\n    σW = exp(x[7])\n    αP = x[8:9]\n    return (;p...,αl,σ,σW,αP,γ)\nend\n\nfunction log_likelihood(x,data,pars)\n    pars = update(x,pars)\n    logP = zeros(eltype(x),9)\n    u = zeros(eltype(x),9)\n    ll = 0.\n    for it in eachindex(data.P)\n        fill!(logP,0.)\n        ll += log_likelihood(logP,u,it,data,pars)\n    end\n    return ll\nend\n\nlog_likelihood (generic function with 3 methods)\n\n\nAnd here is code to maximize the log-likelihood. Notice that we get very different participation elasticities vs labor force elasticities.\n\nB = [[1,2,3],[4,5,6],[7,8,9]]\npars = (;αl = 1., σ = [1.,1.], σW = 1., γ = zeros(3),\n    Hgrid = [0,20.,40.],αP = zeros(2), B)\nx0 = zeros(9)\nx0 = [0.,0.,log(10.),0.,0.,0.,0.,0.,0.]\nres = optimize(x-&gt;-log_likelihood(x,data_mle,pars),x0,BFGS(),autodiff=:forward,Optim.Options(show_trace = true))\npars = update(res.minimizer,pars)\n\nIter     Function value   Gradient norm \n     0     1.298462e+05     4.851678e+07\n * time: 8.678436279296875e-5\n     1     8.428536e+04     6.296835e+05\n * time: 0.23514485359191895\n     2     7.391455e+04     5.520307e+05\n * time: 0.38832688331604004\n     3     6.370511e+04     4.396082e+06\n * time: 0.6106898784637451\n     4     6.345510e+04     1.117865e+06\n * time: 0.7055327892303467\n     5     5.652808e+04     3.601002e+06\n * time: 1.1712467670440674\n     6     5.644734e+04     3.879931e+06\n * time: 1.3372769355773926\n     7     5.637478e+04     4.047389e+06\n * time: 1.728111982345581\n     8     5.631564e+04     4.429878e+06\n * time: 2.0891449451446533\n     9     5.625670e+04     4.426924e+06\n * time: 2.5212409496307373\n    10     5.621568e+04     4.487090e+06\n * time: 2.5944719314575195\n    11     5.445576e+04     5.568793e+06\n * time: 2.6421899795532227\n    12     5.321236e+04     3.804673e+06\n * time: 2.690370798110962\n    13     5.309794e+04     2.771147e+06\n * time: 2.7412807941436768\n    14     5.263932e+04     1.965296e+06\n * time: 2.790564775466919\n    15     5.251223e+04     1.026082e+06\n * time: 2.841972827911377\n    16     5.249235e+04     4.504616e+05\n * time: 2.8927547931671143\n    17     5.248819e+04     5.498308e+04\n * time: 2.944981813430786\n    18     5.248768e+04     6.342023e+04\n * time: 2.994086980819702\n    19     5.248734e+04     6.166998e+02\n * time: 3.0707948207855225\n    20     5.248377e+04     2.627173e+05\n * time: 3.1757378578186035\n    21     5.247479e+04     3.265779e+05\n * time: 3.2517528533935547\n    22     5.247121e+04     3.335803e+05\n * time: 3.3013668060302734\n    23     5.246340e+04     2.918189e+05\n * time: 3.3502278327941895\n    24     5.245355e+04     2.052197e+05\n * time: 3.4001967906951904\n    25     5.244366e+04     1.847566e+05\n * time: 3.4750797748565674\n    26     5.243858e+04     4.394642e+04\n * time: 3.5480167865753174\n    27     5.243507e+04     6.597019e+04\n * time: 3.5967459678649902\n    28     5.243380e+04     1.882748e+04\n * time: 3.6472280025482178\n    29     5.243337e+04     4.479266e+04\n * time: 3.6955578327178955\n    30     5.243331e+04     1.261248e+03\n * time: 3.7441039085388184\n    31     5.243331e+04     4.613115e+02\n * time: 3.7943758964538574\n    32     5.243331e+04     2.374307e+02\n * time: 3.8450160026550293\n    33     5.243331e+04     7.004367e+00\n * time: 3.896716833114624\n    34     5.243331e+04     6.215960e-01\n * time: 3.9466938972473145\n    35     5.243331e+04     9.749320e-02\n * time: 3.9989638328552246\n    36     5.243331e+04     4.051009e-03\n * time: 4.047600984573364\n    37     5.243331e+04     6.876542e-06\n * time: 4.096721887588501\n    38     5.243331e+04     2.178613e-07\n * time: 4.168418884277344\n    39     5.243331e+04     1.537637e-08\n * time: 4.217536926269531\n    40     5.243331e+04     7.319159e-09\n * time: 4.26809287071228\n\n\n(αl = 2.8294318298128394, σ = [0.37004828541787427, 1.3098778384091117], σW = 0.7186963786509343, γ = [0.672930718912166, 0.06812663769542579, -0.0006981805671947965], Hgrid = [0.0, 20.0, 40.0], αP = [1.5170804560061646, 0.5324967821971897], B = [[1, 2, 3], [4, 5, 6], [7, 8, 9]])"
  },
  {
    "objectID": "assignments/Assignment-2-solutions.html#question-4",
    "href": "assignments/Assignment-2-solutions.html#question-4",
    "title": "Assignment 2 - Solutions",
    "section": "Question 4",
    "text": "Question 4\nWe discussed this in class but let me know if you would like this to be discussed further."
  },
  {
    "objectID": "assignments/Assignment-4.html",
    "href": "assignments/Assignment-4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "This week you will estimate a simple hidden markov model using Expectation-Maximization.\nHere is the model. There is a discrete state variable \\(k\\in\\{1,2,3,..,K\\}\\) and a binary outcome \\(j\\in\\{0,1\\}\\) that:\nLet \\(p\\) be a \\(K\\)-dimensional vector where \\(p_{k}\\) holds the probability that \\(j=1\\) given that the model is in state \\(k\\).\nThe state \\(k\\) is never observed and the outcome \\(j\\) is only observed half the time (i.e. it is missing with probability 0.5). Thus, define \\(j^*\\) to be:\n\\[ j^* = \\left\\{ \\begin{array}{cl} j & \\text{with probability}\\ 0.5 \\\\ -1 & \\text{with probability}\\ 0.5 \\end{array} \\right. \\]\nYour task is to estimate the vector of outcome probabilities \\(p\\) non-parametrically using the EM algorithm with the Forward-Back routine to calculate the E-step.\nThe chunk of code below parameterizes the true vector \\(p\\) for this model and writes code to simulate panel data:\n\\[ (j^*_{nt})_{t=1,n=1}^{T,N} \\]\nTo start with, we’ll assume \\(k_{n,1} = 1\\) for all \\(n\\).\nusing Random, Distributions\nK = 5\nPj = 1 ./ (1 .+ exp.(LinRange(-1,1,K))) #&lt;- choice probability as a function of k\nknext(k,j,K) = min(K,k+j) \n\nfunction simulate(N,T,Pj)\n    J = zeros(T,N)\n    K = length(Pj)\n    for n in axes(J,2)\n        k = 1\n        for t in axes(J,1)\n            j = rand()&lt;Pj[k]\n            # record j probabilistically\n            if rand()&lt;0.5\n                J[t,n] = -1\n            else\n                J[t,n] = j\n            end\n            # update state:\n            k = knext(k,j,K)\n        end\n    end\n    return J\nend\n\nJ_data = simulate(1000,10,Pj)\n\n10×1000 Matrix{Float64}:\n  1.0   1.0  -1.0  -1.0  -1.0  -1.0  …  -1.0   1.0   0.0   1.0   1.0  -1.0\n -1.0   1.0  -1.0   0.0  -1.0  -1.0      1.0   1.0  -1.0   0.0  -1.0  -1.0\n -1.0  -1.0   1.0   0.0  -1.0  -1.0      1.0  -1.0   0.0   0.0  -1.0  -1.0\n  1.0  -1.0  -1.0   1.0   1.0  -1.0     -1.0  -1.0  -1.0  -1.0  -1.0  -1.0\n  0.0  -1.0   1.0  -1.0  -1.0   1.0     -1.0   1.0  -1.0   1.0  -1.0   1.0\n  0.0   1.0  -1.0   1.0  -1.0  -1.0  …   1.0   0.0   0.0  -1.0   0.0   1.0\n  0.0  -1.0   1.0   0.0  -1.0  -1.0     -1.0  -1.0  -1.0   1.0   0.0  -1.0\n -1.0  -1.0  -1.0   0.0   0.0  -1.0      0.0  -1.0  -1.0  -1.0   1.0  -1.0\n -1.0  -1.0   0.0   0.0  -1.0  -1.0     -1.0  -1.0   0.0   1.0  -1.0  -1.0\n -1.0  -1.0   1.0   0.0   0.0  -1.0      0.0  -1.0   0.0   0.0  -1.0   1.0\nSince the outcomes \\(j\\) are periodically unobserved, there are two ways to set up the EM problem:\nWhen the number of discrete outcomes is larger, it becomes more difficult to integrate them out when missing. Since \\(j\\) is only binary here, this homework will step you through using the second approach. You can use the first approach if you prefer."
  },
  {
    "objectID": "assignments/Assignment-4.html#part-1",
    "href": "assignments/Assignment-4.html#part-1",
    "title": "Assignment 4",
    "section": "Part 1",
    "text": "Part 1\nWrite a function that (given a guess of the parameters \\(p\\)) takes a sequence of \\((j^*)^{T}_{t=1}\\) and runs the forward-back algorithm. In doing so your function should fill in three objects: (1) A \\(K\\times T\\) array of backward looking probabilities (\\(\\alpha\\)); (2) a \\(K\\times T\\) array of forward probabilities (\\(\\beta\\)); and (3) a \\(K\\times T\\) array of posterior probabilities over each state \\(k\\) (\\(Q\\)).\nRecall from class that (1) \\(\\alpha\\) is the joint probability of the state today with the sequence of outcomes up until today; and (2) \\(\\beta\\) is the conditional probability of future outcomes given the state today.\n\\[ \\alpha[k,s] = \\mathbb{P}[k_{s}=k,(j^*)^{s}_{t=1}] \\]\n\\[ \\beta[k,s] = \\mathbb{P}[(j^*)_{t=s+1}^{T} | k_{s}=k] \\]\nand\n\\[ Q[k,s] = \\mathbb{P}[k_{s}=k | (j^*)_{t=1}^{T} ] \\]"
  },
  {
    "objectID": "assignments/Assignment-4.html#part-2",
    "href": "assignments/Assignment-4.html#part-2",
    "title": "Assignment 4",
    "section": "Part 2",
    "text": "Part 2\nWrite a function that iterates over all observations and calculates posterior state probabilities for every observation. i.e. fill in a \\(K\\times T \\times N\\) array of posterior weights."
  },
  {
    "objectID": "assignments/Assignment-4.html#part-3",
    "href": "assignments/Assignment-4.html#part-3",
    "title": "Assignment 4",
    "section": "Part 3",
    "text": "Part 3\nTake as given a set of posterior weights \\(q_{ntk} = Q[n,t,k]\\). The expected log-likelihood for the M-step is:\n\\[ \\mathcal{L}(p) = \\sum_{n}\\sum_{t}\\sum_{k}q_{ntk}\\left(\\mathbf{1}\\{j^*_{nt}=1\\}\\log(p_{k}) + \\mathbf{1}\\{j^*_{nt}=0\\}\\log(1-p_{k})\\right) \\]\nShow that the non-parametric maximum likelihood estimate of \\(p\\) given the posterior weights is a frequency estimator:\n\\[ \\hat{p}_{k} = \\frac{\\sum_{n}\\sum_{t}\\mathbf{1}\\{j^*_{nt}=1\\} q_{ntk}}{\\sum_{n}\\sum_{t}\\mathbf{1}\\{j^*_{nt}\\neq -1\\} q_{ntk}} \\]\nWrite a function to calculate this frequency estimator given posterior weights."
  },
  {
    "objectID": "assignments/Assignment-4.html#part-4",
    "href": "assignments/Assignment-4.html#part-4",
    "title": "Assignment 4",
    "section": "Part 4",
    "text": "Part 4\nRun the E-M routine by iterating on this E-step and M-step until you get convergence in \\(\\hat{p}\\). Does it look like you can recover the true parameters?"
  },
  {
    "objectID": "assignments/Assignment-4.html#extra-credit",
    "href": "assignments/Assignment-4.html#extra-credit",
    "title": "Assignment 4",
    "section": "Extra credit",
    "text": "Extra credit\nIf you found this exercise too straightforward, try adjusting the model so that transitions depend probabilistically on the state \\(k\\) and the choice \\(j\\). This requires you to update also posterior transition probabilities and estimate those transition probabilities as part of the maximization step."
  },
  {
    "objectID": "assignments/Assignment-4-solutions.html",
    "href": "assignments/Assignment-4-solutions.html",
    "title": "Assignment 4",
    "section": "",
    "text": "This week you will estimate a simple hidden markov model using Expectation-Maximization.\nHere is the model. There is a discrete state variable \\(k\\in\\{1,2,3,..,K\\}\\) and a binary outcome \\(j\\in\\{0,1\\}\\) that:\nLet \\(p\\) be a \\(K\\)-dimensional vector where \\(p_{k}\\) holds the probability that \\(j=1\\) given that the model is in state \\(k\\).\nThe state \\(k\\) is never observed and the outcome \\(j\\) is only observed half the time (i.e. it is missing with probability 0.5). Thus, define \\(j^*\\) to be:\n\\[ j^* = \\left\\{ \\begin{array}{cl} j & \\text{with probability}\\ 0.5 \\\\ -1 & \\text{with probability}\\ 0.5 \\end{array} \\right. \\]\nYour task is to estimate the vector of outcome probabilities \\(p\\) non-parametrically using the EM algorithm with the Forward-Back routine to calculate the E-step.\nThe chunk of code below parameterizes the true vector \\(p\\) for this model and writes code to simulate panel data:\n\\[ (j^*_{nt})_{t=1,n=1}^{T,N} \\]\nTo start with, we’ll assume \\(k_{n,1} = 1\\) for all \\(n\\).\nusing Random, Distributions\nK = 5\nT = 10\nPj = 1 ./ (1 .+ exp.(LinRange(-1,1,K))) #&lt;- choice probability as a function of k\nknext(k,j,K) = min(K,k+j) \n\nfunction simulate(N,T,Pj)\n    J = zeros(Int64,T,N)\n    K = length(Pj)\n    for n in axes(J,2)\n        k = 1\n        for t in axes(J,1)\n            j = rand()&lt;Pj[k]\n            # record j probabilistically\n            if rand()&lt;0.5\n                J[t,n] = -1\n            else\n                J[t,n] = j\n            end\n            # update state:\n            k = knext(k,j,K)\n        end\n    end\n    return J\nend\n\nN = 1000\nJ_data = simulate(N,T,Pj)\n\n10×1000 Matrix{Int64}:\n -1  -1   1  -1  -1  -1   0   1   1  …  -1  -1   1   1   0  -1  -1   1   0\n -1   1  -1  -1  -1   0  -1  -1  -1     -1   0   1   1   1   1   1   1   1\n -1  -1  -1   0  -1   0  -1  -1   0      1  -1  -1  -1   1  -1   0   1  -1\n  1  -1   1   1   1   1   1   0  -1      1   1   0   0   1  -1  -1   0   0\n  1   1   1   0   1   1  -1  -1  -1      1   1  -1  -1   0  -1  -1  -1   0\n -1  -1  -1  -1  -1   1  -1  -1   1  …   0   0  -1  -1   1   1  -1   0   0\n  0   1  -1   0   0  -1  -1   0  -1     -1  -1   0  -1  -1  -1   1  -1  -1\n  0   1  -1   0   1   0  -1   1  -1      0  -1   0   0   0  -1  -1   0  -1\n  0   0   0   0  -1   0  -1  -1   0     -1   1  -1  -1   0   0  -1   0   0\n -1  -1   0  -1   0   0  -1   1  -1     -1   0  -1  -1   0  -1   0   0   0\nSince the outcomes \\(j\\) are periodically unobserved, there are two ways to set up the EM problem:\nWhen the number of discrete outcomes is larger, it becomes more difficult to integrate them out when missing. Since \\(j\\) is only binary here, this homework will step you through using the second approach. You can use the first approach if you prefer."
  },
  {
    "objectID": "assignments/Assignment-4-solutions.html#part-1",
    "href": "assignments/Assignment-4-solutions.html#part-1",
    "title": "Assignment 4",
    "section": "Part 1",
    "text": "Part 1\nWrite a function that (given a guess of the parameters \\(p\\)) takes a sequence of \\((j^*)^{T}_{t=1}\\) and runs the forward-back algorithm. In doing so your function should fill in three objects: (1) A \\(K\\times T\\) array of backward looking probabilities (\\(\\alpha\\)); (2) a \\(K\\times T\\) array of forward probabilities (\\(\\beta\\)); and (3) a \\(K\\times T\\) array of posterior probabilities over each state \\(k\\) (\\(Q\\)).\nRecall from class that (1) \\(\\alpha\\) is the joint probability of the state today with the sequence of outcomes up until today; and (2) \\(\\beta\\) is the conditional probability of future outcomes given the state today.\n\\[ \\alpha[k,s] = \\mathbb{P}[k_{s}=k,(j^*)^{s}_{t=1}] \\]\n\\[ \\beta[k,s] = \\mathbb{P}[(j^*)_{t=s+1}^{T} | k_{s}=k] \\]\nand\n\\[ Q[k,s] = \\mathbb{P}[k_{s}=k | (j^*)_{t=1}^{T} ] \\]\nThis is a problem where there is a lot of sparseness. For the size of this problem we won’t have to worry about exploiting that, but it’s something you want to think about for larger problems. I’ll introduce you to a sparse implementation in class and in code.\nSo, ignoring sparsenss, here is a function that gives transition probabilities given the three potential outcomes for \\(j^*\\).\n\nusing LinearAlgebra\n# this function assumes Π is a K x K x 3 array of transition probabilities where\nfunction get_transitions!(Π,p)\n    fill!(Π,0.)\n    # for j^*=-1 (i.e. missing)\n    for k in axes(Π,2)\n        Π[k,k,1] += p[k]\n        k_up = min(K,k+1)\n        Π[k_up,k,1] += 1-p[k]\n    end\n    # for j^* = 0\n    Π[:,:,2] = I(K) #&lt;- transitions are the identity matrix\n    # for j^*=1\n    for k in axes(Π,2)\n        k_up = min(K,k+1)\n        Π[k_up,k,3] = 1.\n    end\nend\n\nΠ = zeros(K,K,3)\nget_transitions!(Π,Pj)\n\nNow a function for the forward-back routine:\n\nfunction forward_back!(Q,α,β,j_obs,Π,p)\n    K,T = size(α)\n    fill!(β,0.)\n    @views fill!(β[:,T],1.)\n    fill!(α,0.)\n    α[1,1] = 1. #&lt;- since we know that all units begin in state 1\n    # run it forward\n    for t in 2:T\n        jlag_idx = 2 + j_obs[t-1]\n        for k in axes(α,1)\n            for klag in axes(α,1)\n                α[k,t] += Π[k,klag,jlag_idx] *  α[klag,t-1]\n            end\n            if j_obs[t]&gt;-1\n                α[k,t] *= j_obs[t]*p[k] + (1-j_obs[t])*(1-p[k])\n            end\n        end\n    end\n    # run it back\n    for t in reverse(1:T-1)\n        j_idx = 2 + j_obs[t]\n        for k in axes(β,1)\n            for knext in axes(β,1)\n                if j_obs[t+1]!=-1\n                    pj = j_obs[t+1]*p[knext] + (1-j_obs[t+1])*(1-p[knext])\n                else\n                    pj = 1.\n                end\n                β[k,t] += pj * Π[knext,k,j_idx]\n            end\n        end\n    end\n    # calculate the posteriors\n    for t in axes(Q,2)\n        @views Q[:,t] .= α[:,t] .* β[:,t]\n        @views Q[:,t] ./= sum(Q[:,t])\n    end\nend\n\nα = zeros(K,T)\nβ = zeros(K,T)\nQ = zeros(K,T)\nforward_back!(Q,α,β,J_data[:,1],Π,Pj)"
  },
  {
    "objectID": "assignments/Assignment-4-solutions.html#part-2",
    "href": "assignments/Assignment-4-solutions.html#part-2",
    "title": "Assignment 4",
    "section": "Part 2",
    "text": "Part 2\nWrite a function that iterates over all observations and calculates posterior state probabilities for every observation. i.e. fill in a \\(K\\times T \\times N\\) array of posterior weights.\nI’ll also add a line of code to update the transition matrices \\(\\Pi\\).\n\nfunction forward_back!(Q,α,β,J_obs::Array{Int64},Π,p)\n    get_transitions!(Π,p)\n    for n in axes(J_obs,2)\n        @views forward_back!(Q[:,:,n],α,β,J_obs[:,n],Π,p)\n    end\nend\n\nQ = zeros(K,T,N)\nforward_back!(Q,α,β,J_data,Π,Pj)"
  },
  {
    "objectID": "assignments/Assignment-4-solutions.html#part-3",
    "href": "assignments/Assignment-4-solutions.html#part-3",
    "title": "Assignment 4",
    "section": "Part 3",
    "text": "Part 3\nTake as given a set of posterior weights \\(q_{ntk} = Q[n,t,k]\\). The expected log-likelihood for the M-step is:\n\\[ \\mathcal{L}(p) = \\sum_{n}\\sum_{t}\\sum_{k}q_{ntk}\\left(\\mathbf{1}\\{j^*_{nt}=1\\}\\log(p_{k}) + \\mathbf{1}\\{j^*_{nt}=0\\}\\log(1-p_{k})\\right) \\]\nShow that the non-parametric maximum likelihood estimate of \\(p\\) given the posterior weights is a frequency estimator:\n\\[ \\hat{p}_{k} = \\frac{\\sum_{n}\\sum_{t}\\mathbf{1}\\{j^*_{nt}=1\\} q_{ntk}}{\\sum_{n}\\sum_{t}\\mathbf{1}\\{j^*_{nt}\\neq -1\\} q_{ntk}} \\]\nWrite a function to calculate this frequency estimator given posterior weights.\n\nfunction m_step(Q,J_data)\n    K = size(Q,1)\n    p = zeros(K)\n    denom = zeros(K)\n    for n in axes(J_data,2)\n        for t in axes(J_data,1)\n            if J_data[t,n]!=-1\n                @views denom .+= Q[:,t,n]\n                if J_data[t,n]==1\n                    @views p .+= Q[:,t,n]\n                end\n            end\n        end\n    end\n    return p ./ denom\nend\n\np_est = m_step(Q,J_data)\n\n5-element Vector{Float64}:\n 0.6893915560871474\n 0.548451505221972\n 0.4505939493128934\n 0.32605335098046356\n 0.24167980459619326"
  },
  {
    "objectID": "assignments/Assignment-4-solutions.html#part-4",
    "href": "assignments/Assignment-4-solutions.html#part-4",
    "title": "Assignment 4",
    "section": "Part 4",
    "text": "Part 4\nRun the E-M routine by iterating on this E-step and M-step until you get convergence in \\(\\hat{p}\\). Does it look like you can recover the true parameters?\n\nfunction EM_routine(p0,Π,data ; max_iter = 1000, tol = 1e-7, verbose = true)\n    (;α,β,Q,J_data) = data\n    err = Inf\n    iter = 0\n    while err&gt;tol && iter&lt;max_iter\n        forward_back!(Q,α,β,J_data,Π,p0)\n        p1 = m_step(Q,J_data)\n        err = maximum(abs.(p1 .- p0))\n        iter += 1\n        if mod(iter,10)==0 && verbose\n            println(\"Current error is $err\")\n        end\n        p0 = p1\n    end\n    return p0\nend\n\ndata = (;α,β,Q,J_data)\n\n# let's try using a \n\np_guess = fill(1 / K, K)\np_est = EM_routine(p_guess,Π,data)\n\n[p_est Pj]\n\nCurrent error is 0.002843866497475034\nCurrent error is 0.0005218130317447001\nCurrent error is 8.811623202636953e-5\nCurrent error is 1.4687127339818584e-5\nCurrent error is 2.4432631483550793e-6\nCurrent error is 4.0632648923288883e-7\n\n\n5×2 Matrix{Float64}:\n 0.701427  0.731059\n 0.524607  0.622459\n 0.496549  0.5\n 0.29034   0.377541\n 0.264751  0.268941\n\n\nLooks good! You could test the estimator a bit more by running it over a number of samples to see how you do.\n\nB = 100\nP_montecarlo = zeros(K,B)\nfor b in axes(P_montecarlo,2)\n    J_data = simulate(N,T,Pj)\n    data = (;data...,J_data)\n    P_montecarlo[:,b] = EM_routine(p_guess,Π,data ; verbose = false)\nend\n\n\nusing Plots\nhistogram(P_montecarlo[1,:],legend = false)\nplot!([Pj[1],Pj[1]],[0,30],linewidth=3,legend = false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see a little bit of bias, but then MLE is not guaranteed to be unbiased. You could re-run this with a larger sample size if you wanted to convince yourself of consistency."
  },
  {
    "objectID": "assignments/Assignment-4-solutions.html#extra-credit",
    "href": "assignments/Assignment-4-solutions.html#extra-credit",
    "title": "Assignment 4",
    "section": "Extra credit",
    "text": "Extra credit\nIf you found this exercise too straightforward, try adjusting the model so that transitions depend probabilistically on the state \\(k\\) and the choice \\(j\\). This requires you to update also posterior transition probabilities and estimate those transition probabilities as part of the maximization step."
  },
  {
    "objectID": "assignments/Assignment-5.html",
    "href": "assignments/Assignment-5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "In this assignment you will implement a choice probability estimator for a toy model of dynamic labor supply.\n\n\nAgents in the model make one of four choices (\\(j \\in \\{0,1,2,3\\}\\)) that correspond to a binary work decision (\\(H_{j}\\)) and a binary program participation decision (\\(P_{j}\\)):\n\\[ \\{H_0,\\ H_1,\\ H_2,\\ H_3\\} = \\{0,1,0,1\\},\\qquad \\{P_0,\\ P_1,\\ P_2,\\ P_3\\} = \\{0,0,1,1\\} \\]\n\n\n\nPotential earnings are given by:\n\\[ \\log(W) = \\phi_0 + \\phi_1 \\kappa \\]\nwhere \\(\\kappa\\) is accumulated labor market experience that evolves according to:\n\\[ \\kappa_{t+1} = \\kappa_{t} + H_{jt} \\]\nWeekly net income is a function of this work decision and the participation decision:\n\\[ Y_{j}(W,\\omega) = H_{j}W + P_{j}\\mathbf{1}\\{\\omega&lt;\\Omega\\}(200 - 0.5\\times H_{j}W) \\]\nwhere:\n\n\\(W\\) is the individual’s potential wage\n\\(\\Omega\\) is the lifetime limit on program participation\n\\(\\omega\\) is the individual’s cumulative prior program use.\n\n\n\n\nIndividuals discount the future at an exponential rate of \\(1-\\beta\\). Per-period payoffs are given by:\n\\[ u_{j}(W) = \\log(\\gamma + Y_{j}(W,\\omega)) - \\alpha P_{j} - \\varphi H_{j} \\]\nThe per-period payoff for any individual \\(n\\) at time \\(t\\) is given by\n\\[ U_{ntj} = u_{j}(W_{nt}) + \\epsilon_{ntj} \\]\nwhere \\(\\epsilon_{nt}\\) is a preference shock that has a type 1 extreme value distribution with scale parameter \\(\\sigma\\). It is iid across individuals and time periods and us only observed by individuals when period \\(t\\) arrives.\n\n\n\nAll individuals begin in \\(t=1\\) with \\(\\omega_{1}=0\\) and \\(\\kappa_{1}=0\\) and live for \\(T\\) periods.\n\n\n\nAssume that log-wages are observed with measurement error:\n\\[ \\log(W^o) = \\log(W) + \\xi,\\qquad \\xi\\sim\\mathcal{N}(0,\\sigma^2_\\xi) \\]\n\n\n\nHere is code to solve the model for a default set of parameters.\n\nT = 10 #&lt;- length of life-cycle\nϕ = [2., 0.05]\nΩ = 4 #&lt;- time limit\nwagefunc(κ,ϕ) = exp(ϕ[1] + ϕ[2]*κ)\nnet_income(W,H,P,eligible) = W*H + P*eligible*(200 - 0.5*W*H) \nj_idx(H,P) = P*2 + H + 1\nutility(W,H,P,eligible,pars) = log(pars.γ + net_income(W,H,P,eligible)) - pars.α*P - pars.φ*H   \npars = (;α = 1., φ = 1., σ = 1., γ = 1., Ω, T,ϕ,β = 0.9)\n\nfunction iterate!(P,v,V,pars,ω_idx,κ_idx,t)\n    (;β,ϕ,Ω,σ) = pars\n    ω = ω_idx - 1\n    κ = κ_idx - 1\n    W = wagefunc(κ,ϕ)\n    eligible = ω&lt;Ω\n    for P in 0:1, H in 0:1\n        j = j_idx(H,P)\n        κ_next = min(κ_idx + H,T)\n        ω_next = min(ω_idx + P,Ω+1)\n        v[j]  = utility(W,H,P,eligible,pars) + β*V[ω_next,κ_next,t+1]\n    end\n    norm = sum(exp.(v ./ σ))\n    P[:,ω_idx,κ_idx,t] = exp.(v ./ σ) ./ norm\n    V[ω_idx,κ_idx,t] = σ * log(norm)\nend\n\nfunction solve!(P,v,V,pars)\n    for t in reverse(axes(P,4))\n        for κ_idx in axes(P,3), ω_idx in axes(P,2)\n            iterate!(P,v,V,pars,ω_idx,κ_idx,t)\n        end\n    end\nend\n\nV = zeros(Ω+1,T,T+1)\nP = zeros(4,Ω+1,T,T)\nv = zeros(4)\n\nsolve!(P,v,V,pars)"
  },
  {
    "objectID": "assignments/Assignment-5.html#introduction-and-setup",
    "href": "assignments/Assignment-5.html#introduction-and-setup",
    "title": "Assignment 5",
    "section": "",
    "text": "In this assignment you will implement a choice probability estimator for a toy model of dynamic labor supply.\n\n\nAgents in the model make one of four choices (\\(j \\in \\{0,1,2,3\\}\\)) that correspond to a binary work decision (\\(H_{j}\\)) and a binary program participation decision (\\(P_{j}\\)):\n\\[ \\{H_0,\\ H_1,\\ H_2,\\ H_3\\} = \\{0,1,0,1\\},\\qquad \\{P_0,\\ P_1,\\ P_2,\\ P_3\\} = \\{0,0,1,1\\} \\]\n\n\n\nPotential earnings are given by:\n\\[ \\log(W) = \\phi_0 + \\phi_1 \\kappa \\]\nwhere \\(\\kappa\\) is accumulated labor market experience that evolves according to:\n\\[ \\kappa_{t+1} = \\kappa_{t} + H_{jt} \\]\nWeekly net income is a function of this work decision and the participation decision:\n\\[ Y_{j}(W,\\omega) = H_{j}W + P_{j}\\mathbf{1}\\{\\omega&lt;\\Omega\\}(200 - 0.5\\times H_{j}W) \\]\nwhere:\n\n\\(W\\) is the individual’s potential wage\n\\(\\Omega\\) is the lifetime limit on program participation\n\\(\\omega\\) is the individual’s cumulative prior program use.\n\n\n\n\nIndividuals discount the future at an exponential rate of \\(1-\\beta\\). Per-period payoffs are given by:\n\\[ u_{j}(W) = \\log(\\gamma + Y_{j}(W,\\omega)) - \\alpha P_{j} - \\varphi H_{j} \\]\nThe per-period payoff for any individual \\(n\\) at time \\(t\\) is given by\n\\[ U_{ntj} = u_{j}(W_{nt}) + \\epsilon_{ntj} \\]\nwhere \\(\\epsilon_{nt}\\) is a preference shock that has a type 1 extreme value distribution with scale parameter \\(\\sigma\\). It is iid across individuals and time periods and us only observed by individuals when period \\(t\\) arrives.\n\n\n\nAll individuals begin in \\(t=1\\) with \\(\\omega_{1}=0\\) and \\(\\kappa_{1}=0\\) and live for \\(T\\) periods.\n\n\n\nAssume that log-wages are observed with measurement error:\n\\[ \\log(W^o) = \\log(W) + \\xi,\\qquad \\xi\\sim\\mathcal{N}(0,\\sigma^2_\\xi) \\]\n\n\n\nHere is code to solve the model for a default set of parameters.\n\nT = 10 #&lt;- length of life-cycle\nϕ = [2., 0.05]\nΩ = 4 #&lt;- time limit\nwagefunc(κ,ϕ) = exp(ϕ[1] + ϕ[2]*κ)\nnet_income(W,H,P,eligible) = W*H + P*eligible*(200 - 0.5*W*H) \nj_idx(H,P) = P*2 + H + 1\nutility(W,H,P,eligible,pars) = log(pars.γ + net_income(W,H,P,eligible)) - pars.α*P - pars.φ*H   \npars = (;α = 1., φ = 1., σ = 1., γ = 1., Ω, T,ϕ,β = 0.9)\n\nfunction iterate!(P,v,V,pars,ω_idx,κ_idx,t)\n    (;β,ϕ,Ω,σ) = pars\n    ω = ω_idx - 1\n    κ = κ_idx - 1\n    W = wagefunc(κ,ϕ)\n    eligible = ω&lt;Ω\n    for P in 0:1, H in 0:1\n        j = j_idx(H,P)\n        κ_next = min(κ_idx + H,T)\n        ω_next = min(ω_idx + P,Ω+1)\n        v[j]  = utility(W,H,P,eligible,pars) + β*V[ω_next,κ_next,t+1]\n    end\n    norm = sum(exp.(v ./ σ))\n    P[:,ω_idx,κ_idx,t] = exp.(v ./ σ) ./ norm\n    V[ω_idx,κ_idx,t] = σ * log(norm)\nend\n\nfunction solve!(P,v,V,pars)\n    for t in reverse(axes(P,4))\n        for κ_idx in axes(P,3), ω_idx in axes(P,2)\n            iterate!(P,v,V,pars,ω_idx,κ_idx,t)\n        end\n    end\nend\n\nV = zeros(Ω+1,T,T+1)\nP = zeros(4,Ω+1,T,T)\nv = zeros(4)\n\nsolve!(P,v,V,pars)"
  },
  {
    "objectID": "assignments/Assignment-5.html#part-1",
    "href": "assignments/Assignment-5.html#part-1",
    "title": "Assignment 5",
    "section": "Part 1",
    "text": "Part 1\nWrite a function that simulates a panel dataset consisting of choices and wages for \\(N\\) individuals in every period \\(t\\) of their life-cycle."
  },
  {
    "objectID": "assignments/Assignment-5.html#part-2",
    "href": "assignments/Assignment-5.html#part-2",
    "title": "Assignment 5",
    "section": "Part 2",
    "text": "Part 2\nFor a simulated sample of size \\(N=1000\\), write a function to estimate the array \\(P\\) of choice probabilities using a frequency estimator. Note that in period \\(t\\), only experience levels up to \\(t-1\\) can be observed so you do not need to estimate this array in its entirety."
  },
  {
    "objectID": "assignments/Assignment-5.html#part-3",
    "href": "assignments/Assignment-5.html#part-3",
    "title": "Assignment 5",
    "section": "Part 3",
    "text": "Part 3\nWrite an estimation routine for the structural parameters \\((\\phi,\\alpha,\\varphi,\\sigma,\\gamma,\\beta)\\) that utilizes the first stage estimates \\(\\hat{P}\\) from part 2. You can either work backwards from \\(T\\) or exploit shorter finite dependence horizons.\nOne small hint: you can estimate \\(\\phi\\) directly using OLS."
  },
  {
    "objectID": "assignments/Assignment-5.html#part-4",
    "href": "assignments/Assignment-5.html#part-4",
    "title": "Assignment 5",
    "section": "Part 4",
    "text": "Part 4\nIf you can, write a monte-carlo simulation in which you repeatedly draw a sample of size \\(N=1000\\) and produce an estimate. Use this sample to look at how the estimates are distributed around the true parameters and therefore check that your estimator is working properly."
  },
  {
    "objectID": "assignments/Assignment-6.html",
    "href": "assignments/Assignment-6.html",
    "title": "Assignment 6",
    "section": "",
    "text": "Let’s go back to the simple model of labor supply and program participation from Assignment 2.\nConsider the following extension to the model. Suppose that each individual \\(n\\) belongs to one of \\(K\\) finite types, \\(k(n)\\in\\{1,2,...,K\\}\\). Types determine differences in the cost of work and program participation as well as differences in wages:\n\\[ U_{ntj} = \\log(\\max\\{50,Y_{nt}(W_{nt}H_{j})\\}) + \\alpha_{l,k}\\log(112 - H_{j}) - \\alpha_{P,k,1}\\mathbf{1}\\{P_{j}&gt;0\\} - \\alpha_{P,k,2}\\mathbf{1}\\{P_{j}&gt;1\\} \\]\nand \\[ \\log(W_{nt}) = \\gamma_{k,0} + \\gamma_{k,1}\\text{Age}_{nt} + \\gamma_{k,2}\\text{Age}_{nt}^2 \\]\nSo the parameters \\(\\alpha_{l},\\alpha_{P},\\gamma\\) are now heterogeneous by type."
  },
  {
    "objectID": "assignments/Assignment-6.html#introduction-and-setup",
    "href": "assignments/Assignment-6.html#introduction-and-setup",
    "title": "Assignment 6",
    "section": "",
    "text": "Let’s go back to the simple model of labor supply and program participation from Assignment 2.\nConsider the following extension to the model. Suppose that each individual \\(n\\) belongs to one of \\(K\\) finite types, \\(k(n)\\in\\{1,2,...,K\\}\\). Types determine differences in the cost of work and program participation as well as differences in wages:\n\\[ U_{ntj} = \\log(\\max\\{50,Y_{nt}(W_{nt}H_{j})\\}) + \\alpha_{l,k}\\log(112 - H_{j}) - \\alpha_{P,k,1}\\mathbf{1}\\{P_{j}&gt;0\\} - \\alpha_{P,k,2}\\mathbf{1}\\{P_{j}&gt;1\\} \\]\nand \\[ \\log(W_{nt}) = \\gamma_{k,0} + \\gamma_{k,1}\\text{Age}_{nt} + \\gamma_{k,2}\\text{Age}_{nt}^2 \\]\nSo the parameters \\(\\alpha_{l},\\alpha_{P},\\gamma\\) are now heterogeneous by type."
  },
  {
    "objectID": "assignments/Assignment-6.html#part-1",
    "href": "assignments/Assignment-6.html#part-1",
    "title": "Assignment 6",
    "section": "Part 1",
    "text": "Part 1\nWrite a routine to classify individuals in the data into one of \\(K=3\\) types using K-means clustering. You may find the package Clustering.jl useful."
  },
  {
    "objectID": "assignments/Assignment-6.html#part-2",
    "href": "assignments/Assignment-6.html#part-2",
    "title": "Assignment 6",
    "section": "Part 2",
    "text": "Part 2\nCalculate and plot average work and average program participation over time for each of these types. Comment on what you are seeing."
  },
  {
    "objectID": "assignments/Assignment-6.html#part-3",
    "href": "assignments/Assignment-6.html#part-3",
    "title": "Assignment 6",
    "section": "Part 3",
    "text": "Part 3\nWrite code to estimate this extended model. You could just make a small extension to the maximum likelihood estimator you used in Assignment 2, or you could try another approach if you prefer."
  },
  {
    "objectID": "examples/LeastSquaresClustering.html",
    "href": "examples/LeastSquaresClustering.html",
    "title": "Clustering with Coefficients",
    "section": "",
    "text": "using Random, Distributions, Clustering, LinearAlgebra\nIn class we discussed how to classify individuals by type based on moments or averages. There are ready-made packages for clustering based on sample means, but we have to be careful to choose moments that satisfy the “injectivity” property that we need for consistent classification of types.\nConsider the following model:\n\\[ y_{nt} = \\beta_{0,k} + \\beta_{1,k}x_{nt} + \\epsilon_{nt} \\]\nwhere \\(x_{nt}\\) is an AR(1) process:\n\\[ x_{nt} = \\rho x_{nt-1} + \\xi_{nt}\\qquad \\xi_{nt}\\sim\\mathcal{N}(0,\\sigma^2_\\xi) \\]"
  },
  {
    "objectID": "examples/LeastSquaresClustering.html#an-issue-with-clustering-on-means",
    "href": "examples/LeastSquaresClustering.html#an-issue-with-clustering-on-means",
    "title": "Clustering with Coefficients",
    "section": "An Issue with Clustering on Means",
    "text": "An Issue with Clustering on Means\nNotice that if we clustered based only averages of \\(y\\), we could get a lot of misclassification because of persistent differences in \\(x\\).\n\nfunction gen_data(T,N,pars)\n    (;ρ,β,σξ,σϵ) = pars\n    K = size(β,2)\n    X = zeros(T,N)\n    Y = zeros(T,N)\n    πk = fill(1/K,K)\n    type = rand(Categorical(πk),N)\n    σ_stat = sqrt(σξ^2/(1-ρ^2))\n    for n in axes(X,2)\n        k = type[n]\n        x = rand(Normal(0,σ_stat))\n        for t in axes(X,1)\n            X[t,n] = x\n            Y[t,n] = β[1,k] + β[2,k]*x + rand(Normal(0,σϵ))\n            x = ρ*x + rand(Normal(0,σξ))\n        end\n    end\n    return (;type,X,Y)\nend\n\ngen_data (generic function with 1 method)\n\n\n\np = (;β = [0 0.5 ; 0.3 0.1],ρ = 0.95,σξ = 0.5,σϵ = 0.2)\nd = gen_data(10,1000,p)\n\n(type = [1, 2, 2, 2, 1, 1, 1, 2, 2, 1  …  1, 2, 1, 1, 1, 2, 1, 1, 2, 2], X = [-1.4550229799005174 -0.3235719894861611 … -0.3252102963926345 0.9337550853018559; -1.4453101711005691 0.3197381857283288 … 0.6710521361585193 1.5354948469380725; … ; -0.29949820449345 1.6564662817604354 … -0.6372124345389572 0.9171847039433293; -0.7311611139759487 0.9109754242885167 … -0.8731164815615942 -0.10884432590462445], Y = [-0.551382423663523 0.0881439847705161 … 0.21107863609973787 0.6870144018270388; -0.7040626859383348 0.4464915516381839 … 0.727528604926102 0.4989540522378084; … ; -0.16862695133653327 0.6267404954989101 … 0.2333328051748107 0.4005897820801897; -0.3266392158716257 0.6077923037209964 … 0.7125507711086414 0.6464979375694259])\n\n\nLet’s try clustering with k-means:\n\nres = kmeans(d.Y,2)\n\nKmeansResult{Matrix{Float64}, Float64, Int64}([-0.2678080833248467 0.4678242878166031; -0.2960095107964568 0.4742407792154177; … ; -0.3169829567953903 0.4577755549304992; -0.3096986820077558 0.47204389558301624], [1, 2, 2, 2, 2, 1, 1, 2, 2, 1  …  2, 2, 2, 1, 1, 2, 1, 2, 2, 2], [0.708035542811815, 0.4531583728266524, 0.7469764391671827, 0.5736154324762568, 0.7114268707590039, 0.164137140625503, 1.2610019632836815, 0.2606893217264279, 0.8490512038847164, 0.5408747521124615  …  1.2533746836273982, 0.4566586107970818, 1.1991676336482504, 1.8417661197646482, 1.0070316955214365, 0.17340415650391972, 4.31971298477642, 1.9929271300368452, 0.4382113308574356, 0.4667067001956937], [290, 710], [290, 710], 1060.5947343696905, 6, true)\n\n\nWe get a correct classification rate of:\n\nfunction correct_rate(assignments,type) #&lt;- to evaluate\n    K = maximum(type)\n    return [mean(assignments[type.==k].==mode(assignments[type.==k])) for k in 1:K]\nend\ncorrect_rate(res.assignments,d.type) \n\n2-element Vector{Float64}:\n 0.6029106029106029\n 1.0\n\n\nSo notice we are misclassifying many of the first type. A simple histogram shows us why:\n\nusing Plots\nhistogram(mean(d.Y[:,d.type.==1],dims=1)[:])\nhistogram!(mean(d.Y[:,d.type.==2],dims=1)[:])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoes this go away when we increase the panel dimension?\n\nd = gen_data(20,1000,p)\nres = kmeans(d.Y,2)\ncorrect_rate(res.assignments,d.type) \n\n2-element Vector{Float64}:\n 0.5826923076923077\n 0.99375\n\n\n\nhistogram(mean(d.Y[:,d.type.==1],dims=1)[:])\nhistogram!(mean(d.Y[:,d.type.==2],dims=1)[:])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA bit, yes, but things are still not looking great.\nNOTE I have to fix this! types are not identified up to permutation. need a different statistic for missclassification."
  },
  {
    "objectID": "examples/LeastSquaresClustering.html#an-alternative",
    "href": "examples/LeastSquaresClustering.html#an-alternative",
    "title": "Clustering with Coefficients",
    "section": "An Alternative",
    "text": "An Alternative\nAn alternative would be to use the structure of the model and cluster based on the least squares criterion itself. We could derive a classification \\(\\hat{\\mathcal{K}}\\) and coefficients \\(\\hat{\\beta}\\) to solve:\n\\[ \\hat{\\mathcal{K}},\\hat{\\beta} = \\arg\\min\\sum_{n}\\sum_{t}(Y_{nt} - \\beta_{0,k} - \\beta_{1,k}x_{nt})^2 \\]\nwhich we could solve by iterating in the same way as K-means. The \\(m\\)th iteration would be:\n\nEstimate \\(\\hat{\\beta}^{m+1}\\) by linear regression using \\(\\hat{\\mathcal{K}}^{m}\\).\nRe-assign types as \\(k^{m+1}(n) = \\arg\\min\\sum_t(Y_{nt} - \\hat{\\beta}^{m+1}_{0,k} - \\hat{\\beta}^{m+1}_{1,k}x_{nt})^2\\)\n\nHere’s code to do that:\n\nfunction assign!(type,data,β)\n    K = size(β,2)\n    lsq = zeros(K)\n    for n in axes(data.X,2)\n        x = view(data.X,:,n)\n        y = view(data.Y,:,n)\n        for k in axes(β,2)\n            @views lsq[k] = sum((y .- β[1,k] .- β[2,k]*x).^2)\n        end\n        type[n] = argmin(lsq)\n    end\nend\nfunction estimate!(type,data,β)\n    K = size(β,2)\n    T = size(data.X,1)\n    for k in axes(β,2)\n        Ik = type.==k\n        N = sum(Ik)\n        @views Y = data.Y[:,Ik][:]\n        @views X = [ones(N*T) data.X[:,Ik][:]]\n        β[:,k] = inv(X'*X)*X'*Y\n    end\nend\n\nfunction classify_least_squares!(type,β,data;maxiter = 100,e_tol = 1e-10)\n    err = Inf\n    iter = 1\n    while err&gt;e_tol && iter&lt;maxiter\n        β_old = copy(β)\n        estimate!(type,data,β)\n        assign!(type,data,β)\n        err = norm(β .- β_old,Inf)\n        iter += 1\n    end\n    if iter==maxiter\n        println(\"warning: maximum iterations reached, no convergence yet\")\n    end\nend\n\nclassify_least_squares! (generic function with 1 method)\n\n\n\nβ = zeros(2,2)\ntype = copy(res.assignments)\n# estimate!(type,d,β)\n# assign!(type,d,β)\nclassify_least_squares!(type,β,d)\ncorrect_rate(type,d.type)\n\n2-element Vector{Float64}:\n 1.0\n 0.9875\n\n\nNotice how much better we do now!"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Methods for Structural Microeconometrics",
    "section": "Contact",
    "text": "Contact\nFor appointments and questions, please email."
  },
  {
    "objectID": "assignments_2024/Assignment-1.html",
    "href": "assignments_2024/Assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Here is code to load the dataset and do a little cleaning / filtering.\nThe sample is all mothers in the PSID who are unmarried at the time of their first childbirth.\n\nusing CSV, DataFrames, DataFramesMeta\n\ndata = @chain begin\n    CSV.read(\"../children-cash-transfers/data/MainPanelFile.csv\",DataFrame,missingstring = \"NA\")\n    @select :MID :year :wage :hrs :earn :SOI :CPIU :WelfH :FSInd\n    @subset :year.&gt;=1985 :year.&lt;=2010\n    @transform :AFDC = :WelfH.&gt;0\n    @rename :FS = :FSInd\n    end\n\n89747×10 DataFrame89722 rows omitted\n\n\n\nRow\nMID\nyear\nwage\nhrs\nearn\nSOI\nCPIU\nWelfH\nFS\nAFDC\n\n\n\nInt64\nInt64\nFloat64?\nInt64?\nFloat64?\nInt64\nFloat64\nFloat64?\nInt64?\nBool?\n\n\n\n\n1\n4031\n1990\nmissing\nmissing\nmissing\n43\n0.758793\n0.0\n0\nfalse\n\n\n2\n4031\n1991\nmissing\nmissing\nmissing\n43\n0.790786\n0.0\n0\nfalse\n\n\n3\n4031\n1992\nmissing\nmissing\nmissing\n43\n0.814835\n0.0\n1\nfalse\n\n\n4\n4031\n1993\nmissing\nmissing\nmissing\n43\n0.839034\n0.0\n0\nfalse\n\n\n5\n4031\n1994\nmissing\n0\n0.0\n43\n0.860812\n1704.0\n1\ntrue\n\n\n6\n4031\n1995\nmissing\n0\n0.0\n43\n0.88496\n1704.0\n1\ntrue\n\n\n7\n4031\n1996\nmissing\n0\n0.0\n43\n0.910948\n1704.0\n1\ntrue\n\n\n8\n4031\n1997\nmissing\nmissing\nmissing\n43\n0.932244\nmissing\nmissing\nmissing\n\n\n9\n4031\n1998\nmissing\n0\n0.0\n43\n0.946664\n0.0\n1\nfalse\n\n\n10\n4031\n1999\nmissing\nmissing\nmissing\n43\n0.967426\nmissing\nmissing\nmissing\n\n\n11\n4031\n2000\n3.33333\n120\n400.0\n43\n1.0\n0.0\n0\nfalse\n\n\n12\n4031\n2001\nmissing\nmissing\nmissing\n43\n1.02817\nmissing\nmissing\nmissing\n\n\n13\n4031\n2002\nmissing\n0\n0.0\n43\n1.04457\n0.0\n0\nfalse\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n89736\n9308002\n1999\nmissing\nmissing\nmissing\n39\n0.967426\nmissing\nmissing\nmissing\n\n\n89737\n9308002\n2000\nmissing\nmissing\nmissing\n39\n1.0\nmissing\nmissing\nmissing\n\n\n89738\n9308002\n2001\nmissing\nmissing\nmissing\n39\n1.02817\nmissing\nmissing\nmissing\n\n\n89739\n9308002\n2002\nmissing\nmissing\nmissing\n39\n1.04457\nmissing\nmissing\nmissing\n\n\n89740\n9308002\n2003\nmissing\nmissing\nmissing\n39\n1.06857\nmissing\nmissing\nmissing\n\n\n89741\n9308002\n2004\nmissing\nmissing\nmissing\n39\n1.09708\nmissing\nmissing\nmissing\n\n\n89742\n9308002\n2005\nmissing\nmissing\nmissing\n39\n1.13401\nmissing\nmissing\nmissing\n\n\n89743\n9308002\n2006\nmissing\nmissing\nmissing\n39\n1.17054\nmissing\nmissing\nmissing\n\n\n89744\n9308002\n2007\nmissing\nmissing\nmissing\n39\n1.20414\nmissing\nmissing\nmissing\n\n\n89745\n9308002\n2008\nmissing\nmissing\nmissing\n39\n1.25008\nmissing\nmissing\nmissing\n\n\n89746\n9308002\n2009\nmissing\nmissing\nmissing\n39\n1.24608\nmissing\nmissing\nmissing\n\n\n89747\n9308002\n2010\nmissing\nmissing\nmissing\n39\n1.26647\nmissing\nmissing\nmissing\n\n\n\n\n\n\nYou may be unfamiliar with some of these commands, which make use of DataFrames and DataFramesMeta. In particular, think of the @chain macro as a way to compose functions. So for example:\n\nd1 = @chain d2 begin\n    func1(x)\n    func2(y)\n    func3(z)\nend\n\nis equivalent to calling:\n\nd1 = func3(func2(func1(d2,x),y),z)\n\nIf you want to understand better, google is your friend!"
  },
  {
    "objectID": "assignments_2024/Assignment-1.html#setup-loading-the-data",
    "href": "assignments_2024/Assignment-1.html#setup-loading-the-data",
    "title": "Assignment 1",
    "section": "",
    "text": "Here is code to load the dataset and do a little cleaning / filtering.\nThe sample is all mothers in the PSID who are unmarried at the time of their first childbirth.\n\nusing CSV, DataFrames, DataFramesMeta\n\ndata = @chain begin\n    CSV.read(\"../children-cash-transfers/data/MainPanelFile.csv\",DataFrame,missingstring = \"NA\")\n    @select :MID :year :wage :hrs :earn :SOI :CPIU :WelfH :FSInd\n    @subset :year.&gt;=1985 :year.&lt;=2010\n    @transform :AFDC = :WelfH.&gt;0\n    @rename :FS = :FSInd\n    end\n\n89747×10 DataFrame89722 rows omitted\n\n\n\nRow\nMID\nyear\nwage\nhrs\nearn\nSOI\nCPIU\nWelfH\nFS\nAFDC\n\n\n\nInt64\nInt64\nFloat64?\nInt64?\nFloat64?\nInt64\nFloat64\nFloat64?\nInt64?\nBool?\n\n\n\n\n1\n4031\n1990\nmissing\nmissing\nmissing\n43\n0.758793\n0.0\n0\nfalse\n\n\n2\n4031\n1991\nmissing\nmissing\nmissing\n43\n0.790786\n0.0\n0\nfalse\n\n\n3\n4031\n1992\nmissing\nmissing\nmissing\n43\n0.814835\n0.0\n1\nfalse\n\n\n4\n4031\n1993\nmissing\nmissing\nmissing\n43\n0.839034\n0.0\n0\nfalse\n\n\n5\n4031\n1994\nmissing\n0\n0.0\n43\n0.860812\n1704.0\n1\ntrue\n\n\n6\n4031\n1995\nmissing\n0\n0.0\n43\n0.88496\n1704.0\n1\ntrue\n\n\n7\n4031\n1996\nmissing\n0\n0.0\n43\n0.910948\n1704.0\n1\ntrue\n\n\n8\n4031\n1997\nmissing\nmissing\nmissing\n43\n0.932244\nmissing\nmissing\nmissing\n\n\n9\n4031\n1998\nmissing\n0\n0.0\n43\n0.946664\n0.0\n1\nfalse\n\n\n10\n4031\n1999\nmissing\nmissing\nmissing\n43\n0.967426\nmissing\nmissing\nmissing\n\n\n11\n4031\n2000\n3.33333\n120\n400.0\n43\n1.0\n0.0\n0\nfalse\n\n\n12\n4031\n2001\nmissing\nmissing\nmissing\n43\n1.02817\nmissing\nmissing\nmissing\n\n\n13\n4031\n2002\nmissing\n0\n0.0\n43\n1.04457\n0.0\n0\nfalse\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n89736\n9308002\n1999\nmissing\nmissing\nmissing\n39\n0.967426\nmissing\nmissing\nmissing\n\n\n89737\n9308002\n2000\nmissing\nmissing\nmissing\n39\n1.0\nmissing\nmissing\nmissing\n\n\n89738\n9308002\n2001\nmissing\nmissing\nmissing\n39\n1.02817\nmissing\nmissing\nmissing\n\n\n89739\n9308002\n2002\nmissing\nmissing\nmissing\n39\n1.04457\nmissing\nmissing\nmissing\n\n\n89740\n9308002\n2003\nmissing\nmissing\nmissing\n39\n1.06857\nmissing\nmissing\nmissing\n\n\n89741\n9308002\n2004\nmissing\nmissing\nmissing\n39\n1.09708\nmissing\nmissing\nmissing\n\n\n89742\n9308002\n2005\nmissing\nmissing\nmissing\n39\n1.13401\nmissing\nmissing\nmissing\n\n\n89743\n9308002\n2006\nmissing\nmissing\nmissing\n39\n1.17054\nmissing\nmissing\nmissing\n\n\n89744\n9308002\n2007\nmissing\nmissing\nmissing\n39\n1.20414\nmissing\nmissing\nmissing\n\n\n89745\n9308002\n2008\nmissing\nmissing\nmissing\n39\n1.25008\nmissing\nmissing\nmissing\n\n\n89746\n9308002\n2009\nmissing\nmissing\nmissing\n39\n1.24608\nmissing\nmissing\nmissing\n\n\n89747\n9308002\n2010\nmissing\nmissing\nmissing\n39\n1.26647\nmissing\nmissing\nmissing\n\n\n\n\n\n\nYou may be unfamiliar with some of these commands, which make use of DataFrames and DataFramesMeta. In particular, think of the @chain macro as a way to compose functions. So for example:\n\nd1 = @chain d2 begin\n    func1(x)\n    func2(y)\n    func3(z)\nend\n\nis equivalent to calling:\n\nd1 = func3(func2(func1(d2,x),y),z)\n\nIf you want to understand better, google is your friend!"
  },
  {
    "objectID": "assignments_2024/Assignment-1.html#question-1",
    "href": "assignments_2024/Assignment-1.html#question-1",
    "title": "Assignment 1",
    "section": "Question 1",
    "text": "Question 1\nCalculate average welfare participation (AFDC) by year and plot it. What do you think happened with welfare participation in 1996 and after? If you don’t know the historical context, a quick search online or a read of this paper should help you out.\nIf you are new to julia, here is average hours calculated and plotted to get you started.\n\nusing StatsPlots, Statistics\n\nd = @chain data begin\n    groupby(:year)\n    @combine :Hours = mean(skipmissing(:hrs))\n    @subset .!isnan.(:Hours)\nend\n\n@df d plot(:year,:Hours, legend = :none, linewidth = 2)\nxlabel!(\"Year\")\nylabel!(\"Average Welfare Participation\")"
  },
  {
    "objectID": "assignments_2024/Assignment-1.html#question-2",
    "href": "assignments_2024/Assignment-1.html#question-2",
    "title": "Assignment 1",
    "section": "Question 2",
    "text": "Question 2\nNow write code to\n\nDeflate earnings by CPI (CPIU).\nCalculate annual average earnings for each individual (identified by MID).\nDrop individuals with fewer than 10 years of data.\nCategorize individuals by whether their average earnings is below or above the median across individuals.\nPlot average participation in each year for individuals in each of these two categories.\n\nDo you think this pattern is likely to be generated by a model without persistent unobserved heterogeneity? No strictly correct answer here, just curious to read what you think.\nIn case it helps, here is code for the first three steps. You could edit this to add additional operations to the chain or work with d directly.\n\nd = @chain data begin\n    @transform :earn = :earn ./ :CPIU\n    groupby(:MID)\n    @combine :T = sum(.!ismissing.(:earn)) :earn = mean(skipmissing(:earn)) \n    @subset :T .&gt;= 10\nend\n\n1089×3 DataFrame1064 rows omitted\n\n\n\nRow\nMID\nT\nearn\n\n\n\nInt64\nInt64\nFloat64\n\n\n\n\n1\n4031\n10\n40.0\n\n\n2\n4179\n16\n6089.12\n\n\n3\n7030\n11\n4500.53\n\n\n4\n41007\n12\n16147.7\n\n\n5\n41008\n11\n0.0\n\n\n6\n45030\n11\n14374.3\n\n\n7\n45031\n11\n17693.5\n\n\n8\n47031\n11\n15946.7\n\n\n9\n84005\n18\n30769.6\n\n\n10\n105030\n14\n4633.61\n\n\n11\n106173\n13\n17714.4\n\n\n12\n122173\n13\n56275.2\n\n\n13\n126003\n19\n6705.08\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n1078\n6843006\n19\n1805.93\n\n\n1079\n6843173\n19\n5005.44\n\n\n1080\n6845005\n19\n19795.4\n\n\n1081\n6849005\n19\n2450.55\n\n\n1082\n6849188\n15\n23259.0\n\n\n1083\n6853003\n19\n4856.42\n\n\n1084\n6862005\n11\n18899.4\n\n\n1085\n6862008\n19\n26634.8\n\n\n1086\n6864002\n19\n8695.92\n\n\n1087\n6864003\n18\n13627.5\n\n\n1088\n6867013\n13\n389.097\n\n\n1089\n6872171\n17\n3294.07"
  },
  {
    "objectID": "assignments_2024/Assignment-1.html#question-3",
    "href": "assignments_2024/Assignment-1.html#question-3",
    "title": "Assignment 1",
    "section": "Question 3",
    "text": "Question 3\nThis question is to familiarize you with the module Tranfers.jl which will enable you to calculate post-tax and transfer income for individuals given their earnings, non-labor income, state, year, and family size. The function budget in this module takes the arguments:\n\nE: monthly earnings (either real or nominal)\nN: monthly non-labor income (real or nominal)\nSOI: the SOI code for state of residence\nyear: calendar year\nnum_kids: the number of children\ncpi: set to 1. if E and N are nominal\np: equal to 0 if no programs, 1 if food stamps, 2 if food stamps + welfare.\n\nFor example the function call:\n\nTransfers.budget(500.,0.,23,2000,2,1.,2)\n\ncalculates net income for a mother in Michigan (SOI code 23) with 2 kids, nominal labor income of $500 a month in the year 2000, and no non-labor income.\n\ninclude(\"../children-cash-transfers/src/Transfers.jl\")\n\nTransfers.budget(500.,0.,23,2000,2,1.,2)\n\n978.5408333333334\n\n\nCreate a graph that represents total net transfers for a single mother with two kids in the years 1990 and 2000 and in the states of Mississippi and New York. Depict these transfers as a function of earnings between the values of 0 and $1,000 a month (nominal). You can assume that all households are receiving both food stamps and welfare.\nWhat do you make of the differences in these transfers across states and over time?"
  },
  {
    "objectID": "assignments_2024/Assignment-6.html",
    "href": "assignments_2024/Assignment-6.html",
    "title": "Assignment 6",
    "section": "",
    "text": "Let’s go back to the simple model of labor supply and program participation from Assignment 2.\nConsider the following extension to the model. Suppose that each individual \\(n\\) belongs to one of \\(K\\) finite types, \\(k(n)\\in\\{1,2,...,K\\}\\). Types determine differences in the cost of work and program participation as well as differences in wages:\n\\[ U_{ntj} = \\log(\\max\\{50,Y_{nt}(W_{nt}H_{j})\\}) + \\alpha_{l,k}\\log(112 - H_{j}) - \\alpha_{P,k,1}\\mathbf{1}\\{P_{j}&gt;0\\} - \\alpha_{P,k,2}\\mathbf{1}\\{P_{j}&gt;1\\} \\]\nand \\[ \\log(W_{nt}) = \\gamma_{k,0} + \\gamma_{k,1}\\text{Age}_{nt} + \\gamma_{k,2}\\text{Age}_{nt}^2 \\]\nSo the parameters \\(\\alpha_{l},\\alpha_{P},\\gamma\\) are now heterogeneous by type."
  },
  {
    "objectID": "assignments_2024/Assignment-6.html#introduction-and-setup",
    "href": "assignments_2024/Assignment-6.html#introduction-and-setup",
    "title": "Assignment 6",
    "section": "",
    "text": "Let’s go back to the simple model of labor supply and program participation from Assignment 2.\nConsider the following extension to the model. Suppose that each individual \\(n\\) belongs to one of \\(K\\) finite types, \\(k(n)\\in\\{1,2,...,K\\}\\). Types determine differences in the cost of work and program participation as well as differences in wages:\n\\[ U_{ntj} = \\log(\\max\\{50,Y_{nt}(W_{nt}H_{j})\\}) + \\alpha_{l,k}\\log(112 - H_{j}) - \\alpha_{P,k,1}\\mathbf{1}\\{P_{j}&gt;0\\} - \\alpha_{P,k,2}\\mathbf{1}\\{P_{j}&gt;1\\} \\]\nand \\[ \\log(W_{nt}) = \\gamma_{k,0} + \\gamma_{k,1}\\text{Age}_{nt} + \\gamma_{k,2}\\text{Age}_{nt}^2 \\]\nSo the parameters \\(\\alpha_{l},\\alpha_{P},\\gamma\\) are now heterogeneous by type."
  },
  {
    "objectID": "assignments_2024/Assignment-6.html#part-1",
    "href": "assignments_2024/Assignment-6.html#part-1",
    "title": "Assignment 6",
    "section": "Part 1",
    "text": "Part 1\nWrite a routine to classify individuals in the data into one of \\(K=3\\) types using K-means clustering. You may find the package Clustering.jl useful."
  },
  {
    "objectID": "assignments_2024/Assignment-6.html#part-2",
    "href": "assignments_2024/Assignment-6.html#part-2",
    "title": "Assignment 6",
    "section": "Part 2",
    "text": "Part 2\nCalculate and plot average work and average program participation over time for each of these types. Comment on what you are seeing."
  },
  {
    "objectID": "assignments_2024/Assignment-6.html#part-3",
    "href": "assignments_2024/Assignment-6.html#part-3",
    "title": "Assignment 6",
    "section": "Part 3",
    "text": "Part 3\nWrite code to estimate this extended model. You could just make a small extension to the maximum likelihood estimator you used in Assignment 2, or you could try another approach if you prefer."
  },
  {
    "objectID": "assignments_2024/Assignment-5.html",
    "href": "assignments_2024/Assignment-5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "In this assignment you will implement a choice probability estimator for a toy model of dynamic labor supply.\n\n\nAgents in the model make one of four choices (\\(j \\in \\{0,1,2,3\\}\\)) that correspond to a binary work decision (\\(H_{j}\\)) and a binary program participation decision (\\(P_{j}\\)):\n\\[ \\{H_0,\\ H_1,\\ H_2,\\ H_3\\} = \\{0,1,0,1\\},\\qquad \\{P_0,\\ P_1,\\ P_2,\\ P_3\\} = \\{0,0,1,1\\} \\]\n\n\n\nPotential earnings are given by:\n\\[ \\log(W) = \\phi_0 + \\phi_1 \\kappa \\]\nwhere \\(\\kappa\\) is accumulated labor market experience that evolves according to:\n\\[ \\kappa_{t+1} = \\kappa_{t} + H_{jt} \\]\nWeekly net income is a function of this work decision and the participation decision:\n\\[ Y_{j}(W,\\omega) = H_{j}W + P_{j}\\mathbf{1}\\{\\omega&lt;\\Omega\\}(200 - 0.5\\times H_{j}W) \\]\nwhere:\n\n\\(W\\) is the individual’s potential wage\n\\(\\Omega\\) is the lifetime limit on program participation\n\\(\\omega\\) is the individual’s cumulative prior program use.\n\n\n\n\nIndividuals discount the future at an exponential rate of \\(1-\\beta\\). Per-period payoffs are given by:\n\\[ u_{j}(W) = \\log(\\gamma + Y_{j}(W,\\omega)) - \\alpha P_{j} - \\varphi H_{j} \\]\nThe per-period payoff for any individual \\(n\\) at time \\(t\\) is given by\n\\[ U_{ntj} = u_{j}(W_{nt}) + \\epsilon_{ntj} \\]\nwhere \\(\\epsilon_{nt}\\) is a preference shock that has a type 1 extreme value distribution with scale parameter \\(\\sigma\\). It is iid across individuals and time periods and us only observed by individuals when period \\(t\\) arrives.\n\n\n\nAll individuals begin in \\(t=1\\) with \\(\\omega_{1}=0\\) and \\(\\kappa_{1}=0\\) and live for \\(T\\) periods.\n\n\n\nAssume that log-wages are observed with measurement error:\n\\[ \\log(W^o) = \\log(W) + \\xi,\\qquad \\xi\\sim\\mathcal{N}(0,\\sigma^2_\\xi) \\]\n\n\n\nHere is code to solve the model for a default set of parameters.\n\nT = 10 #&lt;- length of life-cycle\nϕ = [2., 0.05]\nΩ = 4 #&lt;- time limit\nwagefunc(κ,ϕ) = exp(ϕ[1] + ϕ[2]*κ)\nnet_income(W,H,P,eligible) = W*H + P*eligible*(200 - 0.5*W*H) \nj_idx(H,P) = P*2 + H + 1\nutility(W,H,P,eligible,pars) = log(pars.γ + net_income(W,H,P,eligible)) - pars.α*P - pars.φ*H   \npars = (;α = 1., φ = 1., σ = 1., γ = 1., Ω, T,ϕ,β = 0.9)\n\nfunction iterate!(P,v,V,pars,ω_idx,κ_idx,t)\n    (;β,ϕ,Ω,σ) = pars\n    ω = ω_idx - 1\n    κ = κ_idx - 1\n    W = wagefunc(κ,ϕ)\n    eligible = ω&lt;Ω\n    for P in 0:1, H in 0:1\n        j = j_idx(H,P)\n        κ_next = min(κ_idx + H,T)\n        ω_next = min(ω_idx + P,Ω+1)\n        v[j]  = utility(W,H,P,eligible,pars) + β*V[ω_next,κ_next,t+1]\n    end\n    norm = sum(exp.(v ./ σ))\n    P[:,ω_idx,κ_idx,t] = exp.(v ./ σ) ./ norm\n    V[ω_idx,κ_idx,t] = σ * log(norm)\nend\n\nfunction solve!(P,v,V,pars)\n    for t in reverse(axes(P,4))\n        for κ_idx in axes(P,3), ω_idx in axes(P,2)\n            iterate!(P,v,V,pars,ω_idx,κ_idx,t)\n        end\n    end\nend\n\nV = zeros(Ω+1,T,T+1)\nP = zeros(4,Ω+1,T,T)\nv = zeros(4)\n\nsolve!(P,v,V,pars)"
  },
  {
    "objectID": "assignments_2024/Assignment-5.html#introduction-and-setup",
    "href": "assignments_2024/Assignment-5.html#introduction-and-setup",
    "title": "Assignment 5",
    "section": "",
    "text": "In this assignment you will implement a choice probability estimator for a toy model of dynamic labor supply.\n\n\nAgents in the model make one of four choices (\\(j \\in \\{0,1,2,3\\}\\)) that correspond to a binary work decision (\\(H_{j}\\)) and a binary program participation decision (\\(P_{j}\\)):\n\\[ \\{H_0,\\ H_1,\\ H_2,\\ H_3\\} = \\{0,1,0,1\\},\\qquad \\{P_0,\\ P_1,\\ P_2,\\ P_3\\} = \\{0,0,1,1\\} \\]\n\n\n\nPotential earnings are given by:\n\\[ \\log(W) = \\phi_0 + \\phi_1 \\kappa \\]\nwhere \\(\\kappa\\) is accumulated labor market experience that evolves according to:\n\\[ \\kappa_{t+1} = \\kappa_{t} + H_{jt} \\]\nWeekly net income is a function of this work decision and the participation decision:\n\\[ Y_{j}(W,\\omega) = H_{j}W + P_{j}\\mathbf{1}\\{\\omega&lt;\\Omega\\}(200 - 0.5\\times H_{j}W) \\]\nwhere:\n\n\\(W\\) is the individual’s potential wage\n\\(\\Omega\\) is the lifetime limit on program participation\n\\(\\omega\\) is the individual’s cumulative prior program use.\n\n\n\n\nIndividuals discount the future at an exponential rate of \\(1-\\beta\\). Per-period payoffs are given by:\n\\[ u_{j}(W) = \\log(\\gamma + Y_{j}(W,\\omega)) - \\alpha P_{j} - \\varphi H_{j} \\]\nThe per-period payoff for any individual \\(n\\) at time \\(t\\) is given by\n\\[ U_{ntj} = u_{j}(W_{nt}) + \\epsilon_{ntj} \\]\nwhere \\(\\epsilon_{nt}\\) is a preference shock that has a type 1 extreme value distribution with scale parameter \\(\\sigma\\). It is iid across individuals and time periods and us only observed by individuals when period \\(t\\) arrives.\n\n\n\nAll individuals begin in \\(t=1\\) with \\(\\omega_{1}=0\\) and \\(\\kappa_{1}=0\\) and live for \\(T\\) periods.\n\n\n\nAssume that log-wages are observed with measurement error:\n\\[ \\log(W^o) = \\log(W) + \\xi,\\qquad \\xi\\sim\\mathcal{N}(0,\\sigma^2_\\xi) \\]\n\n\n\nHere is code to solve the model for a default set of parameters.\n\nT = 10 #&lt;- length of life-cycle\nϕ = [2., 0.05]\nΩ = 4 #&lt;- time limit\nwagefunc(κ,ϕ) = exp(ϕ[1] + ϕ[2]*κ)\nnet_income(W,H,P,eligible) = W*H + P*eligible*(200 - 0.5*W*H) \nj_idx(H,P) = P*2 + H + 1\nutility(W,H,P,eligible,pars) = log(pars.γ + net_income(W,H,P,eligible)) - pars.α*P - pars.φ*H   \npars = (;α = 1., φ = 1., σ = 1., γ = 1., Ω, T,ϕ,β = 0.9)\n\nfunction iterate!(P,v,V,pars,ω_idx,κ_idx,t)\n    (;β,ϕ,Ω,σ) = pars\n    ω = ω_idx - 1\n    κ = κ_idx - 1\n    W = wagefunc(κ,ϕ)\n    eligible = ω&lt;Ω\n    for P in 0:1, H in 0:1\n        j = j_idx(H,P)\n        κ_next = min(κ_idx + H,T)\n        ω_next = min(ω_idx + P,Ω+1)\n        v[j]  = utility(W,H,P,eligible,pars) + β*V[ω_next,κ_next,t+1]\n    end\n    norm = sum(exp.(v ./ σ))\n    P[:,ω_idx,κ_idx,t] = exp.(v ./ σ) ./ norm\n    V[ω_idx,κ_idx,t] = σ * log(norm)\nend\n\nfunction solve!(P,v,V,pars)\n    for t in reverse(axes(P,4))\n        for κ_idx in axes(P,3), ω_idx in axes(P,2)\n            iterate!(P,v,V,pars,ω_idx,κ_idx,t)\n        end\n    end\nend\n\nV = zeros(Ω+1,T,T+1)\nP = zeros(4,Ω+1,T,T)\nv = zeros(4)\n\nsolve!(P,v,V,pars)"
  },
  {
    "objectID": "assignments_2024/Assignment-5.html#part-1",
    "href": "assignments_2024/Assignment-5.html#part-1",
    "title": "Assignment 5",
    "section": "Part 1",
    "text": "Part 1\nWrite a function that simulates a panel dataset consisting of choices and wages for \\(N\\) individuals in every period \\(t\\) of their life-cycle."
  },
  {
    "objectID": "assignments_2024/Assignment-5.html#part-2",
    "href": "assignments_2024/Assignment-5.html#part-2",
    "title": "Assignment 5",
    "section": "Part 2",
    "text": "Part 2\nFor a simulated sample of size \\(N=1000\\), write a function to estimate the array \\(P\\) of choice probabilities using a frequency estimator. Note that in period \\(t\\), only experience levels up to \\(t-1\\) can be observed so you do not need to estimate this array in its entirety."
  },
  {
    "objectID": "assignments_2024/Assignment-5.html#part-3",
    "href": "assignments_2024/Assignment-5.html#part-3",
    "title": "Assignment 5",
    "section": "Part 3",
    "text": "Part 3\nWrite an estimation routine for the structural parameters \\((\\phi,\\alpha,\\varphi,\\sigma,\\gamma,\\beta)\\) that utilizes the first stage estimates \\(\\hat{P}\\) from part 2. You can either work backwards from \\(T\\) or exploit shorter finite dependence horizons.\nOne small hint: you can estimate \\(\\phi\\) directly using OLS."
  },
  {
    "objectID": "assignments_2024/Assignment-5.html#part-4",
    "href": "assignments_2024/Assignment-5.html#part-4",
    "title": "Assignment 5",
    "section": "Part 4",
    "text": "Part 4\nIf you can, write a monte-carlo simulation in which you repeatedly draw a sample of size \\(N=1000\\) and produce an estimate. Use this sample to look at how the estimates are distributed around the true parameters and therefore check that your estimator is working properly."
  },
  {
    "objectID": "assignments_2024/Assignment-4.html",
    "href": "assignments_2024/Assignment-4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "This week you will estimate a simple hidden markov model using Expectation-Maximization.\nHere is the model. There is a discrete state variable \\(k\\in\\{1,2,3,..,K\\}\\) and a binary outcome \\(j\\in\\{0,1\\}\\) that:\nLet \\(p\\) be a \\(K\\)-dimensional vector where \\(p_{k}\\) holds the probability that \\(j=1\\) given that the model is in state \\(k\\).\nThe state \\(k\\) is never observed and the outcome \\(j\\) is only observed half the time (i.e. it is missing with probability 0.5). Thus, define \\(j^*\\) to be:\n\\[ j^* = \\left\\{ \\begin{array}{cl} j & \\text{with probability}\\ 0.5 \\\\ -1 & \\text{with probability}\\ 0.5 \\end{array} \\right. \\]\nYour task is to estimate the vector of outcome probabilities \\(p\\) non-parametrically using the EM algorithm with the Forward-Back routine to calculate the E-step.\nThe chunk of code below parameterizes the true vector \\(p\\) for this model and writes code to simulate panel data:\n\\[ (j^*_{nt})_{t=1,n=1}^{T,N} \\]\nTo start with, we’ll assume \\(k_{n,1} = 1\\) for all \\(n\\).\nusing Random, Distributions\nK = 5\nPj = 1 ./ (1 .+ exp.(LinRange(-1,1,K))) #&lt;- choice probability as a function of k\nknext(k,j,K) = min(K,k+j) \n\nfunction simulate(N,T,Pj)\n    J = zeros(T,N)\n    K = length(Pj)\n    for n in axes(J,2)\n        k = 1\n        for t in axes(J,1)\n            j = rand()&lt;Pj[k]\n            # record j probabilistically\n            if rand()&lt;0.5\n                J[t,n] = -1\n            else\n                J[t,n] = j\n            end\n            # update state:\n            k = knext(k,j,K)\n        end\n    end\n    return J\nend\n\nJ_data = simulate(1000,10,Pj)\n\n10×1000 Matrix{Float64}:\n -1.0  -1.0  -1.0  -1.0  -1.0  -1.0  …   1.0   1.0  -1.0   1.0  -1.0   0.0\n  0.0  -1.0  -1.0  -1.0   1.0   0.0      1.0  -1.0  -1.0   1.0  -1.0   1.0\n  1.0   1.0   0.0   1.0   0.0   1.0      1.0  -1.0   0.0  -1.0  -1.0  -1.0\n  1.0   1.0   1.0   0.0  -1.0  -1.0     -1.0  -1.0   0.0  -1.0  -1.0  -1.0\n -1.0   1.0  -1.0  -1.0  -1.0   1.0     -1.0   0.0   1.0   1.0   1.0   0.0\n -1.0   0.0  -1.0   0.0  -1.0  -1.0  …  -1.0  -1.0   0.0  -1.0   0.0  -1.0\n -1.0   0.0   0.0  -1.0   0.0   1.0      0.0  -1.0   0.0  -1.0   1.0  -1.0\n -1.0   1.0   0.0  -1.0   0.0   0.0     -1.0   1.0  -1.0   0.0  -1.0  -1.0\n -1.0  -1.0  -1.0   0.0   0.0   1.0      0.0   0.0  -1.0   1.0   0.0  -1.0\n  0.0   0.0  -1.0   0.0  -1.0   1.0     -1.0  -1.0   1.0   1.0  -1.0   1.0\nSince the outcomes \\(j\\) are periodically unobserved, there are two ways to set up the EM problem:\nWhen the number of discrete outcomes is larger, it becomes more difficult to integrate them out when missing. Since \\(j\\) is only binary here, this homework will step you through using the second approach. You can use the first approach if you prefer."
  },
  {
    "objectID": "assignments_2024/Assignment-4.html#part-1",
    "href": "assignments_2024/Assignment-4.html#part-1",
    "title": "Assignment 4",
    "section": "Part 1",
    "text": "Part 1\nWrite a function that (given a guess of the parameters \\(p\\)) takes a sequence of \\((j^*)^{T}_{t=1}\\) and runs the forward-back algorithm. In doing so your function should fill in three objects: (1) A \\(K\\times T\\) array of backward looking probabilities (\\(\\alpha\\)); (2) a \\(K\\times T\\) array of forward probabilities (\\(\\beta\\)); and (3) a \\(K\\times T\\) array of posterior probabilities over each state \\(k\\) (\\(Q\\)).\nRecall from class that (1) \\(\\alpha\\) is the joint probability of the state today with the sequence of outcomes up until today; and (2) \\(\\beta\\) is the conditional probability of future outcomes given the state today.\n\\[ \\alpha[k,s] = \\mathbb{P}[k_{s}=k,(j^*)^{s}_{t=1}] \\]\n\\[ \\beta[k,s] = \\mathbb{P}[(j^*)_{t=s+1}^{T} | k_{s}=k] \\]\nand\n\\[ Q[k,s] = \\mathbb{P}[k_{s}=k | (j^*)_{t=1}^{T} ] \\]"
  },
  {
    "objectID": "assignments_2024/Assignment-4.html#part-2",
    "href": "assignments_2024/Assignment-4.html#part-2",
    "title": "Assignment 4",
    "section": "Part 2",
    "text": "Part 2\nWrite a function that iterates over all observations and calculates posterior state probabilities for every observation. i.e. fill in a \\(K\\times T \\times N\\) array of posterior weights."
  },
  {
    "objectID": "assignments_2024/Assignment-4.html#part-3",
    "href": "assignments_2024/Assignment-4.html#part-3",
    "title": "Assignment 4",
    "section": "Part 3",
    "text": "Part 3\nTake as given a set of posterior weights \\(q_{ntk} = Q[n,t,k]\\). The expected log-likelihood for the M-step is:\n\\[ \\mathcal{L}(p) = \\sum_{n}\\sum_{t}\\sum_{k}q_{ntk}\\left(\\mathbf{1}\\{j^*_{nt}=1\\}\\log(p_{k}) + \\mathbf{1}\\{j^*_{nt}=0\\}\\log(1-p_{k})\\right) \\]\nShow that the non-parametric maximum likelihood estimate of \\(p\\) given the posterior weights is a frequency estimator:\n\\[ \\hat{p}_{k} = \\frac{\\sum_{n}\\sum_{t}\\mathbf{1}\\{j^*_{nt}=1\\} q_{ntk}}{\\sum_{n}\\sum_{t}\\mathbf{1}\\{j^*_{nt}\\neq -1\\} q_{ntk}} \\]\nWrite a function to calculate this frequency estimator given posterior weights."
  },
  {
    "objectID": "assignments_2024/Assignment-4.html#part-4",
    "href": "assignments_2024/Assignment-4.html#part-4",
    "title": "Assignment 4",
    "section": "Part 4",
    "text": "Part 4\nRun the E-M routine by iterating on this E-step and M-step until you get convergence in \\(\\hat{p}\\). Does it look like you can recover the true parameters?"
  },
  {
    "objectID": "assignments_2024/Assignment-4.html#extra-credit",
    "href": "assignments_2024/Assignment-4.html#extra-credit",
    "title": "Assignment 4",
    "section": "Extra credit",
    "text": "Extra credit\nIf you found this exercise too straightforward, try adjusting the model so that transitions depend probabilistically on the state \\(k\\) and the choice \\(j\\). This requires you to update also posterior transition probabilities and estimate those transition probabilities as part of the maximization step."
  },
  {
    "objectID": "assignments_2024/Assignment-4-solutions.html",
    "href": "assignments_2024/Assignment-4-solutions.html",
    "title": "Assignment 4",
    "section": "",
    "text": "This week you will estimate a simple hidden markov model using Expectation-Maximization.\nHere is the model. There is a discrete state variable \\(k\\in\\{1,2,3,..,K\\}\\) and a binary outcome \\(j\\in\\{0,1\\}\\) that:\nLet \\(p\\) be a \\(K\\)-dimensional vector where \\(p_{k}\\) holds the probability that \\(j=1\\) given that the model is in state \\(k\\).\nThe state \\(k\\) is never observed and the outcome \\(j\\) is only observed half the time (i.e. it is missing with probability 0.5). Thus, define \\(j^*\\) to be:\n\\[ j^* = \\left\\{ \\begin{array}{cl} j & \\text{with probability}\\ 0.5 \\\\ -1 & \\text{with probability}\\ 0.5 \\end{array} \\right. \\]\nYour task is to estimate the vector of outcome probabilities \\(p\\) non-parametrically using the EM algorithm with the Forward-Back routine to calculate the E-step.\nThe chunk of code below parameterizes the true vector \\(p\\) for this model and writes code to simulate panel data:\n\\[ (j^*_{nt})_{t=1,n=1}^{T,N} \\]\nTo start with, we’ll assume \\(k_{n,1} = 1\\) for all \\(n\\).\nusing Random, Distributions\nK = 5\nT = 10\nPj = 1 ./ (1 .+ exp.(LinRange(-1,1,K))) #&lt;- choice probability as a function of k\nknext(k,j,K) = min(K,k+j) \n\nfunction simulate(N,T,Pj)\n    J = zeros(Int64,T,N)\n    K = length(Pj)\n    for n in axes(J,2)\n        k = 1\n        for t in axes(J,1)\n            j = rand()&lt;Pj[k]\n            # record j probabilistically\n            if rand()&lt;0.5\n                J[t,n] = -1\n            else\n                J[t,n] = j\n            end\n            # update state:\n            k = knext(k,j,K)\n        end\n    end\n    return J\nend\n\nN = 1000\nJ_data = simulate(N,T,Pj)\n\n10×1000 Matrix{Int64}:\n -1  -1  -1   1   0   1  -1   0  -1  …   1  -1   1  -1  -1   1  -1   0  -1\n -1  -1  -1  -1   1   1  -1  -1  -1      1  -1   1   1   1   1   1  -1  -1\n  1  -1   0   1  -1  -1  -1  -1   1      0  -1   1  -1  -1   0   0  -1   0\n  0  -1   0   0  -1  -1  -1   0  -1      1   0  -1   1  -1   1   1   0  -1\n -1   0   1   1   0   0   0   1   1     -1  -1   0   0  -1  -1   1  -1  -1\n  0   1  -1   0  -1   1   1   1  -1  …   0  -1  -1   0   1  -1  -1  -1  -1\n -1   1   0   0  -1  -1   0  -1   0     -1   1  -1  -1  -1  -1   0  -1  -1\n -1   0   1   0  -1  -1  -1  -1  -1      0  -1   0  -1  -1   1  -1   0   0\n  0   1   0   1   0   0   1   0   1      1  -1  -1  -1   0  -1   0  -1  -1\n -1   1   0   1   0   0   0  -1   0     -1   1  -1   1  -1  -1  -1  -1  -1\nSince the outcomes \\(j\\) are periodically unobserved, there are two ways to set up the EM problem:\nWhen the number of discrete outcomes is larger, it becomes more difficult to integrate them out when missing. Since \\(j\\) is only binary here, this homework will step you through using the second approach. You can use the first approach if you prefer."
  },
  {
    "objectID": "assignments_2024/Assignment-4-solutions.html#part-1",
    "href": "assignments_2024/Assignment-4-solutions.html#part-1",
    "title": "Assignment 4",
    "section": "Part 1",
    "text": "Part 1\nWrite a function that (given a guess of the parameters \\(p\\)) takes a sequence of \\((j^*)^{T}_{t=1}\\) and runs the forward-back algorithm. In doing so your function should fill in three objects: (1) A \\(K\\times T\\) array of backward looking probabilities (\\(\\alpha\\)); (2) a \\(K\\times T\\) array of forward probabilities (\\(\\beta\\)); and (3) a \\(K\\times T\\) array of posterior probabilities over each state \\(k\\) (\\(Q\\)).\nRecall from class that (1) \\(\\alpha\\) is the joint probability of the state today with the sequence of outcomes up until today; and (2) \\(\\beta\\) is the conditional probability of future outcomes given the state today.\n\\[ \\alpha[k,s] = \\mathbb{P}[k_{s}=k,(j^*)^{s}_{t=1}] \\]\n\\[ \\beta[k,s] = \\mathbb{P}[(j^*)_{t=s+1}^{T} | k_{s}=k] \\]\nand\n\\[ Q[k,s] = \\mathbb{P}[k_{s}=k | (j^*)_{t=1}^{T} ] \\]\nThis is a problem where there is a lot of sparseness. For the size of this problem we won’t have to worry about exploiting that, but it’s something you want to think about for larger problems. I’ll introduce you to a sparse implementation in class and in code.\nSo, ignoring sparsenss, here is a function that gives transition probabilities given the three potential outcomes for \\(j^*\\).\n\nusing LinearAlgebra\n# this function assumes Π is a K x K x 3 array of transition probabilities where\nfunction get_transitions!(Π,p)\n    fill!(Π,0.)\n    # for j^*=-1 (i.e. missing)\n    for k in axes(Π,2)\n        Π[k,k,1] += p[k]\n        k_up = min(K,k+1)\n        Π[k_up,k,1] += 1-p[k]\n    end\n    # for j^* = 0\n    Π[:,:,2] = I(K) #&lt;- transitions are the identity matrix\n    # for j^*=1\n    for k in axes(Π,2)\n        k_up = min(K,k+1)\n        Π[k_up,k,3] = 1.\n    end\nend\n\nΠ = zeros(K,K,3)\nget_transitions!(Π,Pj)\n\nNow a function for the forward-back routine:\n\nfunction forward_back!(Q,α,β,j_obs,Π,p)\n    K,T = size(α)\n    fill!(β,0.)\n    @views fill!(β[:,T],1.)\n    fill!(α,0.)\n    α[1,1] = 1. #&lt;- since we know that all units begin in state 1\n    # run it forward\n    for t in 2:T\n        jlag_idx = 2 + j_obs[t-1]\n        for k in axes(α,1)\n            for klag in axes(α,1)\n                α[k,t] += Π[k,klag,jlag_idx] *  α[klag,t-1]\n            end\n            if j_obs[t]&gt;-1\n                α[k,t] *= j_obs[t]*p[k] + (1-j_obs[t])*(1-p[k])\n            end\n        end\n    end\n    # run it back\n    for t in reverse(1:T-1)\n        j_idx = 2 + j_obs[t]\n        for k in axes(β,1)\n            for knext in axes(β,1)\n                if j_obs[t+1]!=-1\n                    pj = j_obs[t+1]*p[knext] + (1-j_obs[t+1])*(1-p[knext])\n                else\n                    pj = 1.\n                end\n                β[k,t] += pj * Π[knext,k,j_idx]\n            end\n        end\n    end\n    # calculate the posteriors\n    for t in axes(Q,2)\n        @views Q[:,t] .= α[:,t] .* β[:,t]\n        @views Q[:,t] ./= sum(Q[:,t])\n    end\nend\n\nα = zeros(K,T)\nβ = zeros(K,T)\nQ = zeros(K,T)\nforward_back!(Q,α,β,J_data[:,1],Π,Pj)"
  },
  {
    "objectID": "assignments_2024/Assignment-4-solutions.html#part-2",
    "href": "assignments_2024/Assignment-4-solutions.html#part-2",
    "title": "Assignment 4",
    "section": "Part 2",
    "text": "Part 2\nWrite a function that iterates over all observations and calculates posterior state probabilities for every observation. i.e. fill in a \\(K\\times T \\times N\\) array of posterior weights.\nI’ll also add a line of code to update the transition matrices \\(\\Pi\\).\n\nfunction forward_back!(Q,α,β,J_obs::Array{Int64},Π,p)\n    get_transitions!(Π,p)\n    for n in axes(J_obs,2)\n        @views forward_back!(Q[:,:,n],α,β,J_obs[:,n],Π,p)\n    end\nend\n\nQ = zeros(K,T,N)\nforward_back!(Q,α,β,J_data,Π,Pj)"
  },
  {
    "objectID": "assignments_2024/Assignment-4-solutions.html#part-3",
    "href": "assignments_2024/Assignment-4-solutions.html#part-3",
    "title": "Assignment 4",
    "section": "Part 3",
    "text": "Part 3\nTake as given a set of posterior weights \\(q_{ntk} = Q[n,t,k]\\). The expected log-likelihood for the M-step is:\n\\[ \\mathcal{L}(p) = \\sum_{n}\\sum_{t}\\sum_{k}q_{ntk}\\left(\\mathbf{1}\\{j^*_{nt}=1\\}\\log(p_{k}) + \\mathbf{1}\\{j^*_{nt}=0\\}\\log(1-p_{k})\\right) \\]\nShow that the non-parametric maximum likelihood estimate of \\(p\\) given the posterior weights is a frequency estimator:\n\\[ \\hat{p}_{k} = \\frac{\\sum_{n}\\sum_{t}\\mathbf{1}\\{j^*_{nt}=1\\} q_{ntk}}{\\sum_{n}\\sum_{t}\\mathbf{1}\\{j^*_{nt}\\neq -1\\} q_{ntk}} \\]\nWrite a function to calculate this frequency estimator given posterior weights.\n\nfunction m_step(Q,J_data)\n    K = size(Q,1)\n    p = zeros(K)\n    denom = zeros(K)\n    for n in axes(J_data,2)\n        for t in axes(J_data,1)\n            if J_data[t,n]!=-1\n                @views denom .+= Q[:,t,n]\n                if J_data[t,n]==1\n                    @views p .+= Q[:,t,n]\n                end\n            end\n        end\n    end\n    return p ./ denom\nend\n\np_est = m_step(Q,J_data)\n\n5-element Vector{Float64}:\n 0.6979638286508735\n 0.5838619353864428\n 0.460315854103342\n 0.3523262347640548\n 0.26638099215665734"
  },
  {
    "objectID": "assignments_2024/Assignment-4-solutions.html#part-4",
    "href": "assignments_2024/Assignment-4-solutions.html#part-4",
    "title": "Assignment 4",
    "section": "Part 4",
    "text": "Part 4\nRun the E-M routine by iterating on this E-step and M-step until you get convergence in \\(\\hat{p}\\). Does it look like you can recover the true parameters?\n\nfunction EM_routine(p0,Π,data ; max_iter = 1000, tol = 1e-7, verbose = true)\n    (;α,β,Q,J_data) = data\n    err = Inf\n    iter = 0\n    while err&gt;tol && iter&lt;max_iter\n        forward_back!(Q,α,β,J_data,Π,p0)\n        p1 = m_step(Q,J_data)\n        err = maximum(abs.(p1 .- p0))\n        iter += 1\n        if mod(iter,10)==0 && verbose\n            println(\"Current error is $err\")\n        end\n        p0 = p1\n    end\n    return p0\nend\n\ndata = (;α,β,Q,J_data)\n\n# let's try using a \n\np_guess = fill(1 / K, K)\np_est = EM_routine(p_guess,Π,data)\n\n[p_est Pj]\n\nCurrent error is 0.0009160958113319517\nCurrent error is 0.00020857660656875554\nCurrent error is 5.858721547563617e-5\nCurrent error is 1.6799070472584e-5\nCurrent error is 4.82623428571527e-6\nCurrent error is 1.3868510386205735e-6\nCurrent error is 3.9853623212682265e-7\nCurrent error is 1.145274604597013e-7\n\n\n5×2 Matrix{Float64}:\n 0.699725  0.731059\n 0.590775  0.622459\n 0.44839   0.5\n 0.352756  0.377541\n 0.284563  0.268941\n\n\nLooks good! You could test the estimator a bit more by running it over a number of samples to see how you do.\n\nB = 100\nP_montecarlo = zeros(K,B)\nfor b in axes(P_montecarlo,2)\n    J_data = simulate(N,T,Pj)\n    data = (;data...,J_data)\n    P_montecarlo[:,b] = EM_routine(p_guess,Π,data ; verbose = false)\nend\n\n\nusing Plots\nhistogram(P_montecarlo[1,:],legend = false)\nplot!([Pj[1],Pj[1]],[0,30],linewidth=3,legend = false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see a little bit of bias, but then MLE is not guaranteed to be unbiased. You could re-run this with a larger sample size if you wanted to convince yourself of consistency."
  },
  {
    "objectID": "assignments_2024/Assignment-4-solutions.html#extra-credit",
    "href": "assignments_2024/Assignment-4-solutions.html#extra-credit",
    "title": "Assignment 4",
    "section": "Extra credit",
    "text": "Extra credit\nIf you found this exercise too straightforward, try adjusting the model so that transitions depend probabilistically on the state \\(k\\) and the choice \\(j\\). This requires you to update also posterior transition probabilities and estimate those transition probabilities as part of the maximization step."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Methods for Structural Microeconometrics",
    "section": "",
    "text": "Welcome to this course! Find details on the syllabus here, and check out the menu to find other materials.\nHere is a link to the git repo for this course, which by cloning is probably the simplest way for you to get access to the data, code, and assignments. I will continue to add materials throughout the course. You may prefer to selectively download these materials."
  },
  {
    "objectID": "syllabus.html#topics",
    "href": "syllabus.html#topics",
    "title": "Methods for Structural Microeconometrics",
    "section": "Topics",
    "text": "Topics\n\nIdentification, Credible Inference, and Marschak’s Maxim\nWe formally define identification and discuss (via examples) what people really mean when they talk about identification and credible inference. We use the Generalized Roy Model to compare identification via functional form to nonparametric identification.\nWe introduce Marschak’s Maxim as a guide for doing empirical model-based research.\n\nReading\nThe two survey articles by Keane (2010) (link) and Angrist and Pischke (2010) (link) - although aging - provide two important perspectives on the issues of credible inference in economics. Low and Meghir (2017) provide a nice review of the advantages of the structural approach.\nThe original paper by Marschak (1953) may be of interest. Heckman and Vytlacil (2007) provide a nice discussion of Marschak’s Maxim in the context of policy evaluation. They introduce (Heckman and Vytlacil 2005; Carneiro, Heckman, and Vytlacil 2011) the Marginal Treatment Effect as a tool for thinking about quasi-experimental estimators and policy evaluation.\n\n\n\nDynamic Discrete Choice\nWe introduce the dynamic discrete choice model and briefly discuss identification when all persistent state variables are observed. We review some of the basics of discrete choice such as the generalized extreme value distribution, which produces tractable choice probabilities with relatively flexible cross-price elasticities.\n\nReading\nRust (1987) is the canonical example demonstrating estimation of dynamic discrete choice models with maximum likelihood and a nested solution method.\nWe show that if one can directly estimate choice probabilities, several tractable approaches produce estimates of structural parameters without repeatedly solving the model. These include Hotz and Miller (1993), Aguirregabiria and Mira (2002), Aguirregabiria and Mira (2007), Pesendorfer and Schmidt-Dengler (2008), and Arcidiacono and Miller (2011). We will review these methods and why they are not appropriate to use in Mullins (2022).\n\n\n\nEstimation of Dynamic Models with Unobserved Heterogeneity\nUsing Mullins (2022) as an example, we talk about the inferential pitfalls that can occur when models fail to account for unobserved heterogeneity. We briefly discuss how this can depend on estimation approaches and sources of identification.\nWe review methods for estimation of dynamic models, including the Expectation-Maximization algorithm (EM) (see, e.g. Arcidiacono and Miller (2011)) and the clustering approach of Bonhomme and Manresa (2015). We review practical considerations for these approaches and introduce the Forward-Back algorithm for implementing EM in hidden Markov Models with time-varying unobserved state variables. We introduce a sparse matrix implementation of this algorithm.\n\n\nIdentification of Dynamic Models with Unobserved Heterogeneity\nWe discuss how either panel data or instrumental variables can facilitate identification of models with unobserved heterogeneity, and briefly review identification results for finite mixtures in panel data settings due to Kasahara and Shimotsu (2009) and Bonhomme, Jochmans, and Robin (2016). Berry and Compiani (2023) analyze identification and estimation of dynamic models with persistent heterogeneity using instrumental variables."
  },
  {
    "objectID": "syllabus.html#assessment",
    "href": "syllabus.html#assessment",
    "title": "Methods for Structural Microeconometrics",
    "section": "Assessment",
    "text": "Assessment\nThere will be 6 problem sets. Your best 4 of these 6 problem sets will be worth 25%. Hence, you can skip two if you want.\nHere is the proposed timeline of due dates. Submissions must be made through Canvas as a notebook (e.g. jupyter or quarto) formatted to html with printed output.\n\n\n\nAssignment\nDue Date\n\n\n\n\nAssignment 1\nNovember 7\n\n\nAssignment 2\nNovember 14\n\n\nAssignment 3\nNovember 21\n\n\nAssignment 4\nNovember 28\n\n\nAssignment 5\nDecember 5\n\n\nAssignment 6\nDecember 12"
  },
  {
    "objectID": "syllabus.html#contact",
    "href": "syllabus.html#contact",
    "title": "Methods for Structural Microeconometrics",
    "section": "Contact",
    "text": "Contact\nFor appointments and questions, please email."
  },
  {
    "objectID": "assignments_2024/Assignment-3.html",
    "href": "assignments_2024/Assignment-3.html",
    "title": "Assignment 3 - Solving the Dynamic Program",
    "section": "",
    "text": "Since you only have a week, I’m not going to make you code every piece of the model solution. The folder /children-cash-transfers/src contains:\n\nfunctions to calculate utility and net income from each choice\nan indexing rule that maps choices to hours and participation\nthe structure for the nested logit and code to calculate nested logit probabilities and inclusive values\nfunctions to calculate transition probabilities\ncode to define a default set of parameters and hyperparameters (the number of wage shocks, number of types, etc)\n\nIf you have cloned the course git repo, you can load all of this source code as follows:\n\ninclude(\"../children-cash-transfers/src/model.jl\")\n\nplain_logit (generic function with 1 method)\n\n\nIt will help to quickly explain some of this, but I recommend you read the code carefully since I won’t explain the functions in depth.\n\n\nRecall that in the model there are only three state variables to track: (1) the individual’s type (\\(k\\)); (2) the wage shock (\\(\\varepsilon\\)); and (3) cumulative welfare use (\\(\\omega\\)). Below we define a struct called model_data that contains all of the exogenous state variables that are taken as given for each individual in the data. The struct is defined in /children-cash-transfers/src/model.jl, but we repeat the definition here:\n\nstruct model_data\n    T::Int64 #&lt;- length of problem\n\n    y0::Int64 #&lt;- year to begin problem\n    age0::Int64 # &lt;- mother's age at start of problem\n    SOI::Vector{Int64} #&lt;- state SOI in each year\n    num_kids::Vector{Int64} #&lt;- number of kids in household that are between age 0 and 17\n    TotKids::Int64 #&lt;- indicares the total number of children that the mother will have over the available panel\n    age_kid::Matrix{Int64} #&lt; age_kid[f,t] is the age of child f at time t. Will be negative if child not born yet.\n    cpi::Vector{Float64} #&lt;- cpi\n\n    R::Vector{Int64} #&lt;- indicates if work requirement in time t\n    Kω::Int64 #&lt;- indicates length of time limit once introduced\n    TL::Vector{Bool} #&lt;- indicating that time limit is in place\nend\n\nEventually we will have one of these objects for every mother we observe in the data, and we’ll solve the resulting dynamic program for each of them. To test our functions below we can create a test version of this struct as well as some default parameters:\n\nmd = test_model()\np = pars(2,5) #&lt;- set Kτ = 2 and Kε = 5\n\n\n\n\nThe function nested_logit takes the value of each choice vj and fills log-choice probabilities into a pre-allocated vector logP. It also returns the inclusive value (the “emax” or continuation value). The input \\(B=(B_1,B_2,...,B_L)\\) specifies the partitions in each layer of the tree, while the input \\(C\\) reports the final choices that are ultimately contained in each node in each layer. By definition, at the highest layer, the partition takes the trivial form \\(B_L=\\{\\{1,2,...K_L\\}\\}\\) where \\(K_L\\) is the number of partitions in layer \\(L-1\\). Similarly at the lowest layer, \\(C_{1}\\) must take the form \\(C_{1} = \\{\\{1\\},\\{2\\},...,\\{J\\}\\}\\). For this model with three participation choices that lead into an extensive marginal labor supply choice, the structure is:\n\nB₁ = [[1,2],[3,4],[5,6]]\nC₁ = [[1,],[2,],[3,],[4,],[5,],[6,]]\n\nB₂ = [[1,2,3]]\nC₂ = [[1,2],[3,4],[5,6]]\n\nB = (B₁,B₂)\nC = (C₁,C₂)\n\nThis is defined in src/model/choices.jl. This is for your understanding, but these inputs are all given to you so you can use the function naively if that is all your time allows for.\n\n\n\nYou may find the following objects useful for iterating over the state variables. Let \\(k_{\\tau}\\in\\{1,...,K_{\\tau}\\}\\) index latent types, let \\(k_{\\varepsilon}\\in\\{1,...,K_{\\varepsilon}\\}\\) index wage shocks, and let \\(k_{\\omega} = \\omega+1\\) index cumulative time use. The total size of the state space is \\(K=K_\\tau\\times K_{\\varepsilon} \\times K_{\\omega}\\).\nOne way to do simple indexing is to just work with multi-dimensional arrays and build this into every function. However if you want to add state variables later on it will make it cumbersome to change. Another option is to use LinearIndices objects and their converse, CartesianIndices.\nHere’s a demonstration:\n\n# Hypothetical state space dimensions:\nKε = 5\nKτ = 5\nKω = 6\n\nk_idx = LinearIndices((Kτ,Kε,Kω))\nk_inv = CartesianIndices(k_idx)\n\n# To get the aggregate index k, call:\nk = k_idx[2,3,2]\n@show k\n# Then if we have k we can work back with:\nk_tuple = Tuple(k_inv[k])\n@show k_tuple\n\nk = 37\nk_tuple = (2, 3, 2)\n\n\n(2, 3, 2)\n\n\nSo when iterating, you could think about passing around state indices along with instances of these linear and cartesian indices that allow you to convert back and forth.\n\n\n\nIn the paper, wage shocks are parameterized with a single parameter \\(\\pi_{W}\\) that dictates the probability that an individual remains in the same place on the grid space, with symmetric probabilities of moving up or down. The function fε in states_transitions.jl takes the current wage shock kε and the total number of shocks Kε, along with πW and returns two tuples. The first tuple is the set of grid points that are possible and the second is the probability of being in each of those points.\nFor example:\n\nfε(3,5,0.9)\n\n((2, 3, 4), (0.04999999999999999, 0.9, 0.04999999999999999))\n\n\nSo the function is telling me that when I’m in state 3 I can move to states 3, 4, or 5 next period with probabilities (0.05,0.9,0.05). When we are at the bottom or the top of the grid space, the probabilities are slightly different:\n\nfε(1,5,0.9)\n\n((1, 2), (0.04999999999999999, 0.95))\n\n\nWe could have alternatively just written the transition probabilities into a matrix, but this approach essentially limits us to points with positive probabilities and will simplify iteration."
  },
  {
    "objectID": "assignments_2024/Assignment-3.html#source-code-for-the-model",
    "href": "assignments_2024/Assignment-3.html#source-code-for-the-model",
    "title": "Assignment 3 - Solving the Dynamic Program",
    "section": "",
    "text": "Since you only have a week, I’m not going to make you code every piece of the model solution. The folder /children-cash-transfers/src contains:\n\nfunctions to calculate utility and net income from each choice\nan indexing rule that maps choices to hours and participation\nthe structure for the nested logit and code to calculate nested logit probabilities and inclusive values\nfunctions to calculate transition probabilities\ncode to define a default set of parameters and hyperparameters (the number of wage shocks, number of types, etc)\n\nIf you have cloned the course git repo, you can load all of this source code as follows:\n\ninclude(\"../children-cash-transfers/src/model.jl\")\n\nplain_logit (generic function with 1 method)\n\n\nIt will help to quickly explain some of this, but I recommend you read the code carefully since I won’t explain the functions in depth.\n\n\nRecall that in the model there are only three state variables to track: (1) the individual’s type (\\(k\\)); (2) the wage shock (\\(\\varepsilon\\)); and (3) cumulative welfare use (\\(\\omega\\)). Below we define a struct called model_data that contains all of the exogenous state variables that are taken as given for each individual in the data. The struct is defined in /children-cash-transfers/src/model.jl, but we repeat the definition here:\n\nstruct model_data\n    T::Int64 #&lt;- length of problem\n\n    y0::Int64 #&lt;- year to begin problem\n    age0::Int64 # &lt;- mother's age at start of problem\n    SOI::Vector{Int64} #&lt;- state SOI in each year\n    num_kids::Vector{Int64} #&lt;- number of kids in household that are between age 0 and 17\n    TotKids::Int64 #&lt;- indicares the total number of children that the mother will have over the available panel\n    age_kid::Matrix{Int64} #&lt; age_kid[f,t] is the age of child f at time t. Will be negative if child not born yet.\n    cpi::Vector{Float64} #&lt;- cpi\n\n    R::Vector{Int64} #&lt;- indicates if work requirement in time t\n    Kω::Int64 #&lt;- indicates length of time limit once introduced\n    TL::Vector{Bool} #&lt;- indicating that time limit is in place\nend\n\nEventually we will have one of these objects for every mother we observe in the data, and we’ll solve the resulting dynamic program for each of them. To test our functions below we can create a test version of this struct as well as some default parameters:\n\nmd = test_model()\np = pars(2,5) #&lt;- set Kτ = 2 and Kε = 5\n\n\n\n\nThe function nested_logit takes the value of each choice vj and fills log-choice probabilities into a pre-allocated vector logP. It also returns the inclusive value (the “emax” or continuation value). The input \\(B=(B_1,B_2,...,B_L)\\) specifies the partitions in each layer of the tree, while the input \\(C\\) reports the final choices that are ultimately contained in each node in each layer. By definition, at the highest layer, the partition takes the trivial form \\(B_L=\\{\\{1,2,...K_L\\}\\}\\) where \\(K_L\\) is the number of partitions in layer \\(L-1\\). Similarly at the lowest layer, \\(C_{1}\\) must take the form \\(C_{1} = \\{\\{1\\},\\{2\\},...,\\{J\\}\\}\\). For this model with three participation choices that lead into an extensive marginal labor supply choice, the structure is:\n\nB₁ = [[1,2],[3,4],[5,6]]\nC₁ = [[1,],[2,],[3,],[4,],[5,],[6,]]\n\nB₂ = [[1,2,3]]\nC₂ = [[1,2],[3,4],[5,6]]\n\nB = (B₁,B₂)\nC = (C₁,C₂)\n\nThis is defined in src/model/choices.jl. This is for your understanding, but these inputs are all given to you so you can use the function naively if that is all your time allows for.\n\n\n\nYou may find the following objects useful for iterating over the state variables. Let \\(k_{\\tau}\\in\\{1,...,K_{\\tau}\\}\\) index latent types, let \\(k_{\\varepsilon}\\in\\{1,...,K_{\\varepsilon}\\}\\) index wage shocks, and let \\(k_{\\omega} = \\omega+1\\) index cumulative time use. The total size of the state space is \\(K=K_\\tau\\times K_{\\varepsilon} \\times K_{\\omega}\\).\nOne way to do simple indexing is to just work with multi-dimensional arrays and build this into every function. However if you want to add state variables later on it will make it cumbersome to change. Another option is to use LinearIndices objects and their converse, CartesianIndices.\nHere’s a demonstration:\n\n# Hypothetical state space dimensions:\nKε = 5\nKτ = 5\nKω = 6\n\nk_idx = LinearIndices((Kτ,Kε,Kω))\nk_inv = CartesianIndices(k_idx)\n\n# To get the aggregate index k, call:\nk = k_idx[2,3,2]\n@show k\n# Then if we have k we can work back with:\nk_tuple = Tuple(k_inv[k])\n@show k_tuple\n\nk = 37\nk_tuple = (2, 3, 2)\n\n\n(2, 3, 2)\n\n\nSo when iterating, you could think about passing around state indices along with instances of these linear and cartesian indices that allow you to convert back and forth.\n\n\n\nIn the paper, wage shocks are parameterized with a single parameter \\(\\pi_{W}\\) that dictates the probability that an individual remains in the same place on the grid space, with symmetric probabilities of moving up or down. The function fε in states_transitions.jl takes the current wage shock kε and the total number of shocks Kε, along with πW and returns two tuples. The first tuple is the set of grid points that are possible and the second is the probability of being in each of those points.\nFor example:\n\nfε(3,5,0.9)\n\n((2, 3, 4), (0.04999999999999999, 0.9, 0.04999999999999999))\n\n\nSo the function is telling me that when I’m in state 3 I can move to states 3, 4, or 5 next period with probabilities (0.05,0.9,0.05). When we are at the bottom or the top of the grid space, the probabilities are slightly different:\n\nfε(1,5,0.9)\n\n((1, 2), (0.04999999999999999, 0.95))\n\n\nWe could have alternatively just written the transition probabilities into a matrix, but this approach essentially limits us to points with positive probabilities and will simplify iteration."
  },
  {
    "objectID": "assignments_2024/Assignment-3.html#part-1",
    "href": "assignments_2024/Assignment-3.html#part-1",
    "title": "Assignment 3 - Solving the Dynamic Program",
    "section": "Part 1",
    "text": "Part 1\nWrite a function calc_vj that calculates the choice-specific value (i.e. the deterministic value of the choice) of a particular choice \\(j\\) in a particular time period \\(t\\) given the state and other exogenous variables. If you are confident you can code this however you like, but given the existing setup you might like to write the function in a way that it takes the following inputs:\n\nj: the discrete choice\nt: the time period in the model\nstate: a tuple that contains the state \\((k_\\tau,k_\\varepsilon,k_\\omega)\\) as well as a linear indexing rule\nV: a vector that contains the continuation value for each state at time \\(t+1\\)\npars: the parameters of the model\nmd: an instance of the model_data object that holds all relevant state variables\n\nVerify that your function works by testing it on the model_data instance created by test_model. Use the @time macro to look at evaluation time and memory allocations."
  },
  {
    "objectID": "assignments_2024/Assignment-3.html#part-2",
    "href": "assignments_2024/Assignment-3.html#part-2",
    "title": "Assignment 3 - Solving the Dynamic Program",
    "section": "Part 2",
    "text": "Part 2\nWrite a function called iterate! that iterates over all states at time period \\(t\\) and fills in choice probabilities and continuation values for period \\(t\\) in pre-allocated arrays. Again, you can do this however you like but here is a suggested set of inputs:\n\nt: the time period\nlogP: a \\(J \\times K \\times T\\) array of choice probabilities where the function will fill in logP[:,:,t]\nV: a \\(K \\times T\\) array of continuation values\nstate_idx: a named tuple that contains the size of the overall state space, a linear indexing rule that maps \\((k_\\tau,k_{\\varepsilon},k_{\\omega}\\)) to an overall state \\(k\\), and a Cartesian Indexing rule that inverts this mapping\nvj: a \\(J\\)-dimensional vector that, for each state, can be used as a container for the choice-specific values\npars: model parameters\nmd: model_data for the problem\n\nVerify that your function works by testing it on the model_data instance created by test_model. Use the @time macro to look at evaluation time and memory allocations."
  },
  {
    "objectID": "assignments_2024/Assignment-3.html#part-3",
    "href": "assignments_2024/Assignment-3.html#part-3",
    "title": "Assignment 3 - Solving the Dynamic Program",
    "section": "Part 3",
    "text": "Part 3\nWrite a function called solve! that performs backward induction to calculate continuation values and choice probabilities (storing them in pre-allocated arrays) in every period of the data across the whole state space. As before, some suggested inputs:\n\nlogP: a \\(J\\times K\\times T\\) array for choice probabilities\nV: a \\(K \\times T\\) array for continuation values\nvj: a container or buffer for choice-specific values in each iteration\npars: model parameters\nmd: an instance of model_data\n\nVerify that your function works by testing it on the model_data instance created by test_model. Use the @time macro to look at evaluation time and memory allocations."
  },
  {
    "objectID": "assignments_2024/Assignment-2-solutions.html",
    "href": "assignments_2024/Assignment-2-solutions.html",
    "title": "Assignment 2 - Solutions",
    "section": "",
    "text": "Read in the data as before.\nusing CSV, DataFrames, DataFramesMeta, Optim\ninclude(\"../children-cash-transfers/src/Transfers.jl\")\n\n# for this simple model we can just drop missing data. When we estimate the model with persistent latent heterogeneity, we will need complete panels (including years where choices are missing).\n\ndata = @chain begin\n    CSV.read(\"../children-cash-transfers/data/MainPanelFile.csv\",DataFrame,missingstring = \"NA\")\n    #@select :MID :year :wage :hrs :earn :SOI :CPIU :WelfH :FSInd :num_child :age\n    @subset :year.&gt;=1985 :year.&lt;=2010\n    @transform :AFDC = :WelfH.&gt;0 \n    @rename :FS = :FSInd\n    @transform :P  = :FS + :AFDC :H = min.(2,round.(Union{Int64, Missing},:hrs / (52*20)))\n    @subset .!ismissing.(:P) .&& .!ismissing.(:H)\n    @transform @byrow :wage = begin\n        if :hrs&gt;0 && :earn&gt;0\n            return :earn / :hrs / :CPIU\n        else\n            return missing\n        end\n    end\nend\n\ndata_mle = (;P = Int64.(data.P), H = Int64.(data.H), year = data.year, age = data.age,\n            soi = data.SOI, num_kids = data.num_child, cpi = data.CPIU,\n            logwage = log.(coalesce.(data.wage,1.)),wage_missing = ismissing.(data.wage))\n\n(P = [2, 2, 2, 1, 0, 0, 0, 2, 1, 0  …  2, 2, 1, 2, 0, 0, 1, 1, 1, 1], H = [0, 0, 0, 0, 0, 0, 0, 0, 0, 2  …  0, 0, 2, 2, 2, 2, 0, 0, 0, 0], year = [1994, 1995, 1996, 1998, 2000, 2002, 2004, 2006, 2008, 1990  …  1991, 1992, 1991, 1992, 1991, 1992, 1991, 1992, 1991, 1992], age = [21, 22, 23, 25, 27, 29, 31, 33, 35, 17  …  21, 22, 23, 24, 39, 40, 30, 31, 25, 26], soi = [43, 43, 43, 43, 43, 43, 43, 43, 43, 17  …  44, 44, 7, 7, 5, 5, 39, 39, 39, 39], num_kids = [2, 2, 3, 3, 3, 3, 3, 3, 3, 1  …  2, 2, 2, 2, 3, 3, 4, 4, 2, 2], cpi = [0.860812349005761, 0.884959812302546, 0.910948243820851, 0.946664188812488, 1.0, 1.04457233785542, 1.09707768072849, 1.17054218546739, 1.25008130459022, 0.758792510685746  …  0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336], logwage = [0.0, 0.0, 0.0, 0.0, 1.2039728043259361, 0.0, 0.0, 0.0, 0.0, 0.8230555833449215  …  0.0, 0.0, 2.6512574120409043, 2.3314412292534223, 1.265948758245773, 1.8744481199656744, 0.0, 0.0, 0.0, 0.0], wage_missing = Bool[1, 1, 1, 1, 0, 1, 1, 1, 1, 0  …  1, 1, 0, 0, 0, 0, 1, 1, 1, 1])\nUtility and indexing:\nj_idx(p,h) = p*3 + h + 1\nfunction utility(p,h,soi,cpi,year,num_kids,wage,pars)\n    (;αl,Hgrid,σ,αP) = pars\n    hrs = Hgrid[1+h]\n    earn = wage * hrs\n    net_income = max(50.,Transfers.budget(earn,0.,soi,year,num_kids,cpi,p))\n    return log(net_income) + αl*log(112-hrs) - αP[1]*(p&gt;0) - αP[2]*(p&gt;1)\nend\n\npars = (;αl = 1., σ = 1., σW = 1., γ = zeros(3),\n    Hgrid = [0,20.,40.],αP = zeros(2))\n\n(αl = 1.0, σ = 1.0, σW = 1.0, γ = [0.0, 0.0, 0.0], Hgrid = [0.0, 20.0, 40.0], αP = [0.0, 0.0])\nChoice probabilities are given by the logit formula.\nfunction choice_prob!(logP,it,data,pars)\n    (;σ,γ) = pars\n    wage = exp(γ[1] + γ[2] * data.age[it] + γ[3] * data.age[it]^2)\n    denom = 0.\n    umax = -Inf\n    for p in 0:2\n        for h in 0:2\n            j = j_idx(p,h)\n            u = utility(p,h,data.soi[it],data.cpi[it],data.year[it],data.num_kids[it],wage,pars)\n            logP[j] = u / σ\n            umax = u&gt;umax ? u : umax\n        end\n    end\n    logP[:] .-= umax / σ\n    denom = log(sum(exp.(logP)))\n    logP[:] .-= denom #&lt;- nornalize choice probabilities\nend\n\nchoice_prob! (generic function with 1 method)\nAnd so the log-likelihood:\nfunction log_likelihood(logP,it,data,pars)\n    (;γ,σW) = pars\n    ll = 0.\n    if !data.wage_missing[it]\n        ll += -0.5 * ((data.logwage[it] - γ[1] - γ[2]*data.age[it] - γ[3]*data.age[it]^2) / σW)^2 - log(σW)\n    end\n    choice_prob!(logP,it,data,pars)\n    j = j_idx(data.P[it],data.H[it])\n    ll += logP[j]\n    return ll\nend\n\nfunction update(x,p)\n    αl = exp(x[1])\n    σ = exp(x[2])\n    γ = x[3:5]\n    σW = exp(x[6])\n    αP = x[7:8]\n    return (;p...,αl,σ,σW,αP,γ)\nend\n\nfunction log_likelihood(x,data,pars)\n    pars = update(x,pars)\n    logP = zeros(eltype(x),9)\n    ll = 0.\n    for it in eachindex(data.P)\n        ll += log_likelihood(logP,it,data,pars)\n    end\n    return ll\nend\n\nlog_likelihood (generic function with 2 methods)\nWe can use Optim to maximize the log-likelihood:\nx0 = zeros(8)\nx0 = [0.,0.,log(10.),0.,0.,0.,0.,0.]\nres = optimize(x-&gt;-log_likelihood(x,data_mle,pars),x0,BFGS(),autodiff=:forward,Optim.Options(show_trace = true))\npars = update(res.minimizer,pars)\n\nIter     Function value   Gradient norm \n     0     7.234596e+04     3.085114e+06\n * time: 0.010987043380737305\n     1     7.219007e+04     7.930873e+05\n * time: 0.4965250492095947\n     2     7.189756e+04     8.240034e+05\n * time: 0.5908100605010986\n     3     6.129802e+04     3.337752e+06\n * time: 0.8107671737670898\n     4     5.904150e+04     1.232987e+06\n * time: 1.0711750984191895\n     5     5.781471e+04     2.641891e+05\n * time: 1.3058781623840332\n     6     5.696965e+04     1.198471e+06\n * time: 1.5016920566558838\n     7     5.683014e+04     1.164517e+06\n * time: 1.6777441501617432\n     8     5.609242e+04     1.940285e+06\n * time: 1.725534200668335\n     9     5.480683e+04     3.811453e+06\n * time: 1.7589399814605713\n    10     5.274754e+04     2.634463e+06\n * time: 1.7925050258636475\n    11     5.214471e+04     7.076234e+05\n * time: 1.825437068939209\n    12     5.191466e+04     9.933081e+04\n * time: 1.8591229915618896\n    13     5.187770e+04     1.158376e+05\n * time: 1.8927600383758545\n    14     5.187629e+04     1.626802e+05\n * time: 1.9251301288604736\n    15     5.187431e+04     4.376935e+04\n * time: 1.9923491477966309\n    16     5.186935e+04     8.792504e+04\n * time: 2.042283058166504\n    17     5.186852e+04     7.387908e+02\n * time: 2.0758960247039795\n    18     5.186850e+04     2.434391e+03\n * time: 2.1094892024993896\n    19     5.186850e+04     9.509518e+01\n * time: 2.142749071121216\n    20     5.186850e+04     1.240425e+01\n * time: 2.1761670112609863\n    21     5.186850e+04     2.647360e-02\n * time: 2.2092361450195312\n    22     5.186850e+04     6.262737e-03\n * time: 2.242264986038208\n    23     5.186850e+04     6.902728e-05\n * time: 2.2754881381988525\n    24     5.186850e+04     5.195348e-05\n * time: 2.3086800575256348\n    25     5.186850e+04     8.976889e-08\n * time: 2.3425211906433105\n    26     5.186850e+04     1.102359e-08\n * time: 2.3924009799957275\n    27     5.186850e+04     3.531738e-09\n * time: 2.4258220195770264\n\n\n(αl = 2.8546233783929478, σ = 0.9509286759263833, σW = 0.7224860062936597, γ = [-0.023620750582814317, 0.10877398375254962, -0.0012722347394312208], Hgrid = [0.0, 20.0, 40.0], αP = [2.0353016361355523, 1.0137619542631455])"
  },
  {
    "objectID": "assignments_2024/Assignment-2-solutions.html#question-1",
    "href": "assignments_2024/Assignment-2-solutions.html#question-1",
    "title": "Assignment 2 - Solutions",
    "section": "Question 1",
    "text": "Question 1\nLet’s use the Hessian of the average log-likelihood to estimate the standard errors.\n\nusing ForwardDiff\nN = length(data_mle.age)\nH = ForwardDiff.hessian(x-&gt;log_likelihood(x,data_mle,pars),res.minimizer) / N\nV = - H / N\n\n8×8 Matrix{Float64}:\n  1.04029e-5    4.11235e-6   -6.42072e-6  …  -2.46919e-6  -1.15412e-6\n  4.11235e-6    1.95866e-5   -1.25177e-5     -1.15811e-5  -5.76085e-6\n -6.42072e-6   -1.25177e-5    6.01038e-5      5.73597e-6   2.50154e-6\n -0.000199471  -0.000389124   0.00193279      0.00017803   7.68593e-5\n -0.00665377   -0.0130835     0.0662433       0.00587496   0.00250189\n -0.0          -0.0           5.82263e-6  …  -0.0         -0.0\n -2.46919e-6   -1.15811e-5    5.73597e-6      9.58444e-6   3.41442e-6\n -1.15412e-6   -5.76085e-6    2.50154e-6      3.41442e-6   4.67709e-6\n\n\nNow we can make the table, but we have to be careful because some of our parameters are transformations of the vector res.minimizer (so we use the delta method to get the standard error of the transformed value)\n\nusing LinearAlgebra\np_str = [\"αl\",\"σ\",\"γ₁\",\"γ₂\",\"γ₃\",\"σW\",\"αP₁\",\"αP₂\"]\np_est = [pars.αl ; pars.σ ; pars.γ ; pars.σW ; pars.αP]\nse = sqrt.(diag(V))\np_se = [se[1:2].*p_est[1:2] ; se[3:5] ; se[6].*p_est[6] ; se[7:8]]\nd = DataFrame(par = p_str, est = p_est, se = p_se)\n\n8×3 DataFrame\n\n\n\nRow\npar\nest\nse\n\n\n\nString\nFloat64\nFloat64\n\n\n\n\n1\nαl\n2.85462\n0.00920715\n\n\n2\nσ\n0.950929\n0.00420851\n\n\n3\nγ₁\n-0.0236208\n0.00775266\n\n\n4\nγ₂\n0.108774\n0.257378\n\n\n5\nγ₃\n-0.00127223\n9.6305\n\n\n6\nσW\n0.722486\n0.00529469\n\n\n7\nαP₁\n2.0353\n0.00309587\n\n\n8\nαP₂\n1.01376\n0.00216266"
  },
  {
    "objectID": "assignments_2024/Assignment-2-solutions.html#question-2",
    "href": "assignments_2024/Assignment-2-solutions.html#question-2",
    "title": "Assignment 2 - Solutions",
    "section": "Question 2",
    "text": "Question 2\nWhat sources of variation are identifying \\(\\sigma\\), the responsiveness of choices to changes in the payoffs? How is the responsivess of program participation and labor supply to changes in incentives limited in this framework?\nSome comments we went over in class:\n\n\\(\\sigma\\) is identified to by covariation in the probability of making particular decisions with changes in the financial returns to those decisions, these in turn are driven by\nThe responsiveness of labor supply and program participation are therefore both dictated by \\(\\sigma\\)"
  },
  {
    "objectID": "assignments_2024/Assignment-2-solutions.html#question-3",
    "href": "assignments_2024/Assignment-2-solutions.html#question-3",
    "title": "Assignment 2 - Solutions",
    "section": "Question 3",
    "text": "Question 3\nFirst we’ll write the function to calculate nested logit probabilities given a vector of utilities u and a partition B.\nFrom next week I’ll introduce a function for evaluating the nested logit for a general number of layers. This function will work for any nested logit with only two layers.\n\nfunction nested_logit(logP,u,σ,B)\n    # calculate nest probabilities and inclusive values\n    for k in eachindex(B)\n        Bₖ = B[k]\n        # find the max\n        vmax = -Inf\n        for j in Bₖ\n            vmax = u[j] &gt; vmax ? u[j] : vmax\n        end\n        # calculate choice probs\n        iv = 0.\n        for j in Bₖ\n            logP[j] += (u[j] - vmax) / σ[2]\n            iv += exp((u[j] - vmax) / σ[2])\n        end\n        logP[Bₖ] .-= log(iv)\n        u[k] = vmax + σ[2] * log(iv) #&lt;- fill in inclusive value\n    end\n    # now calculate the partition probabilities\n    vmax = -Inf\n    for k in eachindex(B)\n        vmax = u[k] &gt; vmax ? u[k] : vmax\n    end\n    iv = 0.\n    for k in eachindex(B)\n        iv += exp((u[k]-vmax) / σ[1])\n        for j in B[k]\n            logP[j] += (u[k]-vmax) / σ[1]\n        end\n    end\n    logP[:] .-= log(iv)\n    V = vmax + σ[1] * log(iv)\nend\n\nnested_logit (generic function with 1 method)\n\n\nNow let’s write a new set of log-likelihood routines.\n\nfunction fill_utilities!(u,it,data,pars)\n    (;γ) = pars\n    wage = exp(γ[1] + γ[2]*data.age[it] + γ[3]*data.age[it]^2)\n    for p in 0:2\n        for h in 0:2\n            j = j_idx(p,h)\n            u[j] = utility(p,h,data.soi[1],data.cpi[1],data.year[1],data.num_kids[1],wage,pars)\n        end\n    end\nend\nfunction log_likelihood(logP,u,it,data,pars)\n    (;γ,σW,σ,B) = pars\n    ll = 0.\n    if !data.wage_missing[it]\n        ll += -0.5 * ((data.logwage[it] - γ[1] - γ[2]*data.age[it] - γ[3]*data.age[it]^2) / σW)^2 - log(σW)\n    end\n    fill_utilities!(u,it,data,pars)\n    V = nested_logit(logP,u,σ,B)\n    j = j_idx(data.P[it],data.H[it])\n    ll += logP[j]\n    return ll\nend\n\nfunction update(x,p)\n    αl = exp(x[1])\n    σ = exp.(x[2:3])\n    γ = x[4:6]\n    σW = exp(x[7])\n    αP = x[8:9]\n    return (;p...,αl,σ,σW,αP,γ)\nend\n\nfunction log_likelihood(x,data,pars)\n    pars = update(x,pars)\n    logP = zeros(eltype(x),9)\n    u = zeros(eltype(x),9)\n    ll = 0.\n    for it in eachindex(data.P)\n        fill!(logP,0.)\n        ll += log_likelihood(logP,u,it,data,pars)\n    end\n    return ll\nend\n\nlog_likelihood (generic function with 3 methods)\n\n\nAnd here is code to maximize the log-likelihood. Notice that we get very different participation elasticities vs labor force elasticities.\n\nB = [[1,2,3],[4,5,6],[7,8,9]]\npars = (;αl = 1., σ = [1.,1.], σW = 1., γ = zeros(3),\n    Hgrid = [0,20.,40.],αP = zeros(2), B)\nx0 = zeros(9)\nx0 = [0.,0.,log(10.),0.,0.,0.,0.,0.,0.]\nres = optimize(x-&gt;-log_likelihood(x,data_mle,pars),x0,BFGS(),autodiff=:forward,Optim.Options(show_trace = true))\npars = update(res.minimizer,pars)\n\nIter     Function value   Gradient norm \n     0     1.298462e+05     4.851678e+07\n * time: 5.1021575927734375e-5\n     1     8.428536e+04     6.296835e+05\n * time: 0.2127220630645752\n     2     7.391455e+04     5.520307e+05\n * time: 0.3516049385070801\n     3     6.462201e+04     3.985311e+06\n * time: 0.505781888961792\n     4     6.437665e+04     7.865056e+05\n * time: 0.5886578559875488\n     5     5.668077e+04     3.694472e+06\n * time: 1.0036170482635498\n     6     5.664736e+04     3.826317e+06\n * time: 1.1597239971160889\n     7     5.658193e+04     3.982393e+06\n * time: 1.5171430110931396\n     8     5.650161e+04     4.463297e+06\n * time: 1.7832438945770264\n     9     5.648749e+04     4.435425e+06\n * time: 2.027708053588867\n    10     5.642168e+04     4.553960e+06\n * time: 2.1180179119110107\n    11     5.509169e+04     5.191110e+06\n * time: 2.2074170112609863\n    12     5.380285e+04     5.361058e+06\n * time: 2.2523438930511475\n    13     5.307073e+04     3.085183e+06\n * time: 2.3642518520355225\n    14     5.266969e+04     6.884474e+05\n * time: 2.4090540409088135\n    15     5.252350e+04     1.081648e+06\n * time: 2.453713893890381\n    16     5.249852e+04     3.018613e+05\n * time: 2.498447895050049\n    17     5.248219e+04     1.782635e+04\n * time: 2.5438220500946045\n    18     5.248035e+04     6.193711e+04\n * time: 2.5882718563079834\n    19     5.248012e+04     3.976973e+04\n * time: 2.6327219009399414\n    20     5.247861e+04     1.108337e+04\n * time: 2.721729040145874\n    21     5.246307e+04     3.026674e+05\n * time: 2.8149349689483643\n    22     5.245834e+04     3.655463e+05\n * time: 2.9038748741149902\n    23     5.244938e+04     3.839334e+05\n * time: 2.9484879970550537\n    24     5.243914e+04     1.006549e+05\n * time: 3.0147929191589355\n    25     5.243416e+04     1.610297e+05\n * time: 3.0811870098114014\n    26     5.243367e+04     7.579392e+04\n * time: 3.126222848892212\n    27     5.243341e+04     4.701303e+04\n * time: 3.1709179878234863\n    28     5.243331e+04     9.355548e+03\n * time: 3.2153658866882324\n    29     5.243331e+04     5.899227e+03\n * time: 3.281589984893799\n    30     5.243331e+04     5.802839e+00\n * time: 3.325974941253662\n    31     5.243331e+04     7.363770e+00\n * time: 3.3699939250946045\n    32     5.243331e+04     2.012395e-02\n * time: 3.413810968399048\n    33     5.243331e+04     1.205871e-01\n * time: 3.4587340354919434\n    34     5.243331e+04     1.520902e-02\n * time: 3.5028269290924072\n    35     5.243331e+04     3.321645e-04\n * time: 3.5468578338623047\n    36     5.243331e+04     7.691392e-06\n * time: 3.5909719467163086\n    37     5.243331e+04     1.869461e-07\n * time: 3.635145902633667\n    38     5.243331e+04     5.864422e-09\n * time: 3.680130958557129\n\n\n(αl = 2.829431829812853, σ = [0.37004828541782886, 1.3098778384091292], σW = 0.7186963786509329, γ = [0.6729307189124956, 0.06812663769540622, -0.0006981805671945225], Hgrid = [0.0, 20.0, 40.0], αP = [1.517080456006113, 0.5324967821971681], B = [[1, 2, 3], [4, 5, 6], [7, 8, 9]])"
  },
  {
    "objectID": "assignments_2024/Assignment-2-solutions.html#question-4",
    "href": "assignments_2024/Assignment-2-solutions.html#question-4",
    "title": "Assignment 2 - Solutions",
    "section": "Question 4",
    "text": "Question 4\nWe discussed this in class but let me know if you would like this to be discussed further."
  },
  {
    "objectID": "assignments_2024/Assignment-7.html",
    "href": "assignments_2024/Assignment-7.html",
    "title": "Assignment 7",
    "section": "",
    "text": "Some ideas for this assignment:\n\nextend previous week to case with time limits\nwalk them through a clustering routine using the wage equation only ** can do an example instread of a homework **\nhave them make an identification argument for their chosen moments from last week?\nhave them estimate types using matrix diagonalization\nhave them evaluate model fit and comment on what they are missing\nhave them run a counterfactual with pre: welfare reform.\ncould make them think about what happens when we add shocks to the wage equation? estimate how?"
  },
  {
    "objectID": "assignments_2024/Assignment-2.html",
    "href": "assignments_2024/Assignment-2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "We are building toward estimating a dynamic discrete choice model with persistent unobserved heterogeneity. To make a start, in this problem set you will estimate a static discrete choice problem on the data."
  },
  {
    "objectID": "assignments_2024/Assignment-2.html#setup-code-for-mle",
    "href": "assignments_2024/Assignment-2.html#setup-code-for-mle",
    "title": "Assignment 2",
    "section": "Setup: Code for MLE",
    "text": "Setup: Code for MLE\n\nReading in Data\nYou are going to build on the code below that estimates a very simple model of labor supply and program participation.\nFirst, here is a chunk of code to read in the data and prepare it. The variables in julia DataFrames are typically not type stable which can slow down your code considerably. So you will note that in the final step here I bring the variables I want to use out of a DataFrame and into a named tuple with vectors containing a fixed type.\n\nusing CSV, DataFrames, DataFramesMeta, Optim\ninclude(\"../children-cash-transfers/src/Transfers.jl\")\n\n# for this simple model we can just drop missing data. When we estimate the model with persistent latent heterogeneity, we will need complete panels (including years where choices are missing).\n\ndata = @chain begin\n    CSV.read(\"../children-cash-transfers/data/MainPanelFile.csv\",DataFrame,missingstring = \"NA\")\n    #@select :MID :year :wage :hrs :earn :SOI :CPIU :WelfH :FSInd :num_child :age\n    @subset :year.&gt;=1985 :year.&lt;=2010\n    @transform :AFDC = :WelfH.&gt;0 \n    @rename :FS = :FSInd\n    @transform :P  = :FS + :AFDC :H = min.(2,round.(Union{Int64, Missing},:hrs / (52*20)))\n    @subset .!ismissing.(:P) .&& .!ismissing.(:H)\n    @transform @byrow :wage = begin\n        if :hrs&gt;0 && :earn&gt;0\n            return :earn / :hrs / :CPIU\n        else\n            return missing\n        end\n    end\nend\n\ndata_mle = (;P = Int64.(data.P), H = Int64.(data.H), year = data.year, age = data.age,\n            soi = data.SOI, num_kids = data.num_child, cpi = data.CPIU,\n            logwage = log.(coalesce.(data.wage,1.)),wage_missing = ismissing.(data.wage))\n\n(P = [2, 2, 2, 1, 0, 0, 0, 2, 1, 0  …  2, 2, 1, 2, 0, 0, 1, 1, 1, 1], H = [0, 0, 0, 0, 0, 0, 0, 0, 0, 2  …  0, 0, 2, 2, 2, 2, 0, 0, 0, 0], year = [1994, 1995, 1996, 1998, 2000, 2002, 2004, 2006, 2008, 1990  …  1991, 1992, 1991, 1992, 1991, 1992, 1991, 1992, 1991, 1992], age = [21, 22, 23, 25, 27, 29, 31, 33, 35, 17  …  21, 22, 23, 24, 39, 40, 30, 31, 25, 26], soi = [43, 43, 43, 43, 43, 43, 43, 43, 43, 17  …  44, 44, 7, 7, 5, 5, 39, 39, 39, 39], num_kids = [2, 2, 3, 3, 3, 3, 3, 3, 3, 1  …  2, 2, 2, 2, 3, 3, 4, 4, 2, 2], cpi = [0.860812349005761, 0.884959812302546, 0.910948243820851, 0.946664188812488, 1.0, 1.04457233785542, 1.09707768072849, 1.17054218546739, 1.25008130459022, 0.758792510685746  …  0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336, 0.790785866939231, 0.8148346032336], logwage = [0.0, 0.0, 0.0, 0.0, 1.2039728043259361, 0.0, 0.0, 0.0, 0.0, 0.8230555833449215  …  0.0, 0.0, 2.6512574120409043, 2.3314412292534223, 1.265948758245773, 1.8744481199656744, 0.0, 0.0, 0.0, 0.0], wage_missing = Bool[1, 1, 1, 1, 0, 1, 1, 1, 1, 0  …  1, 1, 0, 0, 0, 0, 1, 1, 1, 1])\n\n\n\n\nThe Model\nConsider a static model with 9 discrete choices. Hours choices are given by \\(H_{j}\\in\\{0,20,40\\}\\) and participation choices indicated by \\(P_{j}\\in\\{0,1,2\\}\\). Individuals may choose any combination of these, with a Type I extreme value distributed shock \\(\\epsilon_{j}\\) for choice \\(j\\) that is iid over time and across individuals. The scale of these shocks is given by \\(\\sigma\\).\nUtility for individual \\(i\\) at time \\(t\\) is given by:\n\\[ U_{itj} = \\log(\\max\\{50,Y_{it}(W_{it}H_{j})\\}) + \\alpha_{l}\\log(112 - H_{j}) - \\alpha_{P,1}\\mathbf{1}\\{P_{j}&gt;0\\} - \\alpha_{P,2}\\mathbf{1}\\{P_{j}&gt;1\\} \\]\nwhere \\(Y_{it}\\) is a net income function for individual \\(i\\) at time \\(t\\) and depends on their earnings (\\(W_{it}H_{j}\\)), the number of children in the household, and the policy environment in the individual’s state of residence.\nWages are a deterministic function of age only:\n\\[ \\log(W_{it}) = \\gamma_{0} + \\gamma_{1}\\text{Age}_{it} + \\gamma_{2}\\text{Age}_{it}^2 \\]\nand are measured with normally distributed iid measurement error:\n\\[ \\log(W^{o}_{it}) = \\log(W_{it}) + \\zeta_{it},\\qquad\\zeta_{it}\\sim\\mathcal{N}(0,\\sigma^2_{W}) \\]\nThe code below calculates utility and indexes choices.\n\nj_idx(p,h) = p*3 + h + 1\nfunction utility(p,h,soi,cpi,year,num_kids,wage,pars)\n    (;αl,Hgrid,σ,αP) = pars\n    hrs = Hgrid[1+h]\n    earn = wage * hrs\n    net_income = max(50.,Transfers.budget(earn,0.,soi,year,num_kids,cpi,p))\n    return log(net_income) + αl*log(112-hrs) - αP[1]*(p&gt;0) - αP[2]*(p&gt;1)\nend\n\npars = (;αl = 1., σ = 1., σW = 1., γ = zeros(3),\n    Hgrid = [0,20.,40.],αP = zeros(2))\n\n(αl = 1.0, σ = 1.0, σW = 1.0, γ = [0.0, 0.0, 0.0], Hgrid = [0.0, 20.0, 40.0], αP = [0.0, 0.0])\n\n\nChoice probabilities are given by the logit formula.\n\nfunction choice_prob!(logP,it,data,pars)\n    (;σ,γ) = pars\n    wage = exp(γ[1] + γ[2] * data.age[it] + γ[3] * data.age[it]^2)\n    denom = 0.\n    umax = -Inf\n    for p in 0:2\n        for h in 0:2\n            j = j_idx(p,h)\n            u = utility(p,h,data.soi[it],data.cpi[it],data.year[it],data.num_kids[it],wage,pars)\n            logP[j] = u / σ\n            umax = u&gt;umax ? u : umax\n        end\n    end\n    logP[:] .-= umax / σ\n    denom = log(sum(exp.(logP)))\n    logP[:] .-= denom #&lt;- nornalize choice probabilities\nend\n\nchoice_prob! (generic function with 1 method)\n\n\nAnd so the log-likelihood takes a simple form.\n\nfunction log_likelihood(logP,it,data,pars)\n    (;γ,σW) = pars\n    ll = 0.\n    if !data.wage_missing[it]\n        ll += -0.5((data.logwage[it] - γ[1] - γ[2]*data.age[it] - γ[3]*data.age[it]^2) / σW)^2 - log(σW)\n    end\n    choice_prob!(logP,it,data,pars)\n    j = j_idx(data.P[it],data.H[it])\n    ll += logP[j]\n    return ll\nend\n\nfunction update(x,p)\n    αl = exp(x[1])\n    σ = exp(x[2])\n    γ = x[3:5]\n    σW = exp(x[6])\n    αP = x[7:8]\n    return (;p...,αl,σ,σW,αP,γ)\nend\n\nfunction log_likelihood(x,data,pars)\n    pars = update(x,pars)\n    logP = zeros(eltype(x),9)\n    ll = 0.\n    for it in eachindex(data.P)\n        ll += log_likelihood(logP,it,data,pars)\n    end\n    return ll\nend\n\nlog_likelihood (generic function with 2 methods)\n\n\nWe can use Optim to maximize the log-likelihood:\n\nx0 = zeros(8)\nx0 = [0.,0.,log(10.),0.,0.,0.,0.,0.]\nres = optimize(x-&gt;-log_likelihood(x,data_mle,pars),x0,BFGS(),autodiff=:forward,Optim.Options(show_trace = true))\npars = update(res.minimizer,pars)\n\nIter     Function value   Gradient norm \n     0     7.234596e+04     3.085114e+06\n * time: 0.010443925857543945\n     1     7.219007e+04     7.930873e+05\n * time: 0.5091629028320312\n     2     7.189756e+04     8.240034e+05\n * time: 0.6059629917144775\n     3     6.129802e+04     3.337752e+06\n * time: 0.8256139755249023\n     4     5.904150e+04     1.232987e+06\n * time: 1.0926239490509033\n     5     5.781471e+04     2.641891e+05\n * time: 1.3318979740142822\n     6     5.696965e+04     1.198471e+06\n * time: 1.531810998916626\n     7     5.683014e+04     1.164517e+06\n * time: 1.7155508995056152\n     8     5.609242e+04     1.940285e+06\n * time: 1.7655789852142334\n     9     5.480683e+04     3.811453e+06\n * time: 1.7998318672180176\n    10     5.274754e+04     2.634463e+06\n * time: 1.8343539237976074\n    11     5.214471e+04     7.076234e+05\n * time: 1.8681468963623047\n    12     5.191466e+04     9.933081e+04\n * time: 1.9020969867706299\n    13     5.187770e+04     1.158376e+05\n * time: 1.936262845993042\n    14     5.187629e+04     1.626802e+05\n * time: 1.9702098369598389\n    15     5.187431e+04     4.376935e+04\n * time: 2.0381948947906494\n    16     5.186935e+04     8.792504e+04\n * time: 2.088542938232422\n    17     5.186852e+04     7.387908e+02\n * time: 2.1225860118865967\n    18     5.186850e+04     2.434391e+03\n * time: 2.156467914581299\n    19     5.186850e+04     9.509518e+01\n * time: 2.1900689601898193\n    20     5.186850e+04     1.240425e+01\n * time: 2.2237679958343506\n    21     5.186850e+04     2.647360e-02\n * time: 2.2577929496765137\n    22     5.186850e+04     6.262737e-03\n * time: 2.2917888164520264\n    23     5.186850e+04     6.902728e-05\n * time: 2.3260738849639893\n    24     5.186850e+04     5.195348e-05\n * time: 2.3600308895111084\n    25     5.186850e+04     8.976889e-08\n * time: 2.3938188552856445\n    26     5.186850e+04     1.102359e-08\n * time: 2.444626808166504\n    27     5.186850e+04     3.531738e-09\n * time: 2.4784388542175293\n\n\n(αl = 2.8546233783929478, σ = 0.9509286759263833, σW = 0.7224860062936597, γ = [-0.023620750582814317, 0.10877398375254962, -0.0012722347394312208], Hgrid = [0.0, 20.0, 40.0], αP = [2.0353016361355523, 1.0137619542631455])"
  },
  {
    "objectID": "assignments_2024/Assignment-2.html#question-1",
    "href": "assignments_2024/Assignment-2.html#question-1",
    "title": "Assignment 2",
    "section": "Question 1",
    "text": "Question 1\nWrite code to calculate the standard errors for these parameters and present the estimates with standard errors in a table."
  },
  {
    "objectID": "assignments_2024/Assignment-2.html#question-2",
    "href": "assignments_2024/Assignment-2.html#question-2",
    "title": "Assignment 2",
    "section": "Question 2",
    "text": "Question 2\nWhat sources of variation are identifying \\(\\sigma\\), the responsiveness of choices to changes in the payoffs? How is the responsivess of program participation and labor supply to changes in incentives limited in this framework?"
  },
  {
    "objectID": "assignments_2024/Assignment-2.html#question-3",
    "href": "assignments_2024/Assignment-2.html#question-3",
    "title": "Assignment 2",
    "section": "Question 3",
    "text": "Question 3\nRe-estimate the model with a one layer nested logit structure, where the individual first makes one of the three participation choices before making one of the three hours choices. That is, if the choices can be ordered as: \\[ \\{P_{j}\\}_{j=1}^9 = \\{0,0,0,1,1,1,2,2,2\\}, \\{H_{j}\\}_{j=1}^9 = \\{0,20,40,0,20,40,0,20,40\\} \\]\nthen the partition is:\n\\[ \\mathcal{B} = \\{\\{1,2,3\\},\\{4,5,6\\},\\{7,8,9\\}\\} \\]\nLet \\(\\sigma_{H}\\) be the dispersion of shocks within each partition and \\(\\sigma_{P}\\) be the dispersion of shocks across partitions. Intuitively, how are these dispersion parameters identified? (Hint: think about what is changing payoffs across observations and think about the two dimensions of the discrete choice)."
  },
  {
    "objectID": "assignments_2024/Assignment-2.html#question-4",
    "href": "assignments_2024/Assignment-2.html#question-4",
    "title": "Assignment 2",
    "section": "Question 4",
    "text": "Question 4\nWhat sort of variation do you think is in the data that we are failing to put in our model? What do you think are the implications for key parameter estimates?"
  },
  {
    "objectID": "assignments/Assignment-1.html#a-simple-labor-supply-model",
    "href": "assignments/Assignment-1.html#a-simple-labor-supply-model",
    "title": "Assignment 1",
    "section": "",
    "text": "Consider a dynamic labor supply model (with no uncertainty) where each agent \\(n\\) chooses a sequence of consumption and hours, \\(\\{c_{t},h_{t}\\}_{t=1}^{\\infty}\\), to solve: \\[ \\max \\sum_{t=0}^\\infty \\frac{c_{t}^{1-\\sigma}}{1-\\sigma} - \\frac{\\alpha_{n}^{-1}}{1 + 1\\psi}h_{t}^{1+1/\\psi}\\] subject to the intertemporal budget constraint: \\[ \\sum_{t}q_{t}c_{t} \\leq a_{0} + \\sum_{t}q_{t}W_{n,t}h_{t},\\ q_{t} = (1+r)^{-t}.\\] Let \\(H_{n,t}\\) and \\(C_{n,t}\\) be the random variables Labor supply in this model obeys: \\[h_{n,t}\nTo simplify, assume that $\\beta=(1+r)^{-1}$, so that the optimal solution features perfectly smoothed consumption, $c^*_{n}$:\n\\]c^{*}_{n} = $$"
  },
  {
    "objectID": "assignments/Assignment-1.html#setup-1-a-simple-labor-supply-model",
    "href": "assignments/Assignment-1.html#setup-1-a-simple-labor-supply-model",
    "title": "Assignment 1",
    "section": "",
    "text": "Consider a dynamic labor supply model (with no uncertainty) where each agent \\(n\\) chooses a sequence of consumption and hours, \\(\\{c_{t},h_{t}\\}_{t=1}^{\\infty}\\), to solve: \\[ \\max \\sum_{t=0}^\\infty \\beta^{t} \\left(\\frac{c_{t}^{1-\\sigma}}{1-\\sigma} - \\frac{\\alpha_{n}^{-1}}{1 + 1\\psi}h_{t}^{1+1/\\psi}\\right)\\] subject to the intertemporal budget constraint: \\[ \\sum_{t}q_{t}c_{t} \\leq A_{n,0} + \\sum_{t}q_{t}W_{n,t}h_{t},\\qquad q_{t} = (1+r)^{-t}.\\] Let \\(H_{n,t}\\) and \\(C_{n,t}\\) be the realizations of labor supply for agent \\(n\\) at time \\(t\\). Labor supply in this model obeys: \\[H_{n,t} = (\\beta(1+r))^{-t}(\\alpha_{n}W_{n,t})C^{-\\sigma}_{n,t}.\\] To simplify below, assume that \\(\\beta=(1+r)^{-1}\\), so that the optimal solution features perfectly smoothed consumption, \\(C^*_{n}\\). Making appropriate substitutions gives \\(C^*_{n}\\) as the solution to: \\[ \\left(\\sum_{t}q_{t}\\right)C^*_{n} = \\sum_{t}\\left(q_{t}W_{n,t}^{1+\\psi}\\right)\\alpha_{n}^{\\psi}(C^*)^{-\\sigma}_{n} + A_{n,0}.\\]"
  },
  {
    "objectID": "assignments/Assignment-1.html#setup-2-a-diff-in-diff-analysis",
    "href": "assignments/Assignment-1.html#setup-2-a-diff-in-diff-analysis",
    "title": "Assignment 1",
    "section": "Setup 2: A Diff-in-Diff Analysis",
    "text": "Setup 2: A Diff-in-Diff Analysis\nYou are talking to a friend (let’s call them Mr Straw Man) who has data on labor supply for two groups, \\(A\\), and \\(B\\), who are not directly comparable. Assume they have two periods of data (\\(s=1,2\\)), and that group \\(A\\) is subjected to a proportional wage subsidy, \\(1+\\tau\\), in period 2 (\\(s=2\\)). In this case the effective wage is \\(\\tilde{W}_{n,t} = (1+\\tau)W_{n,t}\\). Your friend claims that they can estimate the elasticity of labor supply by first estimating the response to the tax using difference-in-differences, which has a population limit:\n\\[ \\alpha = \\mathbb{E}[\\log(H)|A,s=2] - \\mathbb{E}[\\log(H)|B,s=2] - (\\mathbb{E}[\\log(H)|A,s=1] - \\mathbb{E}[\\log(H)|B,s=1])\\] and then constructing the elasticity as: \\[ \\tilde{\\psi} = \\frac{\\alpha}{\\log(1+\\tau)}.\\]\nYour friend also claims that this elasticity is sufficient to forecast the effects of any other labor supply subsidy in the future."
  },
  {
    "objectID": "assignments/Assignment-1.html#questions",
    "href": "assignments/Assignment-1.html#questions",
    "title": "Assignment 1",
    "section": "Questions",
    "text": "Questions\nIn answering the questions below, you can assume that the distribution of wages and preferences adhere to the following relationship for each group \\(g\\in\\{A,B\\}\\): \\[\\mathbb{E}[\\log(W_{n,t})|g] = \\mu_{W,g} + \\gamma_{t}\\] \\[\\mathbb{E}[\\alpha_{n}|g] = \\mu_{\\alpha,g}\\]\n\nShow that this model satisfies the “parallel trends assumption”, namely that if the policy had never been introduced, we would get \\[ \\mathbb{E}[\\log(H)|A,s=2] - \\mathbb{E}[\\log(H)|B,s=2] - (\\mathbb{E}[\\log(H)|A,s=1] - \\mathbb{E}[\\log(H)|B,s=1]) = 0 \\]\nShow that when the policy is introduced unexpectedly, in period 2, the population limit \\(\\tilde{\\psi}\\) can be written as: \\[ \\tilde{\\psi} =  \\psi - \\sigma\\frac{\\Delta\\mathbb{E}\\left[\\log(C^*)|A\\right]}{\\log(1+\\tau)}\\] where \\[\\Delta\\mathbb{E}\\left[\\log(C^*)|A\\right]\\] is the growth in log consumption induced by the policy.\nDerive a similar expression for \\(\\tilde{\\psi}\\) for the case where the policy is announced in the first period (\\(s=1\\)) and implemented in the second period (\\(s=2\\)).\nUse the model to argue that the size of the effect depends on whether the subsidy is perceived as temporary or permanent.\nSuppose that the exact same policy (same anticipation, same duration, same \\(\\tau\\)) were to be implemented on group \\(B\\). Under what assumptions on the distribution of assets could we expect the same labor supply response in the model? Does this seem reasonable?\nUse these derivations to argue that:\n\nThe estimand \\(\\tilde{\\psi}\\) does not map directly to an underlying structural parameter of interest; and\nThe parameter \\(\\tilde{\\psi}\\) cannot be used to forecast the effect of wage subsidies that are either (a) larger than \\(\\tau\\); or (b) implemented over different horizons; or (c) implemented on group \\(B\\)."
  },
  {
    "objectID": "models/DynamicLaborSupply.html",
    "href": "models/DynamicLaborSupply.html",
    "title": "A Simple Labor Supply Model",
    "section": "",
    "text": "Consider a dynamic labor supply model (with no uncertainty) where each agent \\(n\\) chooses a sequence of consumption and hours, \\(\\{c_{t},h_{t}\\}_{t=1}^{\\infty}\\), to solve: \\[ \\max \\sum_{t=0}^\\infty \\beta^{t} \\left(\\frac{c_{t}^{1-\\sigma}}{1-\\sigma} - \\frac{\\alpha_{n}^{-1}}{1 + 1\\psi}h_{t}^{1+1/\\psi}\\right)\\] subject to the intertemporal budget constraint: \\[ \\sum_{t}q_{t}c_{t} \\leq A_{n,0} + \\sum_{t}q_{t}W_{n,t}h_{t},\\qquad q_{t} = (1+r)^{-t}.\\] Let \\(H_{n,t}\\) and \\(C_{n,t}\\) be the realizations of labor supply for agent \\(n\\) at time \\(t\\). Labor supply in this model obeys: \\[H_{n,t} = (\\beta(1+r))^{-t}(\\alpha_{n}W_{n,t})C^{-\\sigma}_{n,t}.\\] To simplify below, assume that \\(\\beta=(1+r)^{-1}\\), so that the optimal solution features perfectly smoothed consumption, \\(C^*_{n}\\). Making appropriate substitutions gives \\(C^*_{n}\\) as the solution to: \\[ \\left(\\sum_{t}q_{t}\\right)C^*_{n} = \\sum_{t}\\left(q_{t}W_{n,t}^{1+\\psi}\\right)\\alpha_{n}^{\\psi}(C^*)^{-\\sigma}_{n} + A_{n,0}.\\]"
  },
  {
    "objectID": "models/DynamicLaborSupply.html#model-setup-and-solution",
    "href": "models/DynamicLaborSupply.html#model-setup-and-solution",
    "title": "A Simple Labor Supply Model",
    "section": "",
    "text": "Consider a dynamic labor supply model (with no uncertainty) where each agent \\(n\\) chooses a sequence of consumption and hours, \\(\\{c_{t},h_{t}\\}_{t=1}^{\\infty}\\), to solve: \\[ \\max \\sum_{t=0}^\\infty \\beta^{t} \\left(\\frac{c_{t}^{1-\\sigma}}{1-\\sigma} - \\frac{\\alpha_{n}^{-1}}{1 + 1\\psi}h_{t}^{1+1/\\psi}\\right)\\] subject to the intertemporal budget constraint: \\[ \\sum_{t}q_{t}c_{t} \\leq A_{n,0} + \\sum_{t}q_{t}W_{n,t}h_{t},\\qquad q_{t} = (1+r)^{-t}.\\] Let \\(H_{n,t}\\) and \\(C_{n,t}\\) be the realizations of labor supply for agent \\(n\\) at time \\(t\\). Labor supply in this model obeys: \\[H_{n,t} = (\\beta(1+r))^{-t}(\\alpha_{n}W_{n,t})C^{-\\sigma}_{n,t}.\\] To simplify below, assume that \\(\\beta=(1+r)^{-1}\\), so that the optimal solution features perfectly smoothed consumption, \\(C^*_{n}\\). Making appropriate substitutions gives \\(C^*_{n}\\) as the solution to: \\[ \\left(\\sum_{t}q_{t}\\right)C^*_{n} = \\sum_{t}\\left(q_{t}W_{n,t}^{1+\\psi}\\right)\\alpha_{n}^{\\psi}(C^*)^{-\\sigma}_{n} + A_{n,0}.\\]"
  },
  {
    "objectID": "models/DynamicLaborSupply.html#code-to-solve-the-model",
    "href": "models/DynamicLaborSupply.html#code-to-solve-the-model",
    "title": "A Simple Labor Supply Model",
    "section": "Code to solve the model",
    "text": "Code to solve the model\nThere is only one object to solve here which is consumption given a sequence of net wages. If one were to assume also constant wages (or net ages in the case of Assignment 2, the function below solves optimal consumption.\n\nusing Optim\nfunction solve_consumption(r,α,W,A,σ,ψ)\n    Q = 1/ (1 - 1/(1+r))\n    f(c) = (Q * c - Q * W^(1 + ψ) * α^ψ * c^(-σ) - A)^2\n    r = Optim.optimize(f,0.,A+W)\n    return r.minimizer\nend\n\nsolve_consumption (generic function with 1 method)"
  },
  {
    "objectID": "models/DynamicLaborSupply.html#code-to-simulate-a-cross-section",
    "href": "models/DynamicLaborSupply.html#code-to-simulate-a-cross-section",
    "title": "A Simple Labor Supply Model",
    "section": "Code to simulate a cross-section",
    "text": "Code to simulate a cross-section\nHere we’ll assume that wages, tastes for work, and assets co-vary systematically. For simplicity we’ll use a multivariate log-normal distribution.\nBelow is code to simulate a cross-section of 1,000 observations.\n\nusing Distributions\nfunction simulate_data(σ,ψ,r,N)\n    ch = [1. 0. 0.; 0.4 0.9 0.; 0.4 1.1 3.1]\n    #ch = [1. 0. 0.; 0. 1. 0.; 0. 0. 3.1]\n    Σ = ch * ch'\n    X = rand(MvNormal(Σ),N)\n    α = exp.(X[1,:])\n    W = exp.(X[2,:])\n    A = exp.(X[3,:])\n    C = [solve_consumption(r,α[i],W[i],A[i],σ,ψ) for i in eachindex(A)]\n    @views H = exp.( X[1,:] .+ ψ .* X[2,:] .- σ .* log.(C) )\n    return (;α,W,A,C,H)\nend\n\n# assume risk-aversion of 2 and frisch of 0.5\nσ = 2.\nψ = 0.5\nr = 0.05\n\ndat = simulate_data(σ,ψ,r,1_000)\n\n(α = [2.387042085753637, 0.675382543197479, 1.8041373364119184, 0.5177487029077495, 0.09655506981210882, 1.824797940910797, 3.289262038119427, 0.2506585686265941, 1.0732432496225723, 9.220969196700604  …  0.44807287095054077, 0.5774257359764811, 0.32223820131152825, 1.2142108180440008, 0.9198391663056062, 0.8119510006991605, 0.14363053770161133, 7.963050469654547, 0.7690905237621848, 1.8570637722644605], W = [0.5941629684281109, 1.959131372579672, 0.8186297014186358, 0.29582617047519216, 0.20321857668906812, 1.1865304510694517, 0.9534324193921412, 0.8909230785283205, 0.6342253531447469, 6.0287761164877285  …  0.21881949566723746, 0.47603589428150134, 0.25101841997917756, 1.1654047485799317, 0.592025390910184, 1.228147149416917, 0.2323991078228227, 0.37208029557774847, 1.300803443596376, 0.3211222329605617], A = [0.4339543120074736, 26.110042820724665, 0.642998452478576, 4.283388095106766, 0.0009552800495203945, 0.8887607765936064, 8.226456655590136, 2.194682249098367, 0.4414326833972042, 0.3556053392102825  …  0.8816554782582653, 0.023835009218317497, 0.14534563234434092, 13.249239397236687, 2.6242366522659624, 1.6485084474538854, 0.1360250417097477, 0.0945706557502548, 0.01629536325084972, 0.3114722102527127], C = [0.8980478913452508, 1.8805628992380534, 1.008598227916416, 0.5657213984122716, 0.2041738521350642, 1.2184090522580167, 1.3367008684072117, 0.7859977959784272, 0.8128883673790366, 3.5612352948669095  …  0.4236829312278632, 0.49987089050719563, 0.39636404529038805, 1.3697768673760502, 0.8028155967010564, 1.0972187071423138, 0.35103831544979686, 0.46665094203981866, 1.0919556307425837, 0.6325944334114565], H = [2.28146531964173, 0.26730426178705274, 1.604637515456518, 0.879897538633473, 1.044134725228131, 1.338961200467195, 1.7975251269877968, 0.38296581013999187, 1.2934753227993598, 1.785212057599448  …  1.1676420321671064, 1.5944110421773459, 1.0276407470472402, 0.6986068414048688, 1.0981216703170185, 0.7474267149423603, 0.5618947455746683, 22.30557947682825, 0.7356535160995579, 2.629730681735306])"
  },
  {
    "objectID": "models/DynamicLaborSupply.html#simulating-bias-in-ols",
    "href": "models/DynamicLaborSupply.html#simulating-bias-in-ols",
    "title": "A Simple Labor Supply Model",
    "section": "Simulating bias in OLS",
    "text": "Simulating bias in OLS\nWe know that OLS will be biased here, but let’s run a simulation anyway to hammer the point home.\n\nfunction simulate_bias(B,N,σ,ψ,r)\n    ψ_est = zeros(B)\n    σ_est = zeros(B)\n    for b in 1:B\n        dat = simulate_data(σ,ψ,r,N)\n        X = [ones(N) log.(dat.W) log.(dat.C)]\n        Y = log.(dat.H)\n        b_est = inv(X' * X) * X' * Y\n        ψ_est[b] = b_est[2]\n        σ_est[b] = -b_est[3]\n    end\n    return ψ_est,σ_est\nend\n\nψ_est,σ_est = simulate_bias(500,1_000,σ,ψ,r)\n\n([0.8249048777001468, 0.8089259239710392, 0.8379443655296146, 0.8544345756415228, 0.8526595190361529, 0.8983845276540198, 0.7766709389966978, 0.8135140148228429, 0.7882234429034518, 0.84320193295895  …  0.8362508169825612, 0.900122823105285, 0.8483028155727771, 0.8920174865803856, 0.8259524323426323, 0.9046057419459029, 0.8056638010047196, 0.8063225076275563, 0.8186642955221831, 0.7695944544253791], [1.9187717688171582, 1.8620443863464544, 1.9231551333701524, 1.8764499192190964, 1.901653649952178, 1.969609545114789, 1.7846524229246177, 1.930457104562582, 1.8834043992893916, 1.974176170591523  …  1.9421373840184375, 1.9307400236802532, 1.9152815716445895, 1.9462019801676231, 1.9190517019831361, 1.9798692898658017, 1.9284999545456865, 1.8992373328292191, 1.859799472693129, 1.8946789215935422])\n\n\nGiven the covariance structure, we can see a clear bias in the estimates.\n\nusing Plots\nhistogram(ψ_est)"
  }
]